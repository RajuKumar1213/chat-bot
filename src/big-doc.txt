Charles Robert Darwin (/ˈdɑːrwɪn/[5] DAR-win; 12 February 1809 – 19 April 1882) was an English naturalist, geologist, and biologist,[6] widely known for his contributions to evolutionary biology. His proposition that all species of life have descended from a common ancestor is now generally accepted and considered a fundamental scientific concept.[7] In a joint presentation with Alfred Russel Wallace, he introduced his scientific theory that this branching pattern of evolution resulted from a process he called natural selection, in which the struggle for existence has a similar effect to the artificial selection involved in selective breeding.[8] Darwin has been described as one of the most influential figures in human history and was honoured by burial in Westminster Abbey.[9][10]

Darwin's early interest in nature led him to neglect his medical education at the University of Edinburgh; instead, he helped Grant to investigate marine invertebrates. His studies at the University of Cambridge's Christ's College from 1828 to 1831 encouraged his passion for natural science.[11] However, it was his five-year voyage on HMS Beagle from 1831 to 1836 that truly established Darwin as an eminent geologist. The observations and theories he developed during his voyage supported Charles Lyell's concept of gradual geological change. Publication of his journal of the voyage made Darwin famous as a popular author.[12] His first scientific work was The Structure and Distribution of Coral Reefs (1842). Along with his work on barnacles, it won him the Royal Medal in 1853.

Puzzled by the geographical distribution of wildlife and fossils he collected on the voyage, Darwin began detailed investigations and, in 1838, devised his theory of natural selection.[13] Although he discussed his ideas with several naturalists, he needed time for extensive research, and his geological work had priority.[14] He was writing up his theory in 1858 when Wallace sent him an essay that described the same idea, prompting the immediate joint submission of both their theories to the Linnean Society of London.[15] Darwin's work established evolutionary descent with modification as the dominant scientific explanation of natural diversification.[16] Darwin published his theory of evolution with compelling evidence in On the Origin of Species (1859).[17][18] Darwin's work established evolutionary descent with modification as the dominant scientific explanation of natural diversification.[16] He explored coevolution in Fertilisation of Orchids (1862) and human evolution and sexual selection in The Descent of Man, and Selection in Relation to Sex (1871). The Expression of the Emotions in Man and Animals (1872) was an early work of psychology, and one of the first books to feature photographs. His final book was The Formation of Vegetable Mould, through the Actions of Worms (1881).

By the 1870s, the scientific community and a majority of the educated public had accepted evolution as a fact. However, many initially favoured competing explanations that gave only a minor role to natural selection. It was not until the emergence of the modern evolutionary synthesis from the 1930s to the 1950s that a broad consensus developed in which natural selection was the basic mechanism of evolution.[16][19] Darwin's discovery is the unifying theory of the life sciences, explaining the unity and diversity of life.

Biography
Early life and education
Further information: Charles Darwin's education and Darwin–Wedgwood family
Darwin was born in Shrewsbury, Shropshire, on 12 February 1809, at his family's home, The Mount.[20][21] He was the fifth of six children of wealthy society doctor and financier Robert Darwin and Susannah Darwin (née Wedgwood). His grandfathers Erasmus Darwin and Josiah Wedgwood were both prominent abolitionists. Erasmus Darwin had praised general concepts of evolution and common descent in his Zoonomia (1794), a poetic fantasy of gradual creation including undeveloped ideas anticipating concepts his grandson expanded.[22]

Three-quarter length portrait of seated boy smiling and looking at the viewer; he has straight, mid-brown hair and wears dark clothes with a large, frilly, white collar; in his lap he holds a pot of flowering plants
A chalk drawing of the seven-year-old Darwin in 1816, with a potted plant, by Ellen Sharples. Part of a double portrait showing him together with his sister Catherine.
Both families were largely Unitarian, though the Wedgwoods were adopting Anglicanism. Robert Darwin, a freethinker, had baby Charles baptised in November 1809 in the Anglican St Chad's Church, Shrewsbury, but Charles and his siblings attended the local Unitarian Church with their mother. The eight-year-old Charles already had a taste for natural history and collecting when he joined the day school run by its preacher in 1817. That July, his mother died. From September 1818, he joined his older brother Erasmus in attending the nearby Anglican Shrewsbury School as a boarder.[23]

Darwin spent the summer of 1825 as an apprentice doctor, helping his father treat impoverished people in Shropshire, before going to the well-regarded University of Edinburgh Medical School with his brother Erasmus in October 1825. Darwin found lectures dull and surgery distressing, so he neglected his studies.[24] He learned taxidermy in around 40 daily hour-long sessions from John Edmonstone, a Black Briton from Demerara in the South American rainforest, who had been taught there by Charles Waterton, and when brought to Scotland was freed from slavery.[25][26]

In Darwin's second year at the university, he joined the Plinian Society, a student natural-history group featuring lively debates in which radical democratic students with materialistic views challenged orthodox religious concepts of science.[27] He assisted Robert Edmond Grant's investigations of the anatomy and life cycle of marine invertebrates in the Firth of Forth, and on 27 March 1827 presented at the Plinian his own discovery that black spores found in oyster shells were the eggs of a skate leech.[28]

One day, Grant praised Lamarck's evolutionary ideas. Darwin was astonished by Grant's audacity, but had recently read similar ideas in his grandfather Erasmus' journals.[29] Darwin was rather bored by Robert Jameson's natural-history course, which covered geology – including the debate between neptunism and plutonism. He learned the classification of plants. He assisted with work on the collections of the University Museum, one of the largest museums in Europe at the time.[30]

Darwin's neglect of medical studies annoyed his father, who sent him to Christ's College, Cambridge in January 1828 to study for a Bachelor of Arts degree as the first step towards becoming an Anglican country parson. Darwin was unqualified for Cambridge's Tripos exams and was required instead to join the ordinary degree course.[31] He preferred riding and shooting to studying.[32]

Bronze statue of Darwin in 1830 clothes, seated on the arm of a wooden bench; behind him plants partly cover a stone wall, a window has white-painted wooden frames
A bicentennial portrait by Anthony Smith of Darwin as a student, in the courtyard at Christ's College, Cambridge, where he had rooms.[33]
During the first few months of Darwin's enrolment at Christ's College, his second cousin William Darwin Fox was still studying there. Fox impressed him with his butterfly collection, introducing Darwin to entomology and influencing him to pursue beetle collecting.[34][35] He did this zealously and had some of his finds published in James Francis Stephens' Illustrations of British entomology (1829–1932).[35][36]

Through Fox, Darwin became a close friend and follower of botany professor John Stevens Henslow.[34] He met other leading parson-naturalists who saw scientific work as religious natural theology, becoming known to these dons as "the man who walks with Henslow". When his own exams drew near, Darwin applied himself to his studies and was delighted by the language and logic of William Paley's Evidences of Christianity (1795).[37] In his final examination in January 1831, Darwin did well, coming tenth out of 178 candidates for the ordinary degree.[38]

Darwin had to stay at Cambridge until June 1831. He studied Paley's Natural Theology or Evidences of the Existence and Attributes of the Deity (first published in 1802), which made an argument for divine design in nature, explaining adaptation as God acting through laws of nature.[39] He read John Herschel's new book, Preliminary Discourse on the Study of Natural Philosophy (1831), which described the highest aim of natural philosophy as understanding such laws through inductive reasoning based on observation, and Alexander von Humboldt's Personal Narrative of scientific travels in 1799–1804.[40] Inspired with "a burning zeal" to contribute, Darwin planned to visit Tenerife with some classmates after graduation to study natural history in the tropics. In preparation, he joined Adam Sedgwick's geology course, then on 4 August travelled with him to spend a fortnight mapping strata in Wales.[41][42]

Survey voyage on HMS Beagle
Further information: Second voyage of HMS Beagle
Route from Plymouth, England, south to Cape Verde then southwest across the Atlantic to Bahia, Brazil, south to Rio de Janeiro, Montevideo, the Falkland Islands, round the tip of South America then north to Valparaiso and Callao. Northwest to the Galapagos Islands before sailing west across the Pacific to New Zealand, Sydney, Hobart in Tasmania, and King George's Sound in Western Australia. Northwest to the Keeling Islands, southwest to Mauritius and Cape Town, then northwest to Bahia and northeast back to Plymouth.
The round-the-world voyage of the Beagle, 1831–1836
After leaving Sedgwick in Wales, Darwin spent a few days with student friends at Barmouth. He returned home on 29 August to find a letter from Henslow proposing him as a suitable (if unfinished) naturalist for a self-funded supernumerary place on HMS Beagle with captain Robert FitzRoy, a position for a gentleman rather than "a mere collector". The ship was to leave in four weeks on an expedition to chart the coastline of South America.[43][44] Robert Darwin objected to his son's planned two-year voyage, regarding it as a waste of time, but was persuaded by his brother-in-law, Josiah Wedgwood II, to agree to (and fund) his son's participation.[45] Darwin took care to remain in a private capacity to retain control over his collection, intending it for a major scientific institution.[46]

After delays, the voyage began on 27 December 1831; it lasted almost five years. As FitzRoy had intended, Darwin spent most of that time on land investigating geology and making natural history collections, while HMS Beagle surveyed and charted coasts.[16][47] He kept careful notes of his observations and theoretical speculations. At intervals during the voyage, his specimens were sent to Cambridge together with letters including a copy of his journal for his family.[48] He had some expertise in geology, beetle collecting and dissecting marine invertebrates, but in all other areas, was a novice and ably collected specimens for expert appraisal.[49] Despite suffering badly from seasickness, Darwin wrote copious notes while on board the ship. Most of his zoology notes are about marine invertebrates, starting with plankton collected during a calm spell.[47][50]


Darwin (right) on the Beagle's deck at Bahía Blanca in Argentina, with fossils; caricature by Augustus Earle, the initial ship's artist
On their first stop ashore at St Jago in Cape Verde, Darwin found that a white band high in the volcanic rock cliffs included seashells. FitzRoy had given him the first volume of Charles Lyell's Principles of Geology, which set out uniformitarian concepts of land slowly rising or falling over immense periods,[II] and Darwin saw things Lyell's way, theorising and thinking of writing a book on geology.[51] When they reached Brazil, Darwin was delighted by the tropical forest,[52] but detested the sight of slavery there, and disputed this issue with FitzRoy.[53]

The survey continued to the south in Patagonia. They stopped at Bahía Blanca, and in cliffs near Punta Alta, Darwin made a significant find of fossil bones of huge extinct mammals beside modern seashells, indicating recent extinction with no signs of change in climate or catastrophe. He found bony plates like a giant version of the armour on local armadillos. From a jaw and tooth, he identified the gigantic Megatherium, then from Cuvier's description thought the armour was from this animal. The finds were shipped to England, and scientists found the fossils of great interest.[54][55]

On rides with gauchos into the interior to explore geology and collect more fossils, Darwin gained social, political, and anthropological insights into both native and colonial people at a time of revolution, and learnt that two types of rhea had separate but overlapping territories.[56][57] Further south, he saw stepped plains of shingle and seashells as raised beaches at a series of elevations. He read Lyell's second volume and accepted its description of "centres of creation" of species, but his discoveries and theorising challenged Lyell's ideas of smooth continuity and of extinction of species.[58][59] In Tierra del Fuego, Darwin formed the incorrect belief that the archipelago was devoid of reptiles.[60]

Three Fuegians on board, who had been seized during the first Beagle voyage and then given Christian education in England, were returning with a missionary. Darwin found them friendly and civilised, yet at Tierra del Fuego he met "miserable, degraded savages", as different as wild from domesticated animals.[61] He remained convinced that, despite this diversity, all humans were interrelated with a shared origin and potential for improvement towards civilisation. Unlike his scientist friends, he now thought there was no unbridgeable gap between humans and animals.[62] A year on, the mission had been abandoned. The Fuegian they had named Jemmy Button lived like the other natives, had a wife, and had no wish to return to England.[63]

On a sea inlet surrounded by steep hills, with high snow-covered mountains in the distance, someone standing in an open canoe waves at a square-rigged sailing ship, seen from the front
As HMS Beagle surveyed the coasts of South America, Darwin theorised about geology and the extinction of giant mammals. Watercolour by the ship's artist Conrad Martens, who replaced Augustus Earle, in Tierra del Fuego.
Darwin experienced an earthquake in Chile in 1835 and saw signs that the land had just been raised, including mussel beds stranded above high tide. High in the Andes, he saw seashells and several fossil trees that had grown on a sand beach. He theorised that as the land rose, oceanic islands sank, and coral reefs around them grew to form atolls.[64][65]

On the geologically new Galápagos Islands, Darwin looked for evidence attaching wildlife to an older "centre of creation". He found mockingbirds allied to those in Chile but differing from island to island. He heard that slight variations in the shape of tortoise shells showed which island they came from, but failed to collect them, even after eating tortoises taken on board as food.[66][67] In Australia, the marsupial rat-kangaroo and the platypus seemed so unusual that Darwin thought it was almost as though two distinct Creators had been at work.[68] He found the Aboriginal Australians "good-humoured & pleasant", their numbers depleted by European settlement.[69]

FitzRoy investigated how the atolls of the Cocos (Keeling) Islands had formed. The survey supported Darwin's theorising.[65] FitzRoy began writing the official Narrative of the Beagle voyages, and after reading Darwin's diary, he proposed incorporating it into the account.[70] Darwin's Journal was eventually rewritten as a separate third volume, on geology and natural history.[71][72]

In Cape Town, South Africa, Darwin and FitzRoy met John Herschel, who had recently written to Lyell praising his uniformitarianism as opening bold speculation on "that mystery of mysteries, the replacement of extinct species by others" as "a natural in contradistinction to a miraculous process".[73] When organising his notes as the ship sailed home, Darwin wrote that, if his growing suspicions about the mockingbirds, the tortoises and the Falkland Islands fox were correct, "such facts undermine the stability of Species", then cautiously added "would" before "undermine".[74] He later wrote that such facts "seemed to me to throw some light on the origin of species".[75]

Without Darwin's knowledge, extracts from his letters to Henslow had been read to scientific societies, printed as a pamphlet for private distribution among members of the Cambridge Philosophical Society, and reported in magazines,[76] including The Athenaeum.[77] Darwin first heard of this at Cape Town,[78] and at Ascension Island read of Sedgwick's prediction that Darwin "will have a great name among the Naturalists of Europe".[79][80]



The Bhagavad Gita (/ˈbʌɡəvəd ˈɡiːtɑː/;[1] Sanskrit: भगवद्गीता, IPA: [ˌbʱɐɡɐʋɐd ˈɡiːtɑː], romanized: bhagavad-gītā, lit. 'God's song'),[a] often referred to as the Gita (IAST: gītā), is a Hindu scripture, likely composed in the second or first century BCE,[7] which forms part of the epic poem Mahabharata. The Gita is a synthesis of various strands of Indian religious thought, including the Vedic concept of dharma (duty, rightful action); Sankhya-based yoga and jnana (knowledge); and bhakti (devotion).[8][b] Among the Hindu traditions, the Gita holds a unique pan-Hindu influence as the most prominent sacred text and is a central text in the Vedanta and Vaishnava traditions.

While traditionally attributed to the sage Veda Vyasa, the Gita is historiographically regarded as a composite work by multiple authors.[9][10][11] Incorporating teachings from the Upanishads and the samkhya yoga philosophy, the Gita is set in a narrative framework of dialogue between the Pandava prince Arjuna and his charioteer guide Krishna, an avatar of Vishnu, at the onset of the Kurukshetra War.[6]

Though the Gita praises the benefits of yoga[12][13] in releasing man's inner essence from the bounds of desire and the wheel of rebirth,[6] the text propagates the Brahmanic idea of living according to one's duty or dharma, in contrast to the ascetic ideal of seeking liberation by avoiding all karma.[12] Facing the perils of war, Arjuna hesitates to perform his duty (dharma) as a warrior. Krishna persuades him to commence in battle, arguing that while following one's dharma, one should not consider oneself to be the agent of action, but attribute all of one's actions to God (bhakti).[14][15]

The Gita posits the existence of an individual self (mind/ego) and the higher Godself (Krishna, Atman/Brahman) in every being;[c] the Krishna–Arjuna dialogue has been interpreted as a metaphor for an everlasting dialogue between the two.[d] Numerous classical and modern thinkers have written commentaries on the Gita with differing views on its essence and the relation between the individual self (jivatman) and God (Krishna)[16] or the supreme self (Atman/Brahman). In the Gita's Chapter XIII, verses 24–25, four pathways to self-realization are described, which later became known as the four yogas: meditation (raja yoga), insight and intuition (jnana yoga), righteous action (karma yoga), and loving devotion (bhakti yoga). This influential classification gained widespread recognition through Swami Vivekananda's teachings in the 1890s.[17][18] The setting of the text in a battlefield has been interpreted by several modern Indian writers as an allegory for the struggles and vagaries of human life.

Etymology
The Gita in the title of the Bhagavad Gita means "song". Religious leaders and scholars interpret the word Bhagavad in several ways. Accordingly, the title has been interpreted as "the song of God", "the word of God" by theistic schools,[19] "the words of the Lord",[20] "the Divine Song",[21][page needed][22] and "Celestial Song" by others.[23]

The Sanskrit name is often written as Shrimad Bhagavad Gita or Shrimad Bhagavadgita (श्रीमद् भगवद् गीता or श्रीमद् भगवद्गीता). The prefix shrimad (or shrimat) denotes a high degree of respect. The Bhagavad Gita is not to be confused with the Bhagavata Puran, which is one of the eighteen major Puranas dealing with the life of the Hindu God Krishna and various avatars of Vishnu.[24]

The work is also known as the Iswara Gita, the Ananta Gita, the Hari Gita, the Vyasa Gita, or the Gita.[25]

Dating and authorship
Dating

Vāsudeva-Krishna, on a coin of Agathocles of Bactria c. 180 BCE.[26][27] This is "the earliest unambiguous image" of the deity.[28]
The text is generally dated to the second or first century BCE,[3][4][5][6] though later (1st c. CE)[29] and earlier estimates (400-500 BCE)[30] have also been given, while 200 BCE may also be the date of a major revision.[31]

According to Jeaneane Fowler, "the dating of the Gita varies considerably" and depends in part on whether one accepts it to be a part of the early versions of the Mahabharata, or a text that was inserted into the epic at a later date.[32] The earliest "surviving" components therefore are believed to be no older than the earliest "external" references we have to the Mahabharata epic. The Mahabharata – the world's longest poem – is itself a text that was likely written and compiled over several hundred years, one dated between "400 BCE or little earlier, and 2nd century CE, though some claim a few parts can be put as late as 400 CE", states Fowler. The dating of the Gita is thus dependent on the uncertain dating of the Mahabharata. The actual dates of composition of the Gita remain unresolved.[32]

According to Arthur Basham, the context of the Bhagavad Gita suggests that it was composed in an era when the ethics of war were being questioned and renunciation of monastic life was becoming popular.[33] Such an era emerged after the rise of Buddhism and Jainism in the 5th century BCE, and particularly after the semi-legendary life of Ashoka in the 3rd century BCE. Thus, the first version of the Bhagavad Gita may have been composed in or after the 3rd century BCE.[33]

An old torn paper with a painting depicting the Mahabharata war, with some verses recorded in Sanskrit.
A manuscript illustration of the battle of Kurukshetra, fought between the Kauravas and the Pandavas, recorded in the Mahabharata. c. 1700 – c. 1800 CE
Winthrop Sargeant linguistically categorizes the Bhagavad Gita as Epic-Puranic Sanskrit, a language that succeeds Vedic Sanskrit and precedes classical Sanskrit.[34] The text has occasional pre-classical elements of the Vedic Sanskrit language, such as aorists and the prohibitive mā instead of the expected na (not) of classical Sanskrit.[34] This suggests that the text was composed after the Pāṇini era, but before the long compounds of classical Sanskrit became the norm. This would date the text as transmitted by the oral tradition to the later centuries of the 1st-millennium BCE, and the first written version probably to the 2nd or 3rd century CE.[34][35]

Kashi Nath Upadhyaya dates it a bit earlier, but after the rise of Buddhism, as the Mahabharata contains references to the Buddha and Buddhist references. He states that the Gita was always a part of the Mahabharata and that dating the latter suffices in dating the Gita.[36] Based on the estimated dates of the Mahabharata as evidenced by exact quotes of it in the Buddhist literature by Asvaghosa (c. 100 CE), Upadhyaya states that the Mahabharata, and therefore the Gita, must have been well known by then for a Buddhist to be quoting it.[36][note 1] This suggests a terminus ante quem (latest date) of the Gita sometime before the 1st century CE.[36] He cites similar quotes in the dharmasutra texts, the Brahma sutras, and other literature to conclude that the Bhagavad Gita was composed in the fifth or fourth century BCE.[30] The Indologist Étienne Lamotte used a similar analysis to conclude that the Gita in its current form likely underwent one redaction that occurred in the 3rd or 2nd-century BCE.[31]

Authorship
In the Indian tradition, the Bhagavad Gita, as well as the epic Mahabharata of which it is a part, is attributed to the sage Vyasa.[38] A Hindu legend narrates that Vyasa composed it, and Ganesha, who broke one of his tusks, used this tusk to write down the Mahabharata along with the Bhagavad Gita.[9][39][note 2]

Scholars consider Vyasa to be a mythical or symbolic author, in part because Vyasa is also a title or generic name for the compiler of a text, and Vyasa is also regarded by tradition as the compiler of the Vedas and the Puranas, texts dated with a time-difference of circa two millennia.[9][e]

According to Alexus McLeod, a scholar of Philosophy and Asian Studies, it is "impossible to link the Bhagavad Gita to a single author", and it may be the work of many authors.[9][10] This view is shared by the Indologist Arthur Basham, who states that there were three or more authors or compilers of Bhagavad Gita. This is evidenced by the discontinuous intermixing of philosophical verses with theistic or passionately theistic verses, according to Basham.[11][note 3]

J. A. B. van Buitenen, an Indologist known for his translations and scholarship on Mahabharata, finds that the Gita is so contextually and philosophically well-knit within the Mahabharata that it was not an independent text that "somehow wandered into the epic".[42] The Gita, states van Buitenen, was conceived and developed by the Mahabharata authors to "bring to a climax and solution the dharmic dilemma of a war".[42][note 4]

Vāsudeva-Krishna roots
According to Dennis Hudson, there is an overlap between Vedic and Tantric rituals within the teachings found in the Bhagavad Gita.[45] Dennis Hudson places the Pancaratra Agama in the last three or four centuries of 1st-millennium BCE, and proposes that both the tantric and vedic, the Agama and the Gita share the same Vāsudeva-Krishna roots.[46]

According to Hudson, a story in this Vedic text highlights the meaning of the name Vāsudeva as the 'shining one (deva) who dwells (Vasu) in all things and in whom all things dwell', and the meaning of Vishnu to be the 'pervading actor'. In the Bhagavad Gita, similarly, 'Krishna identified himself both with Vāsudeva, Vishnu and their meanings'.[47][note 5] The ideas at the centre of Vedic rituals in Shatapatha Brahmana and the teachings of the Bhagavad Gita revolve around this absolute Person, the primordial genderless absolute, which is the same as the goal of Pancaratra Agama and Tantra.[49]

Manuscripts and layout
Photograph of four pieces of paper with verses in Sanskrit.
A Sanskrit manuscript of the Bhagavad Gita in the Devanagari script. c. 1800 – c. 1900 CE
The Bhagavad Gita manuscript is found in the sixth book of the Mahabharata manuscripts – the Bhisma-parvan. Therein, in the third section, the Gita forms chapters 23–40, that is 6.3.23 to 6.3.40.[50] The Bhagavad Gita is often preserved and studied on its own, as an independent text with its chapters renumbered from 1 to 18.[50] The Bhagavad Gita manuscripts exist in numerous Indic scripts.[51] These include writing systems that are currently in use, as well as early scripts such as the now dormant Sharada script.[51][52] Variant manuscripts of the Gita have been found on the Indian subcontinent.[53][54] Unlike the enormous variations in the remaining sections of the surviving Mahabharata manuscripts, the Gita manuscripts show only minor variations.[53][54]

According to Gambhirananda, the old manuscripts may have had 745 verses, though he agrees that "700 verses is the generally accepted historic standard."[55] Gambhirananda's view is supported by a few versions of chapter 6.43 of the Mahabharata. According to Gita exegesis scholar Robert Minor, these versions state that the Gita is a text where "Kesava [Krishna] spoke 574 slokas, Arjuna 84, Sanjaya 41, and Dhritarashtra 1".[56] An authentic manuscript of the Gita with 745 verses has not been found.[57] Adi Shankara, in his 8th-century commentary, explicitly states that the Gita has 700 verses, which was likely a deliberate declaration to prevent further insertions and changes to the Gita. Since Shankara's time, "700 verses" has been the standard benchmark for the critical edition of the Bhagavad Gita.[57]

Structure
The Bhagavad Gita is a poem written in the Sanskrit language with 18 chapters in total.[58][59] The 700 verses[54] are structured into several ancient Indian poetic meters, with the principal being the Anushthubh chanda. Each shloka consists of a couplet, thus the entire text consists of 1,400 lines. Each shloka has two-quarter verses with exactly eight syllables. Each of these quarters is further arranged into two metrical feet of four syllables each.[58][note 6] The metered verse does not rhyme.[60] While the anushthubh chanda is the principal meter used, it does deploy other elements of Sanskrit prosody (which refers to one of the six Vedangas, or limbs of Vedic statues).[61] At dramatic moments, it uses the tristubh meter found in the Vedas, where each line of the couplet has two-quarter verses with exactly eleven syllables.[60]

Characters
Arjuna, one of the five Pandavas
Krishna, Arjuna's charioteer and guru who was actually an incarnation of Vishnu
Sanjaya, counselor of the Kuru king Dhritarashtra (secondary narrator)
Dhritarashtra, Kuru king (Sanjaya's audience) and father of the Kauravas


Sigmund Freud[a] (born Sigismund Schlomo Freud; 6 May 1856 – 23 September 1939) was an Austrian neurologist and the founder of psychoanalysis, a clinical method for evaluating and treating pathologies seen as originating from conflicts in the psyche, through dialogue between patient and psychoanalyst,[3] and the distinctive theory of mind and human agency derived from it.[4]

Freud was born to Galician Jewish parents in the Moravian town of Freiberg, in the Austrian Empire. He qualified as a doctor of medicine in 1881 at the University of Vienna.[5][6] Upon completing his habilitation in 1885, he was appointed a docent in neuropathology and became an affiliated professor in 1902.[7] Freud lived and worked in Vienna, having set up his clinical practice there in 1886. Following the German annexation of Austria in March 1938, Freud left Austria to escape Nazi persecution. He died in exile in the United Kingdom in September 1939.

In founding psychoanalysis, Freud developed therapeutic techniques such as the use of free association, and he established the central role of transference in the analytic process. Freud's redefinition of sexuality to include its infantile forms led him to formulate the Oedipus complex as the central tenet of psychoanalytical theory.[8] His analysis of dreams as wish fulfillments provided him with models for the clinical analysis of symptom formation and the underlying mechanisms of repression. On this basis, Freud elaborated his theory of the unconscious and went on to develop a model of psychic structure comprising id, ego, and superego.[9] Freud postulated the existence of libido, sexualised energy with which mental processes and structures are invested and that generates erotic attachments and a death drive, the source of compulsive repetition, hate, aggression, and neurotic guilt.[9] In his later work, Freud developed a wide-ranging interpretation and critique of religion and culture.

Though in overall decline as a diagnostic and clinical practice, psychoanalysis remains influential within psychology, psychiatry, psychotherapy, and across the humanities. It thus continues to generate extensive and highly contested debate concerning its therapeutic efficacy, its scientific status, and whether it advances or hinders the feminist cause.[10] Nonetheless, Freud's work has suffused contemporary Western thought and popular culture. W. H. Auden's 1940 poetic tribute to Freud describes him as having created "a whole climate of opinion / under whom we conduct our different lives".[11]

Biography
Early life and education
photograph
Freud's birthplace, a rented room in a locksmith's house, Freiberg, Austrian Empire (Příbor, Czech Republic)
Sigmund Freud was born to Ashkenazi Jewish parents in the Moravian town of Freiberg,[12][13] in the Austrian Empire (now Příbor, Czech Republic), the first of eight children.[14] Both of his parents were from Galicia. His father, Jakob Freud, a wool merchant, had two sons, Emanuel and Philipp, by his first marriage. Jakob's family were Hasidic Jews and, although Jakob himself had moved away from the tradition, he came to be known for his Torah study. He and Freud's mother, Amalia Nathansohn, who was 20 years younger and his third wife, were married by Rabbi Isaac Noah Mannheimer on 29 July 1855.[15] They were struggling financially and living in a rented room, in a locksmith's house at Schlossergasse 117 when their son Sigmund was born.[16] He was born with a caul, which his mother saw as a positive omen for the boy's future.[17]


Freud (aged 16) and his mother, Amalia, in 1872
In 1859, the Freud family left Freiberg. Freud's half-brothers immigrated to Manchester, England, parting him from the "inseparable" playmate of his early childhood, Emanuel's son, John.[18] Jakob Freud took his wife and two children (Freud's sister, Anna, was born in 1858; a brother, Julius born in 1857, had died in infancy) firstly to Leipzig and then in 1860 to Vienna where four sisters and a brother were born: Rosa (b. 1860), Marie (b. 1861), Adolfine (b. 1862), Paula (b. 1864), Alexander (b. 1866). In 1865, the nine-year-old Freud entered the Leopoldstädter Kommunal-Realgymnasium, a prominent high school. He proved to be an outstanding pupil and graduated from the Matura in 1873 with honors. He loved literature and was proficient in German, French, Italian, Spanish, English, Hebrew, Latin and Greek.[19]

Freud entered the University of Vienna at age 17. He had planned to study law, but joined the medical faculty at the university, where his studies included philosophy under Franz Brentano, physiology under Ernst Brücke, and zoology under Darwinist professor Carl Claus.[20] In 1876, Freud spent four weeks at Claus's zoological research station in Trieste, dissecting hundreds of eels in an inconclusive search for their male reproductive organs.[21] In 1877, Freud moved to Ernst Brücke's physiology laboratory, where he spent six years comparing the brains of humans with those of other vertebrates such as frogs, lampreys, as well as invertebrates, for example, crayfish. His research work on the biology of nervous tissue proved seminal for the subsequent discovery of the neuron in the 1890s.[22] Freud's research work was interrupted in 1879 by the obligation to undertake a year's compulsory military service. The lengthy downtimes enabled him to complete a commission to translate four essays from John Stuart Mill's collected works.[23] He graduated with an MD in March 1881.[24]

Early career and marriage
In 1882, Freud began his medical career at Vienna General Hospital. His research work in cerebral anatomy led to the publication in 1884 of an influential paper on the palliative effects of cocaine, and his work on aphasia would form the basis of his first book On Aphasia: A Critical Study, published in 1891.[25] Over three years, Freud worked in various departments of the hospital. His time spent in Theodor Meynert's psychiatric clinic and as a locum in a local asylum led to an increased interest in clinical work. His substantial body of published research led to his appointment as a university lecturer or docent in neuropathology in 1885, a non-salaried post but one that entitled him to give lectures at the University of Vienna.[26]

In 1886, Freud resigned his hospital post and entered private practice, specializing in "nervous disorders". The same year, he married Martha Bernays, the granddaughter of Isaac Bernays, a chief rabbi in Hamburg. Freud was, as an atheist, dismayed at the requirement in Austria for a Jewish religious ceremony and briefly considered, before dismissing, the prospect of joining the Protestant 'Confession' to avoid one.[27] A civil ceremony for Bernays and Freud took place on 13 September, and a religious ceremony took place the following day, with Freud having been hastily tutored in the Hebrew prayers.[28] The Freuds had six children: Mathilde (b. 1887), Jean-Martin (b. 1889), Oliver (b. 1891), Ernst (b. 1892), Sophie (b. 1893), and Anna (b. 1895). From 1891 until they left Vienna in 1938, Freud and his family lived in an apartment at Berggasse 19, near Innere Stadt.

On 8 December 1897, Freud was initiated into the German Jewish cultural association B'nai B'rith, to which he remained linked all his life. Freud gave a speech on the interpretation of dreams, which had an enthusiastic reception. It anticipated the book of the same name, which was published two years later.[29][30][31]

photograph
Freud's home at Berggasse 19, Vienna
In 1896, Minna Bernays, Martha Freud's sister, became a permanent member of the Freud household after the death of her fiancé. "[A] paper published in 1969 by John M. Billinsky contained the transcript of a conversation he had with [Carl] Jung in 1957, in which Jung confided that ... Minna herself had told him about her affair with Freud".[32][33][34] The discovery of a Swiss hotel guest book entry for 13 August 1898, signed by Freud whilst travelling with his sister-in-law, has been presented as evidence of the affair.[35]

Freud began smoking tobacco at age 24; initially a cigarette smoker, he became a cigar smoker.[36] He believed smoking enhanced his capacity to work and that he could exercise self-control in moderating it. Despite health warnings from colleague Wilhelm Fliess, he remained a smoker, eventually developing buccal cancer.[37] Freud suggested to Fliess in 1897 that addictions, including that to tobacco, were substitutes for masturbation, "the one great habit."[38]

Freud had greatly admired his philosophy tutor, Franz Brentano, who was known for his theories of perception and introspection. Brentano discussed the possible existence of the unconscious mind in his Psychology from an Empirical Standpoint (1874). Although Brentano denied its existence, his discussion of the unconscious probably helped introduce Freud to the concept.[39] Freud owned and made use of Charles Darwin's major evolutionary writings and was also influenced by Eduard von Hartmann's The Philosophy of the Unconscious (1869). Other texts of importance to Freud were by Fechner and Herbart,[40] with the latter's Psychology as Science perhaps of underrated significance in this respect.[41] Freud also drew on the work of Theodor Lipps, who was one of the main contemporary theorists of the concepts of the unconscious and empathy.[42]

Though Freud was reluctant to associate his psychoanalytic insights with prior philosophical theories, attention has been drawn to similarities between his work and that of both Schopenhauer[43] and Nietzsche. In 1908, Freud said that he occasionally read Nietzsche and was strongly fascinated by his writings, but did not study him, because he found Nietzsche's "intuitive insights" resembled his own work at the time too much, and also because he was overwhelmed by the "wealth of ideas" he encountered when he read Nietzsche. Freud sometimes would deny the influence of Nietzsche's ideas. One historian quotes Peter L. Rudnytsky, who says that based on Freud's correspondence with his adolescent friend Eduard Silberstein, Freud read Nietzsche's The Birth of Tragedy and probably the first two of the Untimely Meditations when he was seventeen.[44][45] Freud bought Nietzsche's collected works in 1900, telling Wilhelm Fliess that he hoped to find in Nietzsche's works "the words for much that remains mute in me." Later, however, he said he had not yet opened them.[46] Freud came to treat Nietzsche's writings, according to Peter Gay, "as texts to be resisted far more than to be studied."[47] His interest in philosophy declined after he decided on a career in neurology.[48]

Freud read William Shakespeare in English; his understanding of human psychology may have been influenced by Shakespeare's plays.[49]

Freud's Jewish origins and his allegiance to his secular Jewish identity were significant in the formation of his intellectual and moral outlook, especially concerning his intellectual non-conformism, as he pointed out in his Autobiographical Study.[50] They would also have a substantial effect on the content of psychoanalytic ideas, particularly in respect of their common concerns with depth interpretation and "the bounding of desire by law".[51]

Relationship with Fliess
See also: Metapsychology § Freud and the als ob problem
During the formative period of his work, Freud valued and came to rely on the intellectual and emotional support of his friend Wilhelm Fliess, a Berlin-based ear, nose, and throat specialist whom he had first met in 1887. Both men saw themselves as isolated from the prevailing clinical and theoretical mainstream because of their ambitions to develop radical new theories of sexuality. Fliess developed highly eccentric theories of human biorhythms and a nasogenital connection which are today considered pseudoscientific.[52] He shared Freud's views on the importance of certain aspects of sexuality – masturbation, coitus interruptus, and the use of condoms – in the etiology of what was then called the "actual neuroses," primarily neurasthenia and certain physically manifested anxiety symptoms.[53] They maintained an extensive correspondence from which Freud drew on Fliess's speculations on infantile sexuality and bisexuality to elaborate and revise his own ideas. His first attempt at a systematic theory of the mind, his Project for a Scientific Psychology, was developed as a metapsychology with Fliess as interlocutor.[54] However, Freud's efforts to build a bridge between neurology and psychology were eventually abandoned after they had reached an impasse, as his letters to Fliess reveal,[55] though some ideas of the Project were to be taken up again in the concluding chapter of The Interpretation of Dreams.[56]

Freud had Fliess repeatedly operate on his nose and sinuses to treat "nasal reflex neurosis",[57] and subsequently referred his patient Emma Eckstein to him. According to Freud, her history of symptoms included severe leg pains with consequent restricted mobility, as well as stomach and menstrual pains. These pains were, according to Fliess's theories, caused by habitual masturbation, which, as the tissue of the nose and genitalia were linked, was curable by removal of part of the middle turbinate.[58][59] Fliess's surgery proved disastrous, resulting in profuse, recurrent nasal bleeding; he had left a half-metre of gauze in Eckstein's nasal cavity, whose subsequent removal left her permanently disfigured. At first, though aware of Fliess's culpability and regarding the remedial surgery in horror, Freud could bring himself only to intimate delicately in his correspondence with Fliess the nature of his disastrous role, and in subsequent letters maintained a tactful silence on the matter or else returned to the face-saving topic of Eckstein's hysteria. Freud ultimately, in light of Eckstein's history of adolescent self-cutting and irregular nasal (and menstrual) bleeding, concluded that Fliess was "completely without blame", as Eckstein's post-operative haemorrhages were hysterical "wish-bleedings" linked to "an old wish to be loved in her illness" and triggered as a means of "rearousing [Freud's] affection". Eckstein nonetheless continued her analysis with Freud. She was restored to full mobility and went on to practice psychoanalysis herself.[60][61][58]

Freud, who had called Fliess "the Kepler of biology", later concluded that a combination of a homoerotic attachment and the residue of his "specifically Jewish mysticism" lay behind his loyalty to his Jewish friend and his consequent overestimation of both his theoretical and clinical work. Their friendship came to an acrimonious end with Fliess angry at Freud's unwillingness to endorse his general theory of sexual periodicity and accusing him of collusion in the plagiarism of his work. After Fliess failed to respond to Freud's offer of collaboration over the publication of his Three Essays on the Theory of Sexuality in 1906, their relationship came to an end.[62]

Development of psychoanalysis

André Brouillet's A Clinical Lesson at the Salpêtrière (1887) depicting a Charcot demonstration. Freud had a lithograph of this painting placed over the couch in his consulting rooms.[63]
In October 1885, Freud went to Paris on a three-month fellowship to study with Jean-Martin Charcot, a renowned neurologist who was conducting scientific research into hypnosis. He was later to recall the experience of this stay as catalytic in turning him toward the practice of medical psychopathology and away from a less financially promising career in neurology research.[64] Charcot specialized in the study of hysteria and susceptibility to hypnosis, which he frequently demonstrated with patients on stage in front of an audience.

Once he had set up in private practice in Vienna in 1886, Freud began using hypnosis in his clinical work. He adopted the approach of his friend and collaborator, Josef Breuer, in a type of hypnosis that was different from the French methods he had studied, in that it did not use suggestion. The treatment of one particular patient of Breuer's proved to be transformative for Freud's clinical practice. Described as Anna O., she was invited to talk about her symptoms while under hypnosis (she would coin the phrase "talking cure"). Her symptoms became reduced in severity as she retrieved memories of traumatic incidents associated with their onset.

The inconsistent results of Freud's early clinical work eventually led him to abandon hypnosis, having concluded that more consistent and effective symptom relief could be achieved by encouraging patients to talk freely, without censorship or inhibition, about whatever ideas or memories occurred to them. He called this procedure "free association". In conjunction with this, Freud found that patients' dreams could be fruitfully analyzed to reveal the complex structuring of unconscious material and to demonstrate the psychic action of repression which, he had concluded, underlay symptom formation. By 1896, he was using the term "psychoanalysis" to refer to his new clinical method and the theories on which it was based.[65]

Ornate staircase, a landing with an interior door and window, staircase continuing up
Approach to Freud's consulting rooms at Berggasse 19
Freud's development of these new theories took place during a period in which he experienced heart irregularities, disturbing dreams, and periods of depression — a "neurasthenia" that he linked to the death of his father in 1896 and that prompted a "self-analysis" of his own dreams and memories of childhood. His explorations of his feelings of hostility to his father and rivalrous jealousy over his mother's affections led him to fundamentally revise his theory of the origin of the neuroses.[66]

Based on his early clinical work, Freud postulated that unconscious memories of sexual molestation in early childhood were a necessary precondition for psychoneuroses (hysteria and obsessional neurosis), a formulation now known as Freud's seduction theory.[67] In the light of his self-analysis, Freud abandoned the theory that every neurosis can be traced back to the effects of infantile sexual abuse, now arguing that infantile sexual scenarios still had a causative function. It did not matter whether they were real or imagined, and in either case, they became pathogenic only when acting as repressed memories.[68]

This transition from the theory of infantile sexual trauma as a general explanation of how all neuroses originate to one that presupposes autonomous infantile sexuality provided the basis for Freud's subsequent formulation of the theory of the Oedipus complex.[69]

Freud described the evolution of his clinical method and set out his theory of the psychogenetic origins of hysteria, demonstrated in several case histories, in Studies on Hysteria published in 1895 (co-authored with Josef Breuer). In 1899, he published The Interpretation of Dreams in which, following a critical review of existing theory, Freud gives detailed interpretations of his own and his patients' dreams in terms of wish-fulfillments made subject to the repression and censorship of the "dream-work". He then sets out the theoretical model of mental structure (the unconscious, pre-conscious, and conscious) on which this account is based. An abridged version, On Dreams, was published in 1901. In works that would win him a more general readership, Freud applied his theories outside the clinical setting in The Psychopathology of Everyday Life (1901) and Jokes and their Relation to the Unconscious (1905).[70] In Three Essays on the Theory of Sexuality, published in 1905, Freud elaborates his theory of infantile sexuality, describing its "polymorphous perverse" forms and the functioning of the "drives", to which it gives rise, in the formation of sexual identity.[71] The same year he published Fragment of an Analysis of a Case of Hysteria, which became one of his more famous and controversial case studies.[72] Known as the 'Dora' case study, for Freud it was illustrative of hysteria as a symptom and contributed to his understanding of the importance of transference as a clinical phenomenon. In other early case studies, Freud described the symptomatology of obsessional neurosis in the case of the Rat man, and phobia in the case of Little Hans.[73]

Early followers
In 1902, Freud, at last, realised his long-standing ambition to be made a university professor. The title "professor extraordinarius"[74] was important to Freud for the recognition and prestige it conferred, there being no salary or teaching duties attached to the post (he would be granted the enhanced status of "professor ordinarius" in 1920).[75] Despite support from the university, his appointment had been blocked in successive years by the political authorities and it was secured only with the intervention of an influential ex-patient, Baroness Marie Ferstel, who (supposedly) had to bribe the minister of education with a valuable painting.[76]

Freud continued with the regular series of lectures on his work, which, since the mid-1880s as a docent of Vienna University, he had been delivering to small audiences every Saturday evening at the lecture hall of the university's psychiatric clinic.[77] From the autumn of 1902, several Viennese physicians who had expressed interest in Freud's work were invited to meet at his apartment every Wednesday afternoon to discuss issues relating to psychology and neuropathology.[78] This group was called the Wednesday Psychological Society (Psychologische Mittwochs-Gesellschaft) and it marked the beginnings of the worldwide psychoanalytic movement.[79]

Freud founded this discussion group at the suggestion of the physician Wilhelm Stekel. Stekel had studied medicine; his conversion to psychoanalysis is variously attributed to his successful treatment by Freud for a sexual problem or as a result of his reading The Interpretation of Dreams, to which he subsequently gave a positive review in the Viennese daily newspaper Neues Wiener Tagblatt.[80] The other three original members whom Freud invited to attend, Alfred Adler, Max Kahane, and Rudolf Reitler, were also physicians[81] and all five were Jewish by birth.[82] Both Kahane and Reitler were childhood friends of Freud who had gone to university with him and kept abreast of Freud's developing ideas by attending his Saturday evening lectures.[83] In 1901, Kahane, who first introduced Stekel to Freud's work,[77] had opened an out-patient psychotherapy institute of which he was the director in Vienna.[78] In the same year, his medical textbook, Outline of Internal Medicine for Students and Practicing Physicians, was published. In it, he provided an outline of Freud's psychoanalytic method.[77] Kahane broke with Freud and left the Wednesday Psychological Society in 1907 for unknown reasons, and in 1923, he died by suicide.[84] Reitler was the director of an establishment providing thermal cures in Dorotheergasse, which had been founded in 1901.[78] He died prematurely in 1917. Adler, regarded as the most formidable intellect among the early Freud circle, was a socialist who, in 1898, had written a health manual for the tailoring trade. He was particularly interested in the potential social impact of psychiatry.[85]

Time is the continuous progression of existence that occurs in an apparently irreversible succession from the past, through the present, and into the future.[1][2][3] Time dictates all forms of action, age, and causality, being a component quantity of various measurements used to sequence events, to compare the duration of events (or the intervals between them), and to quantify rates of change of quantities in material reality or in the conscious experience.[4][5][6][7] Time is often referred to as a fourth dimension, along with three spatial dimensions.[8]

Time is primarily measured in linear spans or periods, ordered from shortest to longest. Practical, human-scale measurements of time are performed using clocks and calendars, reflecting a 24-hour day collected into a 365-day year linked to the astronomical motion of the Earth. Scientific measurements of time instead vary from Planck time at the shortest to billions of years at the longest. Measurable time is believed to have effectively begun with the Big Bang 13.8 billion years ago, encompassed by the chronology of the universe. Modern physics understands time to be inextricable from space within the concept of spacetime described by general relativity.[9] Time can therefore be dilated by velocity and matter to pass faster or slower for an external observer, though this is considered negligible outside of extreme conditions, namely relativistic speeds or the gravitational pulls of black holes.

Throughout history, time has been an important subject of study in religion, philosophy, and science. Temporal measurement has occupied scientists and technologists, and has been a prime motivation in navigation and astronomy. Time is also of significant social importance, having economic value ("time is money") as well as personal value, due to an awareness of the limited time in each day ("carpe diem") and in human life spans.

Definition
The concept of time can be complex. Multiple notions exist, and defining time in a manner applicable to all fields without circularity has consistently eluded scholars.[7][10][11] Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems.[12][13][14] Traditional definitions of time involved the observation of periodic motion such as the apparent motion of the sun across the sky, the phases of the moon, and the passage of a free-swinging pendulum. More modern systems include the Global Positioning System, other satellite systems, Coordinated Universal Time and mean solar time. Although these systems differ from one another, with careful measurements they can be synchronized.

In physics, time is a fundamental concept to define other quantities, such as velocity. To avoid a circular definition,[15] time in physics is operationally defined as "what a clock reads", specifically a count of repeating events such as the SI second.[6][16][17] Although this aids in practical measurements, it does not address the essence of time. Physicists developed the concept of the spacetime continuum, where events are assigned four coordinates: three for space and one for time. Events like particle collisions, supernovas, or rocket launches have coordinates that may vary for different observers, making concepts like "now" and "here" relative. In general relativity, these coordinates do not directly correspond to the causal structure of events. Instead, the spacetime interval is calculated and classified as either space-like or time-like, depending on whether an observer exists that would say the events are separated by space or by time.[18] Since the time required for light to travel a specific distance is the same for all observers—a fact first publicly demonstrated by the Michelson–Morley experiment—all observers will consistently agree on this definition of time as a causal relation.[19]

General relativity does not address the nature of time for extremely small intervals where quantum mechanics holds. In quantum mechanics, time is treated as a universal and absolute parameter, differing from general relativity's notion of independent clocks. The problem of time consists of reconciling these two theories.[20] As of 2025, there is no generally accepted theory of quantum general relativity.[21]


World War I,[b] or the First World War (28 July 1914 – 11 November 1918), also known as the Great War, was a global conflict between two coalitions: the Allies (or Entente) and the Central Powers. Major areas of conflict included Europe and the Middle East, as well as parts of Africa and the Asia-Pacific. The war saw important developments in weaponry including tanks, aircraft, artillery, machine guns, and chemical weapons. One of the deadliest conflicts in history, it resulted in an estimated 30 million military casualties, and 8 million civilian deaths from war-related causes and genocide. The movement of large numbers of people was a major factor in the deadly Spanish flu pandemic.

The causes of World War I included the rise of the German Empire and decline of the Ottoman Empire, which disturbed the long-standing balance of power in Europe, the exacerbation of imperial rivalries, and an arms race between the great powers. Growing tensions in the Balkans reached a breaking point on 28 June 1914 when Gavrilo Princip, a Bosnian Serb, assassinated Franz Ferdinand, the heir to the Austro-Hungarian throne. Austria-Hungary blamed Serbia, and declared war on 28 July. After Russia mobilised in Serbia's defence, Germany declared war on Russia and France, who had an alliance. The United Kingdom entered the war after Germany invaded Belgium, and the Ottoman Empire joined the Central Powers in November. Germany's strategy in 1914 was to quickly defeat France before transferring its forces to the east, but its advance was halted in September, and by the end of the year the Western Front consisted of a near-continuous line of trenches from the English Channel to Switzerland. The Eastern Front was more dynamic, but neither side gained a decisive advantage, despite costly offensives. Italy, Bulgaria, Romania, Greece and others entered the war from 1915 onward.

Major battles, including those at Verdun, the Somme, and Passchendaele, failed to break the stalemate on the Western Front. In April 1917, the United States joined the Allies after Germany resumed unrestricted submarine warfare against Atlantic shipping. Later that year, the Bolsheviks seized power in Russia in the October Revolution; Soviet Russia signed an armistice with the Central Powers in December, followed by a separate peace in March 1918. That month, Germany launched a spring offensive in the west, which despite initial successes left the German Army exhausted and demoralised. The Allied Hundred Days Offensive, beginning in August 1918, caused a collapse of the German front line. Following the Vardar Offensive, Bulgaria signed an armistice in late September. By early November, the Allies had signed armistices with the Ottomans and with Austria-Hungary, leaving Germany isolated. Facing a revolution at home, Kaiser Wilhelm II abdicated on 9 November, and the war ended with the Armistice of 11 November 1918.

The Paris Peace Conference of 1919–1920 imposed settlements on the defeated powers. Under the Treaty of Versailles, Germany lost significant territories, was disarmed, and was required to pay large war reparations to the Allies. The dissolution of the Russian, German, Austro-Hungarian, and Ottoman empires led to new national boundaries and the creation of new independent states including Poland, Finland, the Baltic states, Czechoslovakia, and Yugoslavia. The League of Nations was established to maintain world peace, but its failure to manage instability during the interwar period contributed to the outbreak of World War II in 1939.

Names
Before World War II, the events of 1914–1918 were generally known as the Great War or simply the World War.[1] In August 1914, the magazine The Independent wrote "This is the Great War. It names itself."[2] In October 1914, the Canadian magazine Maclean's similarly wrote, "Some wars name themselves. This is the Great War."[3] Contemporary Europeans also referred to it as "the war to end war" and "the war to end all wars" due to their perception of its unparalleled scale, devastation, and loss of life.[4] The first recorded use of the term First World War was in September 1914 by German biologist and philosopher Ernst Haeckel who stated, "There is no doubt that the course and character of the feared 'European War' ... will become the first world war in the full sense of the word."[5]

Background
Main article: Causes of World War I
Political and military alliances
Map of Europe focusing on Austria-Hungary and marking the central location of ethnic groups in it including Slovaks, Czechs, Slovenes, Croats, Serbs, Romanians, Ukrainians, Poles.
Rival military coalitions in 1914:[c]
  Triple Entente
  Triple Alliance
For much of the 19th century, the major European powers maintained a tenuous balance of power, known as the Concert of Europe.[6] After 1848, this was challenged by Britain's withdrawal into so-called splendid isolation, the decline of the Ottoman Empire, New Imperialism, and the rise of Prussia under Otto von Bismarck. Victory in the 1870–1871 Franco-Prussian War allowed Bismarck to consolidate a German Empire. Post-1871, the primary aim of French policy was to avenge this defeat,[7][8] but by the early 1890s, this had switched to the expansion of the French colonial empire.[9]

In 1873, Bismarck negotiated the League of the Three Emperors, which included Austria-Hungary, Russia, and Germany. After the 1877–1878 Russo-Turkish War, the League was dissolved due to Austrian concerns over the expansion of Russian influence in the Balkans, an area they considered to be of vital strategic interest. Germany and Austria-Hungary then formed the 1879 Dual Alliance, which became the Triple Alliance when Italy joined in 1882.[10] For Bismarck, the purpose of these agreements was to isolate France by ensuring the three empires resolved any disputes among themselves. In 1887, Bismarck set up the Reinsurance Treaty, a secret agreement between Germany and Russia to remain neutral if either were attacked by France or Austria-Hungary.[11]


World empires and colonies c. 1914
For Bismarck, peace with Russia was the foundation of German foreign policy, but in 1890, he was forced to retire by Wilhelm II. The latter was persuaded not to renew the Reinsurance Treaty by his new Chancellor, Leo von Caprivi.[12] This gave France an opening to agree to the Franco-Russian Alliance in 1894, which was then followed by the 1904 Entente Cordiale with Britain. The Triple Entente was completed by the 1907 Anglo-Russian Convention. While not formal alliances, by settling longstanding colonial disputes in Asia and Africa, British support for France or Russia in any future conflict became a possibility.[13] This was accentuated by British and Russian support for France against Germany during the 1911 Agadir Crisis.[14]

World War II[b] or the Second World War (1 September 1939 – 2 September 1945) was a global conflict between two coalitions: the Allies and the Axis powers. Nearly all of the world's countries participated, with many nations mobilising all resources in pursuit of total war. Tanks and aircraft played major roles, enabling the strategic bombing of cities and delivery of the first and only nuclear weapons ever used in war. World War II is the deadliest conflict in history, causing the death of 70 to 85 million people, more than half of whom were civilians. Millions died in genocides, including the Holocaust, and by massacres, starvation, and disease. After the Allied victory, Germany, Austria, Japan, and Korea were occupied, and German and Japanese leaders were put on trial for war crimes.

The causes of World War II included unresolved tensions in the aftermath of World War I, the rise of fascism in Europe and militarism in Japan. Key events preceding the war included Japan's invasion of Manchuria in 1931, the Spanish Civil War, the outbreak of the Second Sino-Japanese War in 1937, and Germany's annexations of Austria and the Sudetenland. World War II is generally considered to have begun on 1 September 1939, when Nazi Germany, under Adolf Hitler, invaded Poland, after which the United Kingdom and France declared war on Germany. Poland was divided between Germany and the Soviet Union under the Molotov–Ribbentrop Pact. In 1940, the Soviet Union annexed the Baltic states and parts of Finland and Romania. After the fall of France in June 1940, the war continued mainly between Germany, now assisted by Fascist Italy, and the British Empire, with fighting in the Balkans, Mediterranean, and Middle East, the aerial Battle of Britain and the Blitz, and the naval Battle of the Atlantic. Through campaigns and treaties, Germany gained control of much of continental Europe and formed the Axis alliance with Italy, Japan, and other countries. In June 1941, Germany invaded the Soviet Union, opening the Eastern Front and initially making large territorial gains.

In December 1941, Japan attacked American and British territories in Asia and the Pacific, including at Pearl Harbor in Hawaii, leading the United States to enter the war against Japan and Germany. Japan conquered much of coastal China and Southeast Asia, but its advances in the Pacific were halted in June 1942 at the Battle of Midway. In early 1943, Axis forces were defeated in North Africa and at Stalingrad in the Soviet Union, and that year their continued defeats on the Eastern Front, an Allied invasion of Sicily and mainland Italy that resulted in the fall of Fascist Italy; and Allied offensives in the Pacific forced them into retreat on all fronts. In 1944, the Western Allies invaded France at Normandy, as the Soviet Union recaptured its pre-war territory and the US crippled Japan's navy and captured key Pacific islands. The war in Europe concluded with the liberation of German-occupied territories; invasions of Germany by the Western Allies and the Soviet Union, which culminated in the fall of Berlin to Soviet troops; and Germany's unconditional surrender on 8 May 1945. On 6 and 9 August, the US dropped atomic bombs on Hiroshima and Nagasaki in Japan. Faced with an imminent Allied invasion, the prospect of further atomic bombings, and a Soviet declaration of war and invasion of Manchuria, Japan announced its unconditional surrender on 15 August, and signed a surrender document on 2 September 1945.

World War II transformed the political, economic, and social structures of the world, and established the foundation of international relations for the rest of the 20th century and into the 21st century. The United Nations was created to foster international cooperation and prevent future conflicts, with the victorious great powers—China, France, the Soviet Union, the UK, and the US—becoming the permanent members of its security council. The Soviet Union and the US emerged as rival superpowers, setting the stage for the half-century Cold War. In the wake of Europe's devastation, the influence of its great powers waned, triggering the decolonisation of Africa and of Asia. Many countries whose industries had been damaged moved towards economic recovery and expansion.

Start and end dates
See also: List of timelines of World War II
Timelines of World War II
Chronological
Prelude
Events (in Asiain Europe)
Aftermath
1939194019411942
194319441945Aftermath
By topic
Causes (Diplomacy)
Declarations of war
BattlesOperations
By theatre
Battle of Europe air operations
Eastern FrontManhattan Project
United Kingdom home front
Surrender of the Axis armies
vte
Most historians agree that World War II began with the German invasion of Poland on 1 September 1939[1][2] and the United Kingdom and France's declaration of war on Germany two days later. Dates for the beginning of the Pacific War include the start of the Second Sino-Japanese War on 7 July 1937,[3][4] or the earlier Japanese invasion of Manchuria, on 19 September 1931.[5][6] Others follow the British historian A. J. P. Taylor, who stated that the Sino-Japanese War and war in Europe and its colonies occurred simultaneously, and the two wars became World War II in 1941.[7] Other proposed starting dates for World War II include the Italian invasion of Abyssinia on 3 October 1935.[8] The British historian Antony Beevor views the beginning of World War II as the Battles of Khalkhin Gol fought between Japan and the forces of Mongolia and the Soviet Union from May to September 1939.[9] Others view the Spanish Civil War as the start or prelude to World War II.[10][11]

The exact date of the war's end is also not universally agreed upon. It was generally accepted at the time that the war ended with the armistice of 15 August 1945 (V-J Day), rather than with the formal surrender of Japan on 2 September 1945, which officially ended the war in Asia. A peace treaty between Japan and the Allies was signed in 1951.[12] A 1990 treaty regarding Germany's future allowed the reunification of East and West Germany to take place.[13] No formal peace treaty between Japan and the Soviet Union was ever signed,[14] although the state of war between the two countries was terminated by the Soviet–Japanese Joint Declaration of 1956, which also restored full diplomatic relations between them.[15]


Adolf Hitler[a] (20 April 1889 – 30 April 1945) was an Austrian-born German politician who was the dictator of Germany during the Nazi era, which lasted from 1933 until his suicide in 1945. He rose to power as the leader of the Nazi Party,[b] becoming the chancellor of Germany in 1933 and then taking the title of Führer und Reichskanzler in 1934.[c] Germany's invasion of Poland on 1 September 1939 under his leadership marked the outbreak of the Second World War. Throughout the ensuing conflict, Hitler was closely involved in the direction of German military operations as well as the perpetration of the Holocaust, the genocide of about six million Jews and millions of other victims.

Hitler was born in Braunau am Inn in Austria-Hungary and moved to Germany in 1913. He was decorated during his service in the German Army in the First World War, receiving the Iron Cross. In 1919, he joined the German Workers' Party (DAP), the precursor of the Nazi Party, and in 1921, was appointed the leader of the Nazi Party. In 1923, he attempted to seize governmental power in a failed coup in Munich and was sentenced to five years in prison, serving just over a year. While there, he dictated the first volume of his autobiography and political manifesto Mein Kampf (lit. 'My Struggle'). After his early release in 1924, he gained popular support by attacking the Treaty of Versailles as well as promoting pan-Germanism, antisemitism, and anti-communism with charismatic oratory and Nazi propaganda. He frequently denounced communism as being part of an international Jewish conspiracy. By November 1932, the Nazi Party held the most seats in the Reichstag, but not a majority. Former chancellor Franz von Papen and other conservative politicians convinced President Paul von Hindenburg to appoint Hitler as chancellor on 30 January 1933. Shortly thereafter on 23 March, the Reichstag passed the Enabling Act of 1933, which ultimately began the Weimar Republic's transformation into Nazi Germany.

Upon Hindenburg's death on 2 August 1934, Hitler replaced him as head of state and thereafter transformed Germany into a totalitarian dictatorship. Domestically, Hitler implemented numerous racist policies and sought to deport or kill German Jews. His first six years in power resulted in rapid economic recovery from the Great Depression, the abrogation of restrictions imposed on Germany after the First World War, and the annexation of territories inhabited by millions of ethnic Germans, which initially gave him significant popular support. One of Hitler's key goals was Lebensraum (lit. 'living space') for the German people in Eastern Europe, and his aggressive, expansionist foreign policy is considered the primary cause of World War II in Europe. On 1 September 1939, Hitler oversaw the German invasion of Poland, thereby causing Britain and France to declare war on Germany. After ordering an invasion of the Soviet Union in June 1941, he declared war on the United States in December of the same year. By the end of 1941, German forces and the European Axis powers occupied most of Europe and North Africa. These gains were gradually reversed after 1941 until the Allied forces defeated the German military in 1945. On 29 April 1945, Hitler married his longtime partner, Eva Braun, in the Führerbunker in Berlin. They committed suicide the next day to avoid capture by the Soviet Red Army.

The historian and biographer Ian Kershaw described Hitler as "the embodiment of modern political evil".[3] Under Hitler's leadership and racist ideology, the Nazi regime was responsible for the genocide of an estimated six million Jews and millions of other victims, whom he and his followers deemed Untermenschen (lit. 'subhumans') or socially undesirable. Hitler and the Nazis were also responsible for the deliberate killing of an estimated 19.3 million civilians and prisoners of war. In addition, 28.7 million soldiers and civilians died as a result of military action in the European theatre. The number of civilians killed during World War II was unprecedented in warfare, and the casualties make it the deadliest conflict in history.

The Upanishads (/ʊˈpʌnɪʃʌdz/;[1] Sanskrit: उपनिषद्, IAST: Upaniṣad, pronounced [ˈupɐniʂɐd]) are Sanskrit texts of the late Vedic and post-Vedic periods that "document the transition from the archaic ritualism of the Veda into new religious ideas and institutions"[2] and the emergence of the central religious concepts of Hinduism.[2][note 1] They are the most recent addition to the Vedas, the oldest scriptures of Hinduism, and deal with meditation, philosophy, consciousness, and ontological knowledge. Earlier parts of the Vedas dealt with mantras, benedictions, rituals, ceremonies, and sacrifices.[3][4][5]

While among the most important literature in the history of Indian religions and culture, the Upanishads document a wide variety of "rites, incantations, and esoteric knowledge"[6] departing from Vedic ritualism and interpreted in various ways in the later commentarial traditions. The Upanishads are widely known, and their diverse ideas, interpreted in various ways, informed later traditions of Hinduism.[note 1] The central concern of all Upanishads is to discover the relations between ritual, cosmic realities (including gods), and the human body/person,[7] postulating Ātman and Brahman as the "summit of the hierarchically arranged and interconnected universe",[8][9][10] but various ideas about the relation between Atman and Brahman can be found.[10][note 2]

108 Upanishads are known, of which the first dozen or so are the oldest and most important and are referred to as the principal or main (mukhya) Upanishads.[11][12] The mukhya Upanishads are found mostly in the concluding part of the Brahmanas and Aranyakas[13] and were, for centuries, memorized by each generation and passed down orally. The mukhya Upanishads predate the Common Era, but there is no scholarly consensus on their date, or even on which ones are pre- or post-Buddhist. The Brhadaranyaka is seen as particularly ancient by modern scholars.[14][15][16] Of the remainder, 95 Upanishads are part of the Muktikā canon, composed from about the last centuries of 1st-millennium BCE through about 15th-century CE.[17][18] New Upanishads, beyond the 108 in the Muktika canon, continued to be composed through the early modern and modern era,[19] though often dealing with subjects that are unconnected to the Vedas.[20] The mukhya Upanishads, along with the Bhagavad Gita and the Brahmasutra (known collectively as the Prasthanatrayi),[21] are interpreted in divergent ways in the several later schools of Vedanta.[10][note 3][22]

Translations of the Upanishads in the early 19th century started to attract attention from a Western audience. German philosopher Arthur Schopenhauer was deeply impressed by the Upanishads and called them "the most profitable and elevating reading which ... is possible in the world."[23] Modern era Indologists have discussed the similarities between the fundamental concepts in the Upanishads and the works of major Western philosophers.[24][25][26]

Etymology
The Sanskrit term Upaniṣad originally meant “connection” or “equivalence",[27] but came to be understood as "sitting near a teacher,"[27] from upa "by" and ni-ṣad "sit down",[28] "sitting down near", referring to the student sitting down near the teacher while receiving spiritual knowledge (Gurumukh).[29] Other dictionary meanings include "esoteric doctrine" and "secret doctrine". Monier-Williams' Sanskrit Dictionary notes – "According to native authorities, Upanishad means setting to rest ignorance by revealing the knowledge of the supreme spirit."[30]

Adi Shankaracharya explains in his commentary on the Kaṭha and Brihadaranyaka Upanishad that the word means Ātmavidyā, that is, "knowledge of the self", or Brahmavidyā "knowledge of Brahman". The word appears in the verses of many Upanishads, such as the fourth verse of the 13th volume in the first chapter of the Chandogya Upanishad. Max Müller as well as Paul Deussen translate the word Upanishad in these verses as "secret doctrine",[31][32] Robert Hume translates it as "mystic meaning",[33] while Patrick Olivelle translates it as "hidden connections".[34]

Development
Authorship
The authorship of most Upanishads is unknown. Sarvapalli Radhakrishnan states, "almost all the early literature of India was anonymous, we do not know the names of the authors of the Upanishads".[35] The ancient Upanishads are embedded in the Vedas, the oldest of Hinduism's religious scriptures, which some traditionally consider to be apauruṣeya, which means "not of a man, superhuman"[36] and "impersonal, authorless".[37][38][39] The Vedic texts assert that they were skillfully created by Rishis (sages), after inspired creativity, just as a carpenter builds a chariot.[40]

The various philosophical theories in the early Upanishads have been attributed to famous sages such as Yajnavalkya, Uddalaka Aruni, Shvetaketu, Shandilya, Aitareya, Balaki, Pippalada, and Sanatkumara.[35][41] Women, such as Maitreyi and Gargi, participate in the dialogues and are also credited in the early Upanishads.[42] There are some exceptions to the anonymous tradition of the Upanishads. The Shvetashvatara Upanishad, for example, includes closing credits to sage Shvetashvatara, and he is considered the author of the Upanishad.[43]

Many scholars believe that early Upanishads were interpolated[44] and expanded over time. There are differences within manuscripts of the same Upanishad discovered in different parts of South Asia, differences in non-Sanskrit version of the texts that have survived, and differences within each text in terms of meter,[45] style, grammar and structure.[46][47] The existing texts are believed to be the work of many authors.[48]

Chronology
Scholars are uncertain about when the Upanishads were composed.[49] The chronology of the early Upanishads is difficult to resolve, states philosopher and Sanskritist Stephen Phillips,[11] because all opinions rest on scanty evidence and analysis of archaism, style and repetitions across texts, and are driven by assumptions about likely evolution of ideas, and presumptions about which philosophy might have influenced which other Indian philosophies. Indologist Patrick Olivelle says that "in spite of claims made by some, in reality, any dating of these documents [early Upanishads] that attempts a precision closer than a few centuries is as stable as a house of cards".[14]

Some scholars have tried to analyse similarities between Hindu Upanishads and Buddhist literature to establish chronology for the Upanishads.[15] Precise dates are impossible, and most scholars give only broad ranges encompassing various centuries. Gavin Flood states that "the Upanisads are not a homogeneous group of texts. Even the older texts were composed over a wide expanse of time from about 600 to 300 BCE."[50] Stephen Phillips places the early or "principal" Upanishads in the 800 to 300 BCE range.[11]

Patrick Olivelle, a Sanskrit Philologist and Indologist, gives the following chronology for the early Upanishads, also called the Principal Upanishads:[49][14]

The Brhadaranyaka and the Chandogya are the two earliest Upanishads. They are edited texts, some of whose sources are much older than others. The two texts are pre-Buddhist; they may be placed in the 7th to 6th centuries BCE, give or take a century or so.[51][15]
The three other early prose Upanishads—Taittiriya, Aitareya, and Kausitaki come next; all are probably pre-Buddhist and can be assigned to the 6th to 5th centuries BCE.[52]
The Kena is the oldest of the verse Upanishads followed by probably the Katha, Isa, Svetasvatara, and Mundaka. All these Upanishads were composed probably in the last few centuries BCE.[53] According to Olivelle, "All exhibit strong theistic tendencies and are probably the earliest literary products of the theistic tradition, whose later literature includes the Bhagavad Gita and the Puranas."[54]
The two late prose Upanishads, the Prasna and the Mandukya, cannot be much older than the beginning of the common era.[49][14]
Meanwhile, the Indologist Johannes Bronkhorst argues for a later date for the Upanishads than has generally been accepted. Bronkhorst places even the oldest of the Upanishads, such as the Brhadaranyaka as possibly still being composed at "a date close to Katyayana and Patañjali [the grammarian]" (i.e., c. 2nd century BCE).[16]

The later Upanishads, numbering about 95, also called minor Upanishads, are dated from the late 1st-millennium BCE to mid 2nd-millennium CE.[17] Gavin Flood dates many of the twenty Yoga Upanishads to be probably from the 100 BCE to 300 CE period.[18] Patrick Olivelle and other scholars date seven of the twenty Sannyasa Upanishads to likely have been complete sometime between the last centuries of the 1st-millennium BCE to 300 CE.[17] About half of the Sannyasa Upanishads were likely composed in 14th- to 15th-century CE.[17]

Geography

Geography of the Late Vedic Period
The general area of the composition of the early Upanishads is considered as northern India. The region is bounded on the west by the upper Indus valley, on the east by lower Ganges region, on the north by the Himalayan foothills, and on the south by the Vindhya mountain range.[14] Scholars are reasonably sure that the early Upanishads were produced at the geographical center of ancient Brahmanism, Kuru-Panchala, and Kosala-Videha, a "frontier region" of Brahmanism, together with the areas immediately to the south and west of these.[55] This region covers modern Bihar, Nepal, Uttar Pradesh, Uttarakhand, Himachal Pradesh, Haryana, eastern Rajasthan, and northern Madhya Pradesh.[14]

While significant attempts have been made recently to identify the exact locations of the individual Upanishads, the results are tentative. Witzel identifies the center of activity in the Brihadaranyaka Upanishad as the area of Videha, whose king, Janaka, features prominently in the Upanishad.[56] The Chandogya Upanishad was probably composed in a more western than eastern location in the Indian subcontinent, possibly somewhere in the western region of the Kuru-Panchala country.[57]

Compared to the Principal Upanishads, the new Upanishads recorded in the Muktikā belong to an entirely different region, probably southern India, and are considerably relatively recent.[58] In the fourth chapter of the Kaushitaki Upanishad, a location named Kashi (modern Varanasi) is mentioned.[14]

Classification
Muktika canon: major and minor Upanishads
There are more than 200 known Upanishads, one of which, the Muktikā Upanishad, predates 1656 CE[59] and contains a list of 108 canonical Upanishads,[60] including itself as the last. These are further divided into Upanishads associated with Shaktism (goddess Shakti), Sannyasa (renunciation, monastic life), Shaivism (god Shiva), Vaishnavism (god Vishnu), Yoga, and Sāmānya (general, sometimes referred to as Samanya-Vedanta).[61][62]

Some of the Upanishads are categorized as "sectarian" since they present their ideas through a particular god or goddess of a specific Hindu tradition such as Vishnu, Shiva, Shakti, or a combination of these such as the Skanda Upanishad. These traditions sought to link their texts as Vedic, by asserting their texts to be an Upanishad, thereby a Śruti.[63] Most of these sectarian Upanishads, for example the Rudrahridaya Upanishad and the Mahanarayana Upanishad, assert that all the Hindu gods and goddesses are the same, all an aspect and manifestation of Brahman, the Vedic concept for metaphysical ultimate reality before and after the creation of the Universe.[64][65]

Principal Upanishads
Main article: Principal Upanishads
The Principal Upanishads, also known as the Mukhya Upanishads, can be grouped into periods. Of the early periods are the Brihadaranyaka and the Chandogya, the oldest.[66][note 4]


A page of Isha Upanishad manuscript
The Aitareya, Kauṣītaki and Taittirīya Upanishads may date to as early as the mid-1st millennium BCE, while the remnant date from between roughly the 4th to 1st centuries BCE, roughly contemporary with the earliest portions of the Sanskrit epics. One chronology assumes that the Aitareya, Taittiriya, Kausitaki, Mundaka, Prasna, and Katha Upanishads has Buddha's influence, and is consequently placed after the 5th century BCE, while another proposal questions this assumption and dates it independent of Buddha's date of birth. The Kena, Mandukya, and Isa Upanishads are typically placed after these Principal Upanishads, but other scholars date these differently.[15] Not much is known about the authors except for those, like Yajnavalkayva and Uddalaka, mentioned in the texts.[13] A few women discussants, such as Gargi and Maitreyi, the wife of Yajnavalkayva,[68] also feature occasionally.

Each of the principal Upanishads can be associated with one of the schools of exegesis of the four Vedas (shakhas).[69] Many Shakhas are said to have existed, of which only a few remain. The new Upanishads often have little relation to the Vedic corpus and have not been cited or commented upon by any great Vedanta philosopher: their language differs from that of the classic Upanishads, being less subtle and more formalized. As a result, they are not difficult to comprehend for the modern reader.[70]

Veda-Shakha-Upanishad association
Veda	Recension	Shakha	Principal Upanishad
Rig Veda	Only one recension	Shakala	Aitareya
Sama Veda	Only one recension	Kauthuma	Chāndogya
Jaiminiya	Kena
Ranayaniya	
Yajur Veda	Krishna Yajur Veda	Katha	Kaṭha
Taittiriya	Taittirīya
Maitrayani	
Hiranyakeshi (Kapishthala)	
Kathaka	
Shukla Yajur Veda	Vajasaneyi Madhyandina	Isha and Bṛhadāraṇyaka
Kanva Shakha	
Atharva Veda	Two recensions	Shaunaka	Māṇḍūkya and Muṇḍaka
Paippalada	Prashna Upanishad

Neuroscience is the scientific study of the nervous system (the brain, spinal cord, and peripheral nervous system), its functions, and its disorders.[1][2][3] It is a multidisciplinary science that combines physiology, anatomy, molecular biology, developmental biology, cytology, psychology, physics, computer science, chemistry, medicine, statistics, and mathematical modeling to understand the fundamental and emergent properties of neurons, glia, and neural circuits.[4][5][6][7][8] The understanding of the biological basis of learning, memory, behavior, perception, and consciousness has been described by Eric Kandel as the "epic challenge" of the biological sciences.[9]

The scope of neuroscience has broadened over time to include different approaches used to study the nervous system at different scales. The techniques used by neuroscientists have expanded enormously, from molecular and cellular studies of individual neurons to imaging of sensory, motor, and cognitive tasks in the brain.

History
Main article: History of neuroscience

Illustration from Gray's Anatomy (1918) of a lateral view of the human brain, featuring the hippocampus among other neuroanatomical features
The earliest study of the nervous system dates to ancient Egypt. Trepanation, the surgical practice of either drilling or scraping a hole into the skull for the purpose of curing head injuries or mental disorders, or relieving cranial pressure, was first recorded during the Neolithic period. Manuscripts dating to 1700 BC indicate that the Egyptians had some knowledge about symptoms of brain damage.[10]

Early views on the function of the brain regarded it to be a "cranial stuffing" of sorts. In Egypt, from the late Middle Kingdom onwards, the brain was regularly removed in preparation for mummification. It was believed at the time that the heart was the seat of intelligence. According to Herodotus, the first step of mummification was to "take a crooked piece of iron, and with it draw out the brain through the nostrils, thus getting rid of a portion, while the skull is cleared of the rest by rinsing with drugs."[11]

The view that the heart was the source of consciousness was not challenged until the time of the Greek physician Hippocrates. He believed that the brain was not only involved with sensation—since most specialized organs (e.g., eyes, ears, tongue) are located in the head near the brain—but was also the seat of intelligence.[12] Plato also speculated that the brain was the seat of the rational part of the soul.[13] Aristotle, however, believed the heart was the center of intelligence and that the brain regulated the amount of heat from the heart.[14] This view was generally accepted until the Roman physician Galen, a follower of Hippocrates and physician to Roman gladiators, observed that his patients lost their mental faculties when they had sustained damage to their brains.[15]

Abulcasis, Averroes, Avicenna, Avenzoar, and Maimonides, active in the Medieval Muslim world, described a number of medical problems related to the brain. In Renaissance Europe, Vesalius (1514–1564), René Descartes (1596–1650), Thomas Willis (1621–1675) and Jan Swammerdam (1637–1680) also made several contributions to neuroscience.


The Golgi stain first allowed for the visualization of individual neurons.
Luigi Galvani's pioneering work in the late 1700s set the stage for studying the electrical excitability of muscles and neurons. In 1843 Emil du Bois-Reymond demonstrated the electrical nature of the nerve signal,[16] whose speed Hermann von Helmholtz proceeded to measure,[17] and in 1875 Richard Caton found electrical phenomena in the cerebral hemispheres of rabbits and monkeys.[18] Adolf Beck published in 1890 similar observations of spontaneous electrical activity of the brain of rabbits and dogs.[19] Studies of the brain became more sophisticated after the invention of the microscope and the development of a staining procedure by Camillo Golgi during the late 1890s. The procedure used a silver chromate salt to reveal the intricate structures of individual neurons. His technique was used by Santiago Ramón y Cajal and led to the formation of the neuron doctrine, the hypothesis that the functional unit of the brain is the neuron.[20] Golgi and Ramón y Cajal shared the Nobel Prize in Physiology or Medicine in 1906 for their extensive observations, descriptions, and categorizations of neurons throughout the brain.

In parallel with this research, in 1815 Jean Pierre Flourens induced localized lesions of the brain in living animals to observe their effects on motricity, sensibility and behavior. Work with brain-damaged patients by Marc Dax in 1836 and Paul Broca in 1865 suggested that certain regions of the brain were responsible for certain functions.[21] At the time, these findings were seen as a confirmation of Franz Joseph Gall's theory that language was localized and that certain psychological functions were localized in specific areas of the cerebral cortex.[22][23] The localization of function hypothesis was supported by observations of epileptic patients conducted by John Hughlings Jackson, who correctly inferred the organization of the motor cortex by watching the progression of seizures through the body. Carl Wernicke further developed the theory of the specialization of specific brain structures in language comprehension and production. In 1894, neurologist and psychiatrist Edward Flatau published a human brain atlas “Atlas of the Human Brain and the Course of the Nerve-Fibres” which consisted of long-exposure photographs of fresh brain sections.[24] In 1897, Charles Scott Sherrington introduced the name "synapse" for the connection between neurons.[25]


Brodmann's diagram of the cerebral cortex with the areas he identified
In 1909, German anatomist Korbinian Brodmann published his original research on brain mapping, defining 52 distinct regions of the cerebral cortex, known as Brodmann areas.[26] Modern research through neuroimaging techniques, still uses the Brodmann cerebral cytoarchitectonic map (referring to the study of cell structure) anatomical definitions from this era in continuing to show that distinct areas of the cortex are activated in the execution of specific tasks.[27]

During the 20th century, neuroscience began to be recognized as a distinct academic discipline in its own right, rather than as studies of the nervous system within other disciplines. Eric Kandel and collaborators have cited David Rioch, Francis O. Schmitt, and Stephen Kuffler as having played critical roles in establishing the field.[28] Rioch originated the integration of basic anatomical and physiological research with clinical psychiatry at the Walter Reed Army Institute of Research, starting in the 1950s. During the same period, Schmitt established a neuroscience research program within the Biology Department at the Massachusetts Institute of Technology, bringing together biology, chemistry, physics, and mathematics. The first freestanding neuroscience department (then called Psychobiology) was founded in 1964 at the University of California, Irvine by James L. McGaugh.[29] This was followed by the Department of Neurobiology at Harvard Medical School, which was founded in 1966 by Stephen Kuffler.[30]


3-D sensory and motor homunculus models at the Natural History Museum, London
In the process of treating epilepsy, Wilder Penfield produced maps of the location of various functions (motor, sensory, memory, vision) in the brain.[31][32] He summarized his findings in a 1950 book called The Cerebral Cortex of Man.[33] Wilder Penfield and his co-investigators Edwin Boldrey and Theodore Rasmussen are considered to be the originators of the cortical homunculus.[34]

The understanding of neurons and of nervous system function became increasingly precise and molecular during the 20th century. For example, in 1952, Alan Lloyd Hodgkin and Andrew Huxley presented a mathematical model for the transmission of electrical signals in neurons of the giant axon of a squid, which they called "action potentials", and how they are initiated and propagated, known as the Hodgkin–Huxley model. In 1961–1962, Richard FitzHugh and J. Nagumo simplified Hodgkin–Huxley, in what is called the FitzHugh–Nagumo model. In 1962, Bernard Katz modeled neurotransmission across the space between neurons known as synapses. Beginning in 1966, Eric Kandel and collaborators examined biochemical changes in neurons associated with learning and memory storage in Aplysia. In 1981 Catherine Morris and Harold Lecar combined these models in the Morris–Lecar model. Such increasingly quantitative work gave rise to numerous biological neuron models and models of neural computation.

As a result of the increasing interest about the nervous system, several prominent neuroscience organizations have been formed to provide a forum to all neuroscientists during the 20th century. For example, the International Brain Research Organization was founded in 1961,[35] the International Society for Neurochemistry in 1963,[36] the European Brain and Behaviour Society in 1968,[37] and the Society for Neuroscience in 1969.[38] Recently, the application of neuroscience research results has also given rise to applied disciplines as neuroeconomics,[39] neuroeducation,[40] neuroethics,[41] and neurolaw.[42]

Over time, brain research has gone through philosophical, experimental, and theoretical phases, with work on neural implants and brain simulation predicted to be important in the future.[43]

Modern neuroscience
Main article: Outline of neuroscience

Human nervous system
The scientific study of the nervous system increased significantly during the second half of the twentieth century, principally due to advances in molecular biology, electrophysiology, and computational neuroscience. This has allowed neuroscientists to study the nervous system in all its aspects: how it is structured, how it works, how it develops, how it malfunctions, and how it can be changed.

For example, it has become possible to understand, in much detail, the complex processes occurring within a single neuron. Neurons are cells specialized for communication. They are able to communicate with neurons and other cell types through specialized junctions called synapses, at which electrical or electrochemical signals can be transmitted from one cell to another. Many neurons extrude a long thin filament of axoplasm called an axon, which may extend to distant parts of the body and are capable of rapidly carrying electrical signals, influencing the activity of other neurons, muscles, or glands at their termination points. A nervous system emerges from the assemblage of neurons that are connected to each other in neural circuits, and networks.

The vertebrate nervous system can be split into two parts: the central nervous system (defined as the brain and spinal cord), and the peripheral nervous system. In many species—including all vertebrates—the nervous system is the most complex organ system in the body, with most of the complexity residing in the brain. The human brain alone contains around one hundred billion neurons and one hundred trillion synapses; it consists of thousands of distinguishable substructures, connected to each other in synaptic networks whose intricacies have only begun to be unraveled. At least one out of three of the approximately 20,000 genes belonging to the human genome is expressed mainly in the brain.[44]

Due to the high degree of plasticity of the human brain, the structure of its synapses and their resulting functions change throughout life.[45]

Making sense of the nervous system's dynamic complexity is a formidable research challenge. Ultimately, neuroscientists would like to understand every aspect of the nervous system, including how it works, how it develops, how it malfunctions, and how it can be altered or repaired. Analysis of the nervous system is therefore performed at multiple levels, ranging from the molecular and cellular levels to the systems and cognitive levels. The specific topics that form the main focus of research change over time, driven by an ever-expanding base of knowledge and the availability of increasingly sophisticated technical methods. Improvements in technology have been the primary drivers of progress. Developments in electron microscopy, computer science, electronics, functional neuroimaging, and genetics and genomics have all been major drivers of progress.

Advances in the classification of brain cells have been enabled by electrophysiological recording, single-cell genetic sequencing, and high-quality microscopy, which have combined into a single method pipeline called patch-sequencing in which all three methods are simultaneously applied using miniature tools.[46] The efficiency of this method and the large amounts of data that is generated has allowed researchers to make some general conclusions about cell types; for example that the human and mouse brain have different versions of fundamentally the same cell types.[47]

Molecular and cellular neuroscience
Main articles: Molecular neuroscience and Cellular neuroscience

Photograph of a stained neuron in a chicken embryo
Basic questions addressed in molecular neuroscience include the mechanisms by which neurons express and respond to molecular signals and how axons form complex connectivity patterns. At this level, tools from molecular biology and genetics are used to understand how neurons develop and how genetic changes affect biological functions.[48] The morphology, molecular identity, and physiological characteristics of neurons and how they relate to different types of behavior are also of considerable interest.[49]

Questions addressed in cellular neuroscience include the mechanisms of how neurons process signals physiologically and electrochemically. These questions include how signals are processed by neurites and somas and how neurotransmitters and electrical signals are used to process information in a neuron. Neurites are thin extensions from a neuronal cell body, consisting of dendrites (specialized to receive synaptic inputs from other neurons) and axons (specialized to conduct nerve impulses called action potentials). Somas are the cell bodies of the neurons and contain the nucleus.[50]

Another major area of cellular neuroscience is the investigation of the development of the nervous system.[51] Questions include the patterning and regionalization of the nervous system, axonal and dendritic development, trophic interactions, synapse formation and the implication of fractones in neural stem cells,[52][53] differentiation of neurons and glia (neurogenesis and gliogenesis), and neuronal migration.[54]

Computational neurogenetic modeling is concerned with the development of dynamic neuronal models for modeling brain functions with respect to genes and dynamic interactions between genes, on the cellular level (Computational Neurogenetic Modeling (CNGM) can also be used to model neural systems).[55]

Rama (/ˈrɑːmə/;[4] Sanskrit: राम, IAST: Rāma, Sanskrit: [ˈraːmɐ] ⓘ) is a major deity in Hinduism. He is worshipped as the seventh and one of the most popular avatars of Vishnu.[5] In Rama-centric Hindu traditions, he is considered the Supreme Being. Also considered as the ideal man (maryāda puruṣottama), Rama is the male protagonist of the Hindu epic Ramayana. His birth is celebrated every year on Rama Navami, which falls on the ninth day of the bright half (Shukla Paksha) of the lunar cycle of Chaitra (March–April), the first month in the Hindu calendar.[6][7]

According to the Ramayana, Rama was born to Dasaratha and his first wife Kausalya in Ayodhya, the capital of the Kingdom of Kosala. His siblings included Lakshmana, Bharata, and Shatrughna. He married Sita. Born in a royal family, Rama's life is described in the Hindu texts as one challenged by unexpected changes, such as an exile into impoverished and difficult circumstances, and challenges of ethical questions and moral dilemmas.[8] The most notable story involving Rama is the kidnapping of Sita by the demon-king Ravana, followed by Rama and Lakshmana's journey to rescue her.

The life story of Rama, Sita and their companions allegorically discusses duties, rights and social responsibilities of an individual. It illustrates dharma and dharmic living through model characters.[8][9]

Rama is especially important to Vaishnavism. He is the central figure of the ancient Hindu epic Ramayana, a text historically popular in the South Asian and Southeast Asian cultures.[10][11][12] His ancient legends have attracted bhashya (commentaries) and extensive secondary literature and inspired performance arts. Two such texts, for example, are the Adhyatma Ramayana – a spiritual and theological treatise considered foundational by Ramanandi monasteries,[13] and the Ramcharitmanas – a popular treatise that inspires thousands of Ramlila festival performances during autumn every year in India.[14][15][16]

Rama legends are also found in the texts of Jainism and Buddhism, though he is sometimes called Pauma or Padma in these texts,[17] and their details vary significantly from the Hindu versions.[18] Jain Texts also mention Rama as the eighth balabhadra among the 63 salakapurusas.[19][20][21] In Sikhism, Rama is mentioned as twentieth of the[22] twenty-four divine avatars of Vishnu in the Chaubis Avtar in Dasam Granth.[23]

Etymology and nomenclature
Rama is also known as Ram, Raman, Ramar,[α] and Ramachandra (/ˌrɑːməˈtʃəndrə/;[25] IAST: Rāmacandra, Sanskrit: रामचन्द्र). Rāma is a Vedic Sanskrit word with two contextual meanings. In one context, as found in Atharva Veda, as stated by Monier Monier-Williams, it means "dark, dark-colored, black" and is related to the term ratri, which means night. In another context in other Vedic texts, the word means "pleasing, delightful, charming, beautiful, lovely".[26][27] The word is sometimes used as a suffix in different Indian languages and religions, such as Pali in Buddhist texts, where -rama adds the sense of "pleasing to the mind, lovely" to the composite word.[28]

Rama as a first name appears in the Vedic literature, associated with two patronymic names – Margaveya and Aupatasvini – representing different individuals. A third individual named Rama Jamadagnya is the purported author of hymn 10.110 of the Rigveda in the Hindu tradition.[26] The word Rama appears in ancient literature in reverential terms for three individuals:[26]

Parashu-rama, as the sixth avatar of Vishnu. He is linked to the Rama Jamadagnya of the Rigveda fame.
Rama-chandra, as the seventh avatar of Vishnu and of the ancient Ramayana fame.
Bala-rama, also called Halayudha, as the elder brother of Krishna both of whom appear in the legends of Hinduism, Buddhism and Jainism.
The name Rama appears repeatedly in Hindu texts, for many different scholars and kings in mythical stories.[26] The word also appears in ancient Upanishads and Aranyakas layer of Vedic literature, as well as music and other post-Vedic literature, but in qualifying context of something or someone who is "charming, beautiful, lovely" or "darkness, night".[26]

The Vishnu avatar named Rama is also known by other names. He is called Ramachandra (beautiful, lovely moon),[27] or Dasarathi (son of Dasaratha), or Raghava (descendant of Raghu, solar dynasty in Hindu cosmology).[26][29]

Additional names of Rama include Ramavijaya (Javanese), Phreah Ream (Khmer), Phra Ram (Lao and Thai), Megat Seri Rama (Malay), Raja Bantugan (Maranao), Ramar or Raman (Tamil), and Ramudu (Telugu).[30] In the Vishnu sahasranama, Rama is the 394th name of Vishnu. In some Advaita Vedanta inspired texts, Rama connotes the metaphysical concept of Supreme Brahman who is the eternally blissful spiritual Self (Atman, soul) in whom yogis delight nondualistically.[31]

The root of the word Rama is ram- which means "stop, stand still, rest, rejoice, be pleased".[27]

According to Douglas Q. Adams, the Sanskrit word Rama is also found in other Indo-European languages such as Tocharian ram, reme, *romo- where it means "support, make still", "witness, make evident".[27][32] The sense of "dark, black, soot" also appears in other Indo European languages, such as *remos or Old English romig.[33][β]

Legends
This summary is a traditional legendary account, based on literary details from the Ramayana and other historic mythology-containing texts of Buddhism and Jainism. According to Sheldon Pollock, the figure of Rama incorporates more ancient "morphemes of Indian myths", such as the mythical legends of Bali and Namuci. The ancient sage Valmiki used these morphemes in his Ramayana similes as in sections 3.27, 3.59, 3.73, 5.19 and 29.28.[35]

Birth

Balak Ram, the 5-year-old form of Rama, is the principal deity of the Ram Mandir in Ram Janmabhoomi
The ancient epic Ramayana states in the Balakanda that Rama and his brothers were born to Kaushalya and Dasharatha in Ayodhya, a city on the banks of Sarayu River.[36][37] The Jain versions of the Ramayana, such as the Paumacariya (literally deeds of Padma) by Vimalasuri, also mention the details of the early life of Rama. The Jain texts are dated variously, but generally pre-500 CE, most likely sometime within the first five centuries of the common era.[38] Moriz Winternitz states that the Valmiki Ramayana was already famous before it was recast in the Jain Paumacariya poem, dated to the second half of the 1st century CE, which pre-dates a similar retelling found in the Buddha-carita of Asvagosa, dated to the beginning of the 2nd century CE or prior.[39]

Dasharatha was the king of Kosala, and a part of the Kshatriya solar dynasty of Iksvakus. His mother's name Kaushalya literally implies that she was from Kosala. The kingdom of Kosala is also mentioned in Buddhist and Jain texts, as one of the sixteen Maha janapadas of ancient India, and as an important center of pilgrimage for Jains and Buddhists.[36][40] However, there is a scholarly dispute whether the modern Ayodhya is indeed the same as the Ayodhya and Kosala mentioned in the Ramayana and other ancient Indian texts.[41][γ]

Rama's birth, according to Ramayana, is an incarnation of God (Vishnu) as human. When demigods went to Brahma to seek liberation from Ravana's menace on the Earth (due to powers he had from Brahma's boon to him), Vishnu himself appeared and said he will incarnate as Rama (human) and kill Ravana (since Brahma's boon made him invincible from all, including God, except humans).[43]

Youth, family and marriage to Sita
Further information: Rama's Journey in Mithila
Rama had three brothers, according to the Balakanda section of the Ramayana. These were Lakshmana, Bharata and Shatrughna.[3] The extant manuscripts of the text describes their education and training as young princes, but this is brief. Rama is portrayed as a polite, self-controlled, virtuous youth always ready to help others. His education included the Vedas, the Vedangas as well as the martial arts.[44]

The years when Rama grew up are described in much greater detail by later Hindu texts, such as the Ramavali by Tulsidas. The template is similar to those found for Krishna, but in the poems of Tulsidas, Rama is milder and reserved introvert, rather than the prank-playing extrovert personality of Krishna.[3]


The marriage ceremony of Rama and Sita.[45]
In the kingdom of Mithilā, Janaka conducted a svayamvara ceremony at his capital with the condition that she would marry only a prince who would possess the strength to string the ajagava, one of the bows of the deity Shiva. Many princes attempted and failed to string the bow. During this time, Vishvamitra had brought Rama and his brother Lakshmana to the forest for the protection of a yajna (ritual sacrifice). Hearing about the svayamvara, Vishvamitra asked Rama to participate in the ceremony with the consent of Janaka, who agreed to offer Sita's hand in marriage to the prince if he could fulfil the requisite task. When the bow was brought before him, Rama seized the centre of the weapon, fastened the string taut, and broke it in two in the process. Witnessing his prowess, Janaka agreed to marry his daughter to Rama and invited Dasharatha to his capital.[45] During the homeward journey to Ayodhya, another avatar of Vishnu, Parashurama, challenged Rama to combat, on the condition that he was able to string the bow of Vishnu, Sharanga. When Rama obliged him with success, Parashurama acknowledged the former to be a form of Vishnu and departed to perform penance at the mountain Mahendra. The wedding entourage then reached Ayodhya, entering the city amid great fanfare.[46][47][48] Thereafter, Rama lived happily with Sita for twelve (12) years.[49]

Meanwhile, Rama and his brothers were away, Kaikeyi, the mother of Bharata and the third wife of King Dasharatha, reminds the king that he had promised long ago to comply with one thing she asks, anything. Dasharatha remembers and agrees to do so. She demands that Rama be exiled for fourteen years to Dandaka forest.[44] Dasharatha grieves at her request. Her son Bharata, and other family members become upset at her demand. Rama states that his father should keep his word, adds that he does not crave for earthly or heavenly material pleasures, and seeks neither power nor anything else. He informs of his decision to his wife and tells everyone that time passes quickly. Sita leaves with him to live in the forest, and Lakshmana joins them in their exile as the caring close brother.[44][50]Krishna (/ˈkrɪʃnə/;[12] Sanskrit: कृष्ण, IAST: Kṛṣṇa Sanskrit: [ˈkr̩ʂɳɐ] ⓘ) is a major deity in Hinduism. He is worshipped as the eighth avatar of Vishnu and also as the Supreme God in his own right.[13] He is the god of protection, compassion, tenderness, and love;[14][1] and is widely revered among Hindu divinities.[15] Krishna's birthday is celebrated every year by Hindus on Krishna Janmashtami according to the lunisolar Hindu calendar, which falls in late August or early September of the Gregorian calendar.[16][17][18]

The anecdotes and narratives of Krishna's life are generally titled as Krishna Līlā. He is a central figure in the Mahabharata, the Bhagavata Purana, the Brahma Vaivarta Purana, and the Bhagavad Gita, and is mentioned in many Hindu philosophical, theological, and mythological texts.[19] They portray him in various perspectives: as a god-child, a prankster, a model lover, a divine hero, and the universal supreme being.[20] His iconography reflects these legends and shows him in different stages of his life, such as an infant eating butter, a young boy playing a flute, a handsome youth with Radha or surrounded by female devotees, or a friendly charioteer giving counsel to Arjuna.[21]

The name and synonyms of Krishna have been traced to 1st millennium BCE literature and cults.[22] In some sub-traditions, like Krishnaism, Krishna is worshipped as the Supreme God and Svayam Bhagavan (God Himself). These sub-traditions arose in the context of the medieval era Bhakti movement.[23][24] Krishna-related literature has inspired numerous performance arts such as Bharatanatyam, Kathakali, Kuchipudi, Odissi, and Manipuri dance.[25][26] He is a pan-Hindu god, but is particularly revered in some locations, such as Vrindavan in Uttar Pradesh,[27] Dwarka and Junagadh in Gujarat; the Jagannatha aspect in Odisha, Mayapur in West Bengal;[23][28][29] in the form of Vithoba in Pandharpur, Maharashtra, Shrinathji at Nathdwara in Rajasthan,[23][30] Udupi Krishna in Karnataka,[31] Parthasarathy in Tamil Nadu, Aranmula and Guruvayoorappan (Guruvayoor) in Kerala.[32]

Since the 1960s, the worship of Krishna has also spread to the Western world, largely due to the work of the International Society for Krishna Consciousness (ISKCON).[33]

Names and epithets
Main article: List of titles and names of Krishna
The name "Krishna" originates from the Sanskrit word kṛṣṇa, which means "black", "dark" or "dark blue".[34] The waning moon is called Krishna Paksha, relating to the adjective meaning "darkening".[34] Some Vaishnavas also translate the word as "All-Attractive", though it lacks that meaning in Sanskrit.[35]

As a name of Vishnu, Krishna is listed as the 57th name in the Vishnu Sahasranama. Based on his name, Krishna is often depicted in idols as black- or blue-skinned.[36] Krishna is also known by various other names, epithets, and titles that reflect his many associations and attributes. Among the most common names are Mohan "enchanter"; Govinda "chief herdsman",[37] Keev "prankster", and Gopala "Protector of the 'Go'", which means "soul" or "the cows".[38][39] Some names for Krishna hold regional importance; Jagannatha, found in the Puri Hindu temple, is a popular incarnation in Odisha state and nearby regions of eastern India.[40][41][42]

Historical and literary sources
The tradition of Krishna appears to be an amalgamation of several independent deities of ancient India, the earliest of whom to be attested being Vāsudeva.[43] Vāsudeva was a hero-god of the tribe of the Vrishnis, belonging to the Vrishni heroes, whose worship is attested from the 5th–6th century BCE in the writings of Pāṇini, and from the 2nd century BCE in epigraphy with the Heliodorus pillar.[43] At one point in time, it is thought that the tribe of the Vrishnis fused with the tribe of the Yadavas, whose own hero-god was named Krishna.[43] Vāsudeva and Krishna fused to become a single deity, which appears in the Mahabharata, and they started to be identified with Vishnu in the Mahabharata and the Bhagavad Gita.[43] Around the 4th century CE, another tradition, the cult of Gopala-Krishna of the Ābhīras, the protector of cattle, was also absorbed into the Krishna tradition.[43]

Early epigraphic sources
Main article: Vāsudeva-Krishna
Depiction in coinage (2nd century BCE)

Vāsudeva-Krishna, on a coin of Agathocles of Bactria, c. 180 BCE.[44][45][46] This is "the earliest unambiguous image" of the deity.[47]
Around 180 BCE, the Indo-Greek king Agathocles issued some coinage (discovered in Ai-Khanoum, Afghanistan) bearing images of deities that are now interpreted as being related to Vaisnava imagery in India.[48][49] The deities displayed on the coins appear to be Saṃkarṣaṇa-Balarama with attributes consisting of the gada mace and the plow, and Vāsudeva-Krishna with attributes of the shankha (conch) and the sudarshana chakra wheel.[48][50] According to Bopearachchi, the headdress of the deity is actually a misrepresentation of a shaft with a half-moon parasol on top (chattra).[48]

Inscriptions

Heliodorus Pillar in the Indian state of Madhya Pradesh, erected about 120 BCE. The inscription states that Heliodorus is a Bhagvatena, and a couplet in the inscription closely paraphrases a Sanskrit verse from the Mahabharata.[51][52]
The Heliodorus Pillar, a stone pillar with a Brahmi script inscription, was discovered by colonial era archaeologists in Besnagar (Vidisha, in the central Indian state of Madhya Pradesh). Based on the internal evidence of the inscription, it has been dated to between 125 and 100 BCE and is now known after Heliodorus – an Indo-Greek who served as an ambassador of the Greek king Antialcidas to a regional Indian king, Kasiputra Bhagabhadra.[48][51] The Heliodorus pillar inscription is a private religious dedication of Heliodorus to "Vāsudeva", an early deity and another name for Krishna in the Indian tradition. It states that the column was constructed by "the Bhagavata Heliodorus" and that it is a "Garuda pillar" (both are Vishnu-Krishna-related terms). Additionally, the inscription includes a Krishna-related verse from chapter 11.7 of the Mahabharata stating that the path to immortality and heaven is to correctly live a life of three virtues: self-temperance (damah), generosity (cagah or tyaga), and vigilance (apramadah).[51][53][54] The Heliodorus pillar site was fully excavated by archaeologists in the 1960s. The effort revealed the brick foundations of a much larger ancient elliptical temple complex with a sanctum, mandapas, and seven additional pillars.[55][56] The Heliodorus pillar inscriptions and the temple are among the earliest known evidence of Krishna-Vasudeva devotion and Vaishnavism in ancient India.[57][48][58]


Balarama and Krishna with their attributes at Chilas. The Kharoshthi inscription nearby reads Rama [kri]ṣa. 1st century CE.[47]
The Heliodorus inscription is not isolated evidence. The Hathibada Ghosundi Inscriptions, all located in the state of Rajasthan and dated by modern methodology to the 1st century BCE, mention Saṃkarṣaṇa and Vāsudeva, also mention that the structure was built for their worship in association with the supreme deity Narayana. These four inscriptions are notable for being some of the oldest-known Sanskrit inscriptions.[59]

A Mora stone slab found at the Mathura-Vrindavan archaeological site in Uttar Pradesh, held now in the Mathura Museum, has a Brahmi inscription. It is dated to the 1st century CE and mentions the five Vrishni heroes, otherwise known as Saṃkarṣaṇa, Vāsudeva, Pradyumna, Aniruddha, and Samba.[60][61][62]

The inscriptional record for Vāsudeva starts in the 2nd century BCE with the coinage of Agathocles and the Heliodorus pillar, but the name of Krishna appears rather later in epigraphy. At the Chilas II archaeological site dated to the first half of the 1st-century CE in northwest Pakistan, near the Afghanistan border, are engraved two males, along with many Buddhist images nearby. The larger of the two males held a plough and club in his two hands. The artwork also has an inscription with it in Kharosthi script, which has been deciphered by scholars as Rama-Krsna, and interpreted as an ancient depiction of the two brothers, Balarama and Krishna.[63][64]

The first known depiction of the life of Krishna himself comes relatively late, with a relief found in Mathura, and dated to the 1st–2nd century CE.[65] This fragment seems to show Vasudeva, Krishna's father, carrying baby Krishna in a basket across the Yamuna.[65] The relief shows at one end a seven-hooded Naga crossing a river, where a makara crocodile is thrashing around, and at the other end a person seemingly holding a basket over his head.[65]

Literary sources
Mahabharata
See also: Krishna in the Mahabharata and Bhagavad Gita

Krishna advising Pandavas
The earliest text containing detailed descriptions of Krishna as a personality is the epic Mahabharata, which depicts Krishna as an incarnation of Vishnu.[66] Krishna is central to many of the main stories of the epic. The eighteen chapters of the sixth book (Bhishma Parva) of the epic that constitute the Bhagavad Gita contain the advice of Krishna to Arjuna on the battlefield.

During the ancient times that the Bhagavad Gita was composed in, Krishna was widely seen as an avatar of Vishnu rather than an individual deity, yet he was immensely powerful and almost everything in the universe other than Vishnu was "somehow present in the body of Krishna".[67] Krishna had "no beginning or end", "fill[ed] space", and every god but Vishnu was seen as ultimately him, including Brahma, "storm gods, sun gods, bright gods", light gods, "and gods of ritual."[67] Other forces also existed in his body, such as "hordes of varied creatures" that included "celestial serpents."[67] He is also "the essence of humanity."[67]

The Harivamsa, a later appendix to the Mahabharata, contains a detailed version of Krishna's childhood and youth.[68]

Other sources

Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]

High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."[2][3]

Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI) –AI that can complete virtually any cognitive task at least as well as a human.

Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units started being used to accelerate neural networks, and deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, an ongoing period of rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and modify content has led to several unintended consequences and harms. Ethical concerns have been raised about AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.

Goals
The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]

Reasoning and problem-solving
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]

Many of these algorithms are insufficient for solving large reasoning problems because they experience a "combinatorial explosion": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.

Knowledge representation

An ontology represents knowledge as a set of concepts within a domain and the relationships between those concepts.
Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining "interesting" and actionable inferences from large databases),[21] and other areas.[22]

A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.

Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]

Planning and decision-making
An "agent" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the "utility") that measures how much the agent prefers it. For each possible action, it can calculate the "expected utility": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]

In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is "unknown" or "unobservable") and it may not know for certain what will happen after each possible action (it is not "deterministic"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]

In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.

A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]

Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]

Learning
Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]


In supervised learning, the training data is labelled with the expected answers, while in unsupervised learning, the model identifies patterns or structures in unlabelled data.
There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]

In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as "good".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]

Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]

Natural language processing
Natural language processing (NLP) allows programs to read, write and communicate in human languages.[50] Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]

Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called "micro-worlds" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.

Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or "GPT") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]

Perception
Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]

The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61] object tracking,[62] and robotic perception.[63]

Social intelligence

Kismet, a robot head which was made in the 1990s; it is a machine that can recognize and simulate emotions.[64]
Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.

However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]

General intelligence
A machine with artificial general intelligence would be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[68]A computer is a machine that can be programmed to automatically carry out sequences of arithmetic or logical operations (computation). Modern digital electronic computers can perform generic sets of operations known as programs, which enable computers to perform a wide range of tasks. The term computer system may refer to a nominally complete computer that includes the hardware, operating system, software, and peripheral equipment needed and used for full operation; or to a group of computers that are linked and function together, such as a computer network or computer cluster.

A broad range of industrial and consumer products use computers as control systems, including simple special-purpose devices like microwave ovens and remote controls, and factory devices like industrial robots. Computers are at the core of general-purpose devices such as personal computers and mobile devices such as smartphones. Computers power the Internet, which links billions of computers and users.

Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long, tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II, both electromechanical and using thermionic valves. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power, and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (Moore's law noted that counts doubled every two years), leading to the Digital Revolution during the late 20th and early 21st centuries.

Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, together with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joysticks, etc.), output devices (monitors, printers, etc.), and input/output devices that perform both functions (e.g. touchscreens). Peripheral devices allow information to be retrieved from an external source, and they enable the results of operations to be saved and retrieved.

Etymology
A human computer.
A human computer, with microscope and calculator, 1952
It was not until the mid-20th century that the word acquired its modern definition; according to the Oxford English Dictionary, the first known use of the word computer was in a different sense, in a 1613 book called The Yong Mans Gleanings by the English writer Richard Brathwait: "I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number." This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued to have the same meaning until the middle of the 20th century. During the latter part of this period, women were often hired as computers because they could be paid less than their male counterparts.[1] By 1943, most human computers were women.[2]

The Online Etymology Dictionary gives the first attested use of computer in the 1640s, meaning 'one who calculates'; this is an "agent noun from compute (v.)". The Online Etymology Dictionary states that the use of the term to mean "'calculating machine' (of any type) is from 1897." The Online Etymology Dictionary indicates that the "modern use" of the term, to mean 'programmable digital electronic computer' dates from "1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine".[3] The name has remained, although modern computers are capable of many higher-level functions.

History
Main articles: History of computing and History of computing hardware
For a chronological guide, see Timeline of computing.
Pre-20th century

The Ishango bone, a bone tool dating back to prehistoric Africa
Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was most likely a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, likely livestock or grains, sealed in hollow unbaked clay containers.[a][4] The use of counting rods is one example.


The Chinese suanpan (算盘). The number represented on this abacus is 6,302,715,408.
The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BCE. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.[5]


The Antikythera mechanism, dating back to ancient Greece circa 200–80 BCE, is an early analog computing device.
The Antikythera mechanism is believed to be the earliest known mechanical analog computer, according to Derek J. de Solla Price.[6] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to approximately c. 100 BCE. Devices of comparable complexity to the Antikythera mechanism would not reappear until the fourteenth century.[7]

Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century.[8] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BCE and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[9][10] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[11] Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe,[12] an early fixed-wired knowledge processing machine[13] with a gear train and gear-wheels,[14] c. 1000 AD.

The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.

The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.


A slide rule
The slide rule was invented around 1620–1630, by the English clergyman William Oughtred, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.

In the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically "programmed" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.[15]

In 1831–1835, mathematician and engineer Giovanni Plana devised a Perpetual Calendar machine, which through a system of pulleys and cylinders could predict the perpetual calendar for every year from 0 CE (that is, 1 BCE) to 4000 CE, keeping track of leap years and varying day length. The tide-predicting machine invented by the Scottish scientist Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Sir William Thomson had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.[16] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.

In the 1890s, the Spanish engineer Leonardo Torres Quevedo began to develop a series of advanced analog machines that could solve real and complex roots of polynomials,[17][18][19][20] which were published in 1901 by the Paris Academy of Sciences.[21]

First computer

Charles Babbage

A diagram of a portion of Babbage's Difference engine

The Difference Engine Number 2 at the Intellectual Ventures laboratory in Seattle
Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[22] he conceptualized and invented the first mechanical computer in the early 19th century.

After working on his difference engine he announced his invention in 1822, in a paper to the Royal Astronomical Society, titled "Note on the application of machinery to the computation of astronomical and mathematical tables".[23] He also designed to aid in navigational calculations, in 1833 he realized that a much more general design, an analytical engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The engine would incorporate an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[24][25]

The machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.

Electromechanical calculating machine

Electro-mechanical calculator (1920) by Leonardo Torres Quevedo.
In his work Essays on Automatics published in 1914, Leonardo Torres Quevedo wrote a brief history of Babbage's efforts at constructing a mechanical Difference Engine and Analytical Engine. The paper contains a design of a machine capable to calculate formulas like 

 for a sequence of sets of values. The whole machine was to be controlled by a read-only program, which was complete with provisions for conditional branching. He also introduced the idea of floating-point arithmetic.[26][27][28] In 1920, to celebrate the 100th anniversary of the invention of the arithmometer, Torres presented in Paris the Electromechanical Arithmometer, which allowed a user to input arithmetic problems through a keyboard, and computed and printed the results,[29][30][31][32] demonstrating the feasibility of an electromechanical analytical engine.[33]

Analog computers

An air quality index (AQI) is an approximation of how polluted the air currently is or how polluted it is forecast to become. As air pollution levels rise, so does the AQI, along with the associated public- health risk.

Different countries have their own air quality indices, corresponding to different national air quality standards. These include Canada's Air Quality Health Index, Malaysia's Air Pollution Index, and Singapore's Pollutant Standards Index. Pollutants that are commonly monitored include ground-level ozone, particulates, sulfur dioxide, carbon monoxide, and nitrogen dioxide.

Children, the elderly, and individuals with respiratory or cardiovascular problems are typically the first groups affected by poor air quality. When the AQI is high, governmental bodies generally encourage people to reduce physical activity outdoors, or even avoid going out altogether.

Overview

An air quality measurement station in Edinburgh, Scotland
Computation of the AQI requires an air pollutant concentration over a specified averaging period, obtained from an air monitor or model. Taken together, concentration and time represent the dose of the air pollutant. Health effects corresponding to a given dose are established by epidemiological research.[1] Air pollutants vary in potency, and the function used to convert from air pollutant concentration to AQI varies by pollutant. Its air - quality index values are typically grouped into ranges. Each range is assigned a descriptor, a color code, and a standardized public health advisory.

The AQI can increase due to an increase of air emissions. For example, during rush hour traffic, or when there is an upwind forest fire or from a lack of dilution of air pollutants. Stagnant air, often caused by an anticyclone, temperature inversion, or low wind speeds lets air pollution remain in a local area, leading to high concentrations of pollutants, chemical reactions between air contaminants and hazy conditions.[2]


Signboard in Gulfton, Houston, Texas, indicating an ozone watch
On a day when the AQI is predicted to be elevated due to fine particle pollution, an agency or public - health organization might:

advise sensitive groups, such as the elderly, children, and those with respiratory or cardiovascular problems or suffering from diseases, to avoid outdoor exertion.[3]
declare an "action day" to encourage voluntary measures to curtail air emissions, such as using public transportation.[4]
recommend the use of masks outdoors and air purifiers indoors to prevent fine particles from entering the lungs..[5]
During a period of very poor air quality, such as an air - pollution episode, when the AQI indicates that acute exposure may cause significant harm to the public health, agencies may invoke emergency plans that allow them to order major emitters (such as coal burning industries) to curtail emissions until the hazardous conditions abate.[6]

Most air contaminants do not have an associated AQI. Many countries monitor ground-level ozone, particulates, sulfur dioxide, carbon monoxide and, nitrogen dioxide, and calculate air quality indices for these pollutants.[7]

The definition of the AQI in a particular nation reflects the discourse surrounding the development of national air - quality standards in that nation.[8] A website allowing government agencies anywhere in the world to submit their real-time air monitoring data for display using a common definition of the air quality index has recently become available.[9]

Indices by location
Australia
Each of the states and territories of Australia is responsible for monitoring air - quality and publishing data in accordance with the National Environment Protection (Ambient Air Quality) Measure (NEPM) standards.[10]

Each state and territory publishes air- quality data for individual monitoring locations, and most states and territories publish air -quality indexes for each monitoring location.

Across Australia, a consistent approach is taken with air - quality indexes, using a simple linear scale where 100 represents the maximum concentration standard for each pollutant, as set by the NEPM. These maximum concentration standards are:

Pollutant	Averaging period	Maximum concentration standard
Carbon monoxide	8 hours	9 ppm
Nitrogen dioxide	1 hour	0.12 ppm
1 year	0.03 ppm
Ozone	1 hour	0.10 ppm
4 hours	0.08 ppm
Sulfur dioxide	1 hour	0.20 ppm
1 day	0.08 ppm
1 year	0.02 ppm
Lead	1 year	0.50 μg/m3
PM10	1 day	50 μg/m3
1 year	25 μg/m3
PM2.5	1 day	25 μg/m3
1 year	8 μg/m3

China,[h] officially the People's Republic of China (PRC),[i] is a country in East Asia. With a population exceeding 1.4 billion, it is the second-most populous country after India, representing 17% of the world population. China borders fourteen countries by land[j] across an area of 9.6 million square kilometers (3,700,000 sq mi), making it the third-largest country by area.[k] The country is divided into 33 province-level divisions: 22 provinces,[l] 5 autonomous regions, 4 municipalities, and 2 semi-autonomous special administrative regions. Beijing is the capital, while Shanghai is the most populous city by urban area and largest financial center.

China saw the first humans in the region arriving during the Paleolithic era. By the 2nd millennium BCE dynastic states had emerged in the Yellow River basin. The 8th–3rd centuries BCE saw a breakdown in the authority of the Zhou dynasty, accompanied by the emergence of administrative and military techniques, literature and philosophy. In 221 BCE, China was unified under an emperor, ushering in two millennia of imperial dynasties. With the invention of gunpowder and paper, the establishment of the Silk Road, and the Great Wall, Chinese culture flourished and has heavily influenced its neighbors and lands further afield. China began to cede parts of the country in the 19th century, to European powers by a series of unequal treaties. The 1911 Revolution overthrew the Qing dynasty and the Republic of China was established the following year. The country was unstable and fragmented during the Warlord Era, which ended upon the Northern Expedition conducted by the Kuomintang to reunify the country.

The Chinese Civil War began in 1927, when Kuomintang forces purged members of the Chinese Communist Party (CCP). China was invaded by the Empire of Japan in 1937, leading the CCP and Kuomintang to form the Second United Front to fight the Japanese. The Second Sino-Japanese War ended in a Chinese victory; however, the CCP and the Kuomintang resumed their civil war. In 1949, the CCP proclaimed the People's Republic of China and forced the Kuomintang-led government to retreat to the island of Taiwan. The country was split, with both sides claiming to be the legitimate government. Following the implementation of land reforms, attempts by the CCP to realize communism failed: the Great Leap Forward was responsible for the Great Chinese Famine which resulted in millions of deaths, and the Cultural Revolution was a period of turmoil and persecution. The reform and opening up that began in 1978 moved the country away from a socialist planned economy towards a market-based economy, spurring an economic boom. A movement for political liberalization stalled after the 1989 Tiananmen Square protests and massacre.

Since 1949, China has been a unitary communist state with the CCP as its sole ruling party. It is one of the five permanent members of the UN Security Council and a member of numerous multilateral and regional organizations. Making up around one-fifth of the world economy, the Chinese economy is the world's largest by PPP-adjusted GDP. China is the second-wealthiest country, albeit ranking poorly in measures of democracy and human rights. The country has been one of the fastest-growing economies and is the world's largest manufacturer and exporter, as well as the second-largest importer. China is a nuclear-weapon state with the world's largest standing army and the second-largest defense budget. It is described as either a potential or established superpower due to its influence in the fields of geopolitics, technology, manufacturing, economics and culture. China is known for its cuisine and culture. It is a megadiverse country, and has 60 UNESCO World Heritage Sites.

Etymology
Main article: Names of China

China (today's Guangdong), Mangi (inland of Xanton), and Cataio (inland of China and Chequan, and including the capital Cambalu, Xandu, and a marble bridge) are all shown as separate regions on this 1570 map by Abraham Ortelius.
The word "China" has been used in English since the 16th century; however, it was not used by the Chinese themselves during this period. Its origin has been traced through Portuguese, Malay, and Persian back to the Sanskrit word Cīna, used in ancient India.[14] "China" appears in Richard Eden's 1555 translation[m] of the 1516 journal of the Portuguese explorer Duarte Barbosa.[n][14] Barbosa's usage was derived from Persian Chīn (چین), which in turn derived from Sanskrit Cīna (चीन).[19] The origin of the Sanskrit word is a matter of debate.[14] Cīna was first used in early Hindu scripture, including the Mahabharata (3rd century BCE–4th century CE) and the Laws of Manu (2nd century BCE–2nd century CE).[20] In 1655, Martino Martini suggested that the word China is derived ultimately from the name of the Qin dynasty (221–206 BCE) or the prior state of Qin.[21][20] Although use in Indian sources precedes this dynasty, though not the state, this derivation is still given in various sources.[22] Alternative suggestions include the names for Yelang and the Jing or Chu state.[20][23]

The official name of the modern state is the "People's Republic of China" (simplified Chinese: 中华人民共和国; traditional Chinese: 中華人民共和國; pinyin: Zhōnghuá rénmín gònghéguó). The shorter form is "China" (中国; 中國; Zhōngguó), from zhōng ('central') and guó ('state'), a term which developed under the Western Zhou dynasty in reference to its royal demesne.[o][p] It was used in official documents as an synonym for the state under the Qing.[26] The name Zhongguo is also translated as 'Middle Kingdom' in English.[27] China is sometimes referred to as mainland China or "the Mainland" when distinguishing it from the Republic of China or the PRC's Special Administrative Regions.[28][29][30]

History
Main article: History of China
For a chronological guide, see Timeline of Chinese history.
Prehistory

10,000-year-old pottery, Xianren Cave culture (18000–7000 BCE)
Archaeological evidence suggests that early hominids inhabited China 2.25 million years ago.[31] The hominid fossils of Peking Man, a Homo erectus who used fire,[32] have been dated to between 680,000 and 780,000 years ago.[33] The fossilized teeth of Homo sapiens (dated to 125,000–80,000 years ago) have been discovered in Fuyan Cave.[34] Chinese proto-writing existed in Jiahu around 6600 BCE,[35] at Damaidi around 6000 BCE,[36] Dadiwan from 5800 to 5400 BCE, and Banpo dating from the 5th millennium BCE. Some scholars have suggested that the Jiahu symbols (7th millennium BCE) constituted the earliest Chinese writing system.[35]The United States of America (USA), also known as the United States (U.S.) or America, is a country primarily located in North America. It is a federal republic of 50 states and a federal capital district, Washington, D.C. The 48 contiguous states border Canada to the north and Mexico to the south, with the semi-exclave of Alaska in the northwest and the archipelago of Hawaii in the Pacific Ocean. The United States also asserts sovereignty over five major island territories and various uninhabited islands in Oceania and the Caribbean.[j] It is a megadiverse country, with the world's third-largest land area[c] and third-largest population, exceeding 340 million.[k]

Paleo-Indians first migrated from North Asia to North America over 12,000 years ago, and formed various civilizations. Spanish colonization established Spanish Florida in 1513, the first European colony in what is now the continental United States. British colonization followed with the 1607 settlement of Virginia, the first of the Thirteen Colonies. Enslavement of Africans was practiced in all colonies by 1770 and supplied most of the labor for the Southern Colonies' plantation economy. Clashes with the British Crown over taxation and lack of parliamentary representation sparked the American Revolution, leading to the Declaration of Independence on July 4, 1776. Victory in the 1775–1783 Revolutionary War brought international recognition of U.S. sovereignty and fueled westward expansion, dispossessing native inhabitants. As more states were admitted, a North–South division over slavery led the Confederate States of America to attempt secession and fight the Union in the 1861–1865 American Civil War. With the United States' victory and reunification, slavery was abolished nationally. By 1900, the country had established itself as a great power, a status solidified after its involvement in World War I. Following Japan's attack on Pearl Harbor in 1941, the U.S. entered World War II. Its aftermath left the U.S. and the Soviet Union as rival superpowers, competing for ideological dominance and international influence during the Cold War. The Soviet Union's collapse in 1991 ended the Cold War, leaving the U.S. as the world's sole superpower.

The U.S. national government is a presidential constitutional federal republic and representative democracy with three separate branches: legislative, executive, and judicial. It has a bicameral national legislature composed of the House of Representatives (a lower house based on population) and the Senate (an upper house based on equal representation for each state). Federalism grants substantial autonomy to the 50 states. In addition, 574 Native American tribes have sovereignty rights, and there are 326 Native American reservations. Since the 1850s, the Democratic and Republican parties have dominated American politics, while American values are based on a democratic tradition inspired by the American Enlightenment movement.

A developed country, the U.S. ranks high in economic competitiveness, innovation, and higher education. Accounting for over a quarter of nominal global GDP, its economy has been the world's largest since about 1890. It is the wealthiest country, with the highest disposable household income per capita among OECD members, though its wealth inequality is highly pronounced. Shaped by centuries of immigration, the culture of the U.S. is diverse and globally influential. Making up more than a third of global military spending, the country has one of the strongest militaries and is a designated nuclear state. A member of numerous international organizations, the U.S. plays a major role in global political, cultural, economic, and military affairs.

Etymology
Further information: Names of the United States, Demonyms for the United States, United Colonies, and Naming of the Americas
Documented use of the phrase "United States of America" dates back to January 2, 1776. On that day, Stephen Moylan, a Continental Army aide to General George Washington, wrote a letter to Joseph Reed, Washington's aide-de-camp, seeking to go "with full and ample powers from the United States of America to Spain" to seek assistance in the Revolutionary War effort.[22][23] The first known public usage is an anonymous essay published in the Williamsburg newspaper The Virginia Gazette on April 6, 1776.[22] Sometime on or after June 11, 1776, Thomas Jefferson wrote "United States of America" in a rough draft of the Declaration of Independence,[22] which was adopted by the Second Continental Congress on July 4, 1776.[24]

The term "United States" and its initialism "U.S.", used as nouns or as adjectives in English, are common short names for the country. The initialism "USA", a noun, is also common.[25] "United States" and "U.S." are the established terms throughout the U.S. federal government, with prescribed rules.[l] "The States" is an established colloquial shortening of the name, used particularly from abroad;[27] "stateside" is the corresponding adjective or adverb.[28]

"America" is the feminine form of the first word of Americus Vesputius, the Latinized name of Italian explorer Amerigo Vespucci (1454–1512);[m] it was first used as a place name by the German cartographers Martin Waldseemüller and Matthias Ringmann in 1507.[29][n] Vespucci first proposed that the West Indies discovered by Christopher Columbus in 1492 were part of a previously unknown landmass and not among the Indies at the eastern limit of Asia.[30][31][32] In English, the term "America" usually does not refer to topics unrelated to the United States, despite the usage of "the Americas" to describe the totality of the continents of North and South America.[33]

History
Main article: History of the United States
For a topical guide, see Outline of the history of the United States.
Indigenous peoples
Main articles: History of Native Americans in the United States and Pre-Columbian era

Cliff Palace, a settlement of ancestors of the Native American Pueblo peoples in present-day Montezuma County, Colorado, built between c. 1200 and 1275[34]
The first inhabitants of North America migrated from Siberia over 12,000 years ago, either across the Bering land bridge or along the now-submerged Ice Age coastline.[35][36] The Clovis culture, which appeared around 11,000 BC, is believed to be the first widespread culture in the Americas.[37][38] Over time, Indigenous North American cultures grew increasingly sophisticated, and some, such as the Mississippian culture, developed agriculture, architecture, and complex societies.[39] In the post-archaic period, the Mississippian cultures were located in the midwestern, eastern, and southern regions, and the Algonquian in the Great Lakes region and along the Eastern Seaboard, while the Hohokam culture and Ancestral Puebloans inhabited the Southwest.[40] Native population estimates of what is now the United States before the arrival of European immigrants range from around 500,000[41][42] to nearly 10 million.[42][43]

European exploration, colonization and conflict (1513–1765)
Main articles: Colonial history of the United States and Colonial American military history

The colonial possessions of Britain (the Thirteen Colonies in pink and others in purple), France (in blue), and Spain (in orange) in North America, 1750
Christopher Columbus began exploring the Caribbean for Spain in 1492, leading to Spanish-speaking settlements and missions from what are now Puerto Rico and Florida to New Mexico and California. The first Spanish colony in the present-day continental United States was Spanish Florida, chartered in 1513.[44][45][46][47] After several settlements failed there due to starvation and disease, Spain's first permanent town, Saint Augustine, was founded in 1565.[48]

France established its own settlements in French Florida in 1562, but they were either abandoned (Charlesfort, 1578) or destroyed by Spanish raids (Fort Caroline, 1565). Permanent French settlements were founded much later along the Great Lakes (Fort Detroit, 1701), the Mississippi River (Saint Louis, 1764) and especially the Gulf of Mexico (New Orleans, 1718).[49] Early European colonies also included the thriving Dutch colony of New Nederland (settled 1626, present-day New York) and the small Swedish colony of New Sweden (settled 1638 in what became Delaware). British colonization of the East Coast began with the Virginia Colony (1607) and the Plymouth Colony (Massachusetts, 1620).[50][51]

The Mayflower Compact in Massachusetts and the Fundamental Orders of Connecticut established precedents for local representative self-governance and constitutionalism that would develop throughout the American colonies.[52][53] While European settlers in what is now the United States experienced conflicts with Native Americans, they also engaged in trade, exchanging European tools for food and animal pelts.[54][o] Relations ranged from close cooperation to warfare and massacres. The colonial authorities often pursued policies that forced Native Americans to adopt European lifestyles, including conversion to Christianity.[58][59] Along the eastern seaboard, settlers trafficked African slaves through the Atlantic slave trade.[60]

The original Thirteen Colonies[p] that would later found the United States were administered as possessions of the British Empire by Crown-appointed governors,[61] though local governments held elections open to most white male property owners.[62][63] The colonial population grew rapidly from Maine to Georgia, eclipsing Native American populations;[64] by the 1770s, the natural increase of the population was such that only a small minority of Americans had been born overseas.[65] The colonies' distance from Britain facilitated the entrenchment of self-governance,[66] and the First Great Awakening, a series of Christian revivals, fueled colonial interest in guaranteed religious liberty.[67]

American Revolution an

The Fountainhead is a 1943 novel by Russian-born American author Ayn Rand, her first major literary success. The novel's protagonist, Howard Roark, is an intransigent young architect who battles against conventional standards and refuses to compromise with an architectural establishment unwilling to accept innovation. Roark embodies what Rand believed to be the ideal man, and his struggle reflects Rand's belief that individualism is superior to collectivism.

Roark is opposed by what he calls "second-handers", who value conformity over independence and integrity. These include Roark's former classmate, Peter Keating, who succeeds by following popular styles but turns to Roark for help with design problems. Ellsworth Toohey, a socialist architecture critic who uses his influence to promote his political and social agenda, tries to destroy Roark's career. Tabloid newspaper publisher Gail Wynand seeks to shape popular opinion; he befriends Roark, then betrays him when public opinion turns in a direction he cannot control. The novel's most controversial character is Roark's lover, Dominique Francon. She believes that non-conformity has no chance of winning, so she alternates between helping Roark and working to undermine him.

Twelve publishers rejected the manuscript before an editor at the Bobbs-Merrill Company risked his job to get it published. Contemporary reviewers' opinions were polarized. Some praised the novel as a powerful paean to individualism, while others thought it overlong and lacking sympathetic characters. Initial sales were slow, but the book gained a following by word of mouth and became a bestseller. More than 10 million copies of The Fountainhead have been sold worldwide, and it has been translated into more than 30 languages. The novel attracted a new following for Rand and has enjoyed a lasting influence, especially among architects, entrepreneurs, American conservatives, and libertarians.[1]

The novel has been adapted into other media several times. An illustrated version was syndicated in newspapers in 1945. Warner Bros. produced a film version in 1949; Rand wrote the screenplay, and Gary Cooper played Roark. Critics panned the film, which did not recoup its budget; several directors and writers have considered developing a new film adaptation. In 2014, Belgian theater director Ivo van Hove created a stage adaptation, which received mixed reviews.

Plot
Black and white photo of Ayn Rand
Ayn Rand in 1943
In early 1922, Howard Roark is expelled from the architecture department of the Stanton Institute of Technology because he has not adhered to the school's preference for historical convention in building design. Roark goes to New York City and gets a job with Henry Cameron. Cameron was once a renowned architect, but now gets few commissions. In the meantime, Roark's popular but vacuous fellow student and housemate Peter Keating (whom Roark sometimes helped with projects) graduates with high honors. He too moves to New York, where he has been offered a position with the prestigious architecture firm, Francon & Heyer. Keating ingratiates himself with Guy Francon and works to remove rivals among his coworkers. After Francon's partner, Lucius Heyer, suffers a fatal stroke brought on by Keating's antagonism, Francon chooses Keating to replace him. Meanwhile, Roark and Cameron create inspired work, but struggle financially.

After Cameron retires, Keating hires Roark, whom Francon soon fires for refusing to design a building in the classical style. Roark works briefly at another firm, then opens his own office but has trouble finding clients and closes it down. He gets a job in a granite quarry owned by Francon. There he meets Francon's daughter Dominique, a columnist for The New York Banner, while she is staying at her family's estate nearby. They are immediately attracted to each other, leading to a rough sexual encounter that Dominique later calls a rape.[2] Shortly after, Roark is notified that a client is ready to start a new building, and he returns to New York. Dominique also returns to New York and learns that Roark is an architect. She attacks his work in public, but visits him for secret sexual encounters.

Ellsworth M. Toohey, who writes a popular architecture column in the Banner, is an outspoken socialist who shapes public opinion through his column and a circle of influential associates. Toohey sets out to destroy Roark through a smear campaign. He recommends Roark to Hopton Stoddard, a wealthy acquaintance who wants to build a Temple of the Human Spirit. Roark's unusual design includes a nude statue modeled on Dominique; Toohey persuades Stoddard to sue Roark for malpractice. Toohey and several architects (including Keating) testify at the trial that Roark is incompetent as an architect for his rejection of historical styles. Dominique also argues for the prosecution in tones that can be interpreted to be speaking more in Roark's defense than for the plaintiff, but he loses the case. Dominique decides that since she cannot have the world she wants, in which men like Roark are recognized for their greatness, she will live entirely in the world she has, which shuns Roark and praises Keating. She marries Keating and turns herself over to him, doing and saying whatever he wants, and actively persuading potential clients to hire him instead of Roark.

To win Keating a prestigious commission offered by Gail Wynand, the owner and editor-in-chief of the Banner, Dominique agrees to sleep with Wynand. Wynand is so strongly attracted to Dominique that he pays Keating to divorce her, after which Wynand and Dominique marry. Wanting to build a home for himself and his new wife, Wynand discovers that Roark designed every building he likes and so hires him. Roark and Wynand become close friends; Wynand is unaware of Roark's past relationship with Dominique.

Washed up and out of the public eye, Keating pleads with Toohey to use his influence to get the commission for the much-sought-after Cortlandt housing project. Keating knows his most successful projects were aided by Roark, so he asks for Roark's help in designing Cortlandt. Roark agrees in exchange for complete anonymity and Keating's promise that it will be built exactly as designed. After taking a long vacation with Wynand, Roark returns to find that Keating was not able to prevent major changes from being made in Cortlandt's construction. Roark dynamites the project to prevent the subversion of his vision.

Roark is arrested and his action is widely condemned, but Wynand decides to use his papers to defend his friend. This unpopular stance hurts the circulation of his newspapers, and Wynand's employees go on strike after Wynand dismisses Toohey for disobeying him and criticizing Roark. Faced with the prospect of closing the paper, Wynand gives in and publishes a denunciation of Roark. At his trial, Roark makes a lengthy speech about the value of ego and integrity, and he is found not guilty. Dominique leaves Wynand for Roark. Wynand, who has betrayed his own values by attacking Roark, finally grasps the nature of the power he thought he held. He shuts down the Banner and commissions a final building from Roark, a skyscraper that will serve as a monument to human achievement. Eighteen months later, the Wynand Building is under construction. Dominique, now Roark's wife, enters the site to meet him atop its steel framework.

Major characters
Howard Roark
Portrait photo of Frank Lloyd Wright
In writing the character of Howard Roark, Rand was inspired by the architect Frank Lloyd Wright.
Rand's stated goal in writing fiction was to portray her vision of an ideal man.[3][4] The character of Howard Roark, the protagonist of The Fountainhead, was the first instance where she believed she had achieved this.[5] Roark embodies Rand's egoistic moral ideals,[6] especially the virtues of independence[7] and integrity.[8]

The character of Roark was at least partly inspired by American architect Frank Lloyd Wright. Rand described the inspiration as limited to specific ideas he had about architecture and "the pattern of his career".[9] She denied that Wright had anything to do with the philosophy expressed by Roark or the events of the plot.[10][11] Rand's denials have not stopped commentators from claiming stronger connections between Wright and Roark.[11][12] Wright equivocated about whether he thought Roark was based on him, sometimes implying that he did, at other times denying it.[13] Wright biographer Ada Louise Huxtable described significant differences between Wright's philosophy and Rand's and quoted him, declaring, "I deny the paternity and refuse to marry the mother."[14] Architecture critic Martin Filler said that Roark resembles the Swiss-French modernist architect Le Corbusier more closely than Wright.[15]

Peter Keating


Atlas Shrugged is a 1957 novel by Ayn Rand. It is her longest novel, the fourth and final one published during her lifetime, and the one she considered her magnum opus in the realm of fiction writing.[1] She described the theme of Atlas Shrugged as "the role of man's mind in existence" and it includes elements of science fiction, mystery, and romance. The book explores a number of philosophical themes from which Rand would subsequently develop Objectivism, including reason, property rights, individualism, libertarianism, and capitalism and depicts what Rand saw as the failures of governmental coercion. Of Rand's works of fiction, it contains her most extensive statement of her philosophical system.

The book depicts a dystopian United States in which heavy industry companies suffer under increasingly burdensome laws and regulations. Railroad executive Dagny Taggart and her lover, steel magnate Hank Rearden, struggle against "looters" who want to exploit their productivity. They discover that a mysterious figure called John Galt is persuading other business leaders to abandon their companies and disappear as a strike of productive individuals against the looters. The novel ends with the strikers planning to build a new capitalist society based on Galt's philosophy.

Atlas Shrugged received largely negative reviews, but achieved enduring popularity and ongoing sales in the following decades. The novel has been cited as an influence on a variety of libertarian and conservative thinkers and politicians. After several unsuccessful attempts to adapt the novel for film or television, a film trilogy was released from 2011 to 2014 to negative reviews; two theatrical adaptations have also been staged.

Synopsis
Setting
Atlas Shrugged is set in a dystopian United States at an unspecified time, in which the country has a "national legislature" instead of Congress and a "head of state" instead of a president. The United States appears to be approaching an economic collapse, with widespread shortages, business failures, and decreased productivity. Writer Edward Younkins said, "The story may be simultaneously described as anachronistic and timeless. The pattern of industrial organization appears to be that of the late 1800s—the mood seems to be close to that of the depression-era 1930s. Both the social customs and the level of technology remind one of the 1950s".[2]

Many early 20th-century technologies are available, but later technologies such as jet planes and computers are largely absent.[3] There is very little mention of historical people or events, not even major events such as World War II.[4] Aside from the United States, most countries are referred to as "People's States" that are implied to be either socialist or communist.[2][5]

Plot
See also: List of Atlas Shrugged characters
A diesel-engine train sitting at a station
Rand studied operations of the New York Central Railroad as research for the story.
Dagny Taggart, the operating vice-president of Taggart Transcontinental Railroad, keeps the company going amid a sustained economic depression. As economic conditions worsen and government enforces statist controls on successful businesses, people repeat the cryptic phrase "Who is John Galt?" which means: "Don't ask questions nobody can answer."[6]

Her brother Jim, the railroad's president, seems to make irrational decisions, such as buying from Orren Boyle's unreliable Associated Steel. Dagny is also disappointed to discover that the Argentine billionaire Francisco d'Anconia, her childhood friend and first love, is risking his family's copper company by constructing the San Sebastián copper mines, even though Mexico will probably nationalize them.

Despite the risk, Jim and Boyle invest heavily in a railway for the region while ignoring the Rio Norte Line in Colorado, where entrepreneur Ellis Wyatt has discovered large oil reserves. Mexico nationalizes the mines and railroad line, but the mines are discovered to be worthless. To recoup the railroad's losses, Jim influences the National Alliance of Railroads to prohibit competition in prosperous areas such as Colorado. Wyatt demands that Dagny supply adequate rails to his wells before the ruling takes effect.

In Philadelphia, self-made steel magnate Hank Rearden develops Rearden Metal, an alloy lighter and stronger than conventional steel. Dagny opts to use Rearden Metal in the Rio Norte Line, becoming the first major customer for the product. After Hank refuses to sell the metal to the State Science Institute, a government research foundation run by Dr. Robert Stadler, the Institute publishes a report condemning the metal without identifying problems with it. As a result, many significant organizations boycott the line. Although Stadler agrees with Dagny's complaints about the unscientific tone of the report, he refuses to override it. To protect Taggart Transcontinental from the boycott, Dagny decides to build the Rio Norte Line as an independent company named the John Galt Line.

Hank is unhappy with his manipulative wife Lillian, but feels obliged to stay with her. He is attracted to Dagny, and when he joins her for the inauguration of the John Galt Line, they become lovers. On a vacation, Hank and Dagny discover an abandoned factory with an incomplete but revolutionary motor that runs on atmospheric static electricity. They begin searching for the inventor, and Dagny hires scientist Quentin Daniels to reconstruct the motor; however, a series of economically harmful directives are issued by Wesley Mouch, a former Rearden lobbyist who betrayed Hank in return for a job leading a government agency. Wyatt and other important business leaders quit and disappear, leaving their industries to failure.

Dagny and Hank realize that Francisco is hurting his copper company intentionally, although they do not understand why. When the government imposes a directive that forbids employees from leaving their jobs and nationalizes all patents, Dagny violates the law by resigning in protest. To gain Hank's compliance, the government blackmails him with threats to publicize his affair with Dagny. After a major disaster in one of Taggart Transcontinental's tunnels, Dagny returns to work. On her return, she receives notice that Quentin Daniels is also quitting in protest and she rushes across the country to convince him to stay.

Photo of the town of Ouray
Ouray, Colorado, was the basis for Rand's descriptions of Galt's Gulch.
On her way to Daniels, Dagny meets a hobo with a story that reveals the motor was invented and abandoned by an engineer named John Galt, who is the inspiration for the common saying. When she chases after Daniels in a private plane, she crashes and discovers the secret behind the disappearances of business leaders: Galt is leading a strike of "the men of the mind". She has crashed in their hiding place, an isolated valley known as Galt's Gulch. As she recovers from her injuries, the strikers explain their motives, and she learns that the strikers include Francisco and many prominent people, such as her favorite composer, Richard Halley, and infamous pirate Ragnar Danneskjöld. Dagny falls in love with Galt, who asks her to join the strike.

Reluctant to abandon her railroad, Dagny leaves Galt's Gulch, but finds the government has devolved into dictatorship. Francisco finishes sabotaging his mines and quits. After he helps stop an armed takeover of Hank's steel mill, Francisco convinces Hank to join the strike. Galt follows Dagny to New York, where he hacks into a national radio broadcast to deliver a three-hour speech that explains the novel's theme and Rand's Objectivism.[7]

The authorities capture Galt and unsuccessfully attempt to persuade him to lead the restoration of the country's economy. Jim then decides to torture Galt, but becomes delirious after witnessing how the authorities are too incompetent to even fix the torture device. Dagny rescues Galt, the government collapses, and the novel closes as Galt announces that the strikers can rejoin the world.

History
Context and writing
Photo of Ayn Rand
Ayn Rand in 1943
Rand's stated goal for writing the novel was "to show how desperately the world needs prime movers and how viciously it treats them" and to portray "what happens to the world without them".[8] The core idea for the book came to her during a 1943 telephone conversation with her friend Isabel Paterson, who asserted that Rand owed it to her readers to write fiction about her philosophy. Rand disagreed and replied, "What if I went on strike? What if all the creative minds of the world went on strike? ... That would make a good novel". After the conversation ended, Rand's husband Frank O'Connor, who had overheard, affirmed to Rand, "That would make a good novel."[9] Rand then began Atlas Shrugged to depict the morality of rational self-interest,[10] by exploring the consequences of a strike by intellectuals refusing to supply their inventions, art, business leadership, scientific research, or new ideas to the rest of the world.[11]

Rand began the first draft of the novel on September 2, 1946.[12] She initially thought it would be easy to write and completed quickly, but as she considered the complexity of the philosophical issues she wanted to address, she realized it would take longer.[13] After ending a contract to write screenplays for Hal Wallis and finishing her obligations for the film adaptation of The Fountainhead, Rand worked full-time on the novel that she tentatively titled The Strike. By the summer of 1950, she had written 18 chapters;[14] by September 1951, she had written 21 chapters and was working on the last of the novel's three sections.[15]

As Rand completed new chapters, she read them to a circle of young admirers who had begun gathering at her home to discuss philosophy. This group included Nathaniel Branden, his wife Barbara Branden, Barbara's cousin Leonard Peikoff, and economist Alan Greenspan.[16] Progress on the novel slowed considerably in 1953, when Rand began working on Galt's lengthy radio address. She spent more than two years completing the speech, finishing it on October 13, 1955.[17] The remaining chapters proceeded more quickly, and by November 1956 Rand was ready to submit the almost-completed manuscript to publishers.[18] Atlas Shrugged was Rand's last completed work of fiction. It marked a turning point in her life—the end of her career as a novelist and the beginning of her role as a popular philosopher.[19][20]

Influences
Photo of J. Robert Oppenheimer
Rand used interviews with scientist J. Robert Oppenheimer for the character Robert Stadler.
Rand biographer Anne Heller traces some ideas that would go into Atlas Shrugged back to a never-written novel that Rand outlined when she was a student at Petrograd State University. The futuristic story featured an American heiress luring the most talented men away from a mostly communist Europe. The heiress would have had an assistant called Eddie Willers, the name of Dagny's assistant in Atlas Shrugged.[21]

To depict the industrial setting of Atlas Shrugged, Rand conducted research on the American railroad and steel industries. She toured and inspected a number of industrial facilities, such as the Kaiser Steel plant,[22] visited facilities of the New York Central Railroad,[23][24] and briefly operated a locomotive on the Twentieth Century Limited.[25] Rand also used her previous research for an uncompleted screenplay about the development of the atomic bomb, including her interviews of J. Robert Oppenheimer, which influenced the character Robert Stadler and the novel's depiction of the development of "Project X".[26]

Rand's descriptions of Galt's Gulch were based on the town of Ouray, Colorado, which Rand and her husband visited in 1951 when they were relocating from Los Angeles to New York.[15] Other details of the novel were affected by the experiences and comments of her friends. For example, her portrayal of leftist intellectuals (such as the characters Balph Eubank and Simon Pritchett) was influenced by the college experiences of Nathaniel and Barbara Branden,[27] and Alan Greenspan provided information on the economics of the steel industry.[28]

American libertarian writer Justin Raimondo described similarities between Atlas Shrugged and Garet Garrett's 1922 novel The Driver, which is about an idealized industrialist named Henry Galt, who is a transcontinental railway owner trying to improve the world and fighting against government and socialism.[29] Raimondo believed the earlier novel influenced Rand's writing in ways she failed to acknowledge, although there was no "word-for-word plagiarism" and The Driver was published four years before Rand emigrated to the United States.[30] Journalist Jeff Walker echoed Raimondo's comparisons in his book The Ayn Rand Cult and listed The Driver as one of several unacknowledged precursors to Atlas Shrugged.[31] In contrast, Chris Matthew Sciabarra said he "could not find any evidence to link Rand to Garrett",[32] and considered Raimondo's claims to be "unsupported".[33] Liberty magazine editor R. W. Bradford said Raimondo made an unconvincing comparison based on a coincidence of names and common literary devices.[34]

Publishing history
Photo of Bennet Cerf
Random House CEO Bennett Cerf oversaw the novel's publication in 1957.
Due to the success of Rand's 1943 novel The Fountainhead, she had no trouble attracting a publisher for Atlas Shrugged. This was a contrast to her previous novels, which she had struggled to place. Even before she began writing it, she had been approached by publishers interested in her next novel. However, her contract for The Fountainhead gave the first option to its publisher, Bobbs-Merrill Company. After reviewing a partial manuscript, they asked her to discuss cuts and other changes. She refused, and Bobbs-Merrill rejected the book.[35]

Hiram Hayden, an editor she liked who had left Bobbs-Merrill, asked her to consider his new employer, Random House. In an early discussion about the difficulties of publishing a controversial novel, Random House president Bennett Cerf proposed that Rand should submit the manuscript to multiple publishers simultaneously and ask how they would respond to its ideas, so she could evaluate who might best promote her work. Rand was impressed by the bold suggestion and by her overall conversations with them. After speaking with a few other publishers from about a dozen who were interested, Rand decided multiple submissions were not needed; she offered the manuscript to Random House. Upon reading the portion Rand submitted, Cerf declared it a "great book" and offered Rand a contract. It was the first time Rand had worked with a publisher whose executives seemed enthusiastic about one of her books.[36]

When the completed manuscript exceeded 600,000 words, Cerf asked Rand to make cuts, but backed off when she compared the idea to cutting the Bible.[37] With 1168 pages in the first edition, Atlas Shrugged is Rand's longest published book.[38] Random House published the novel on October 10, 1957. The initial print run was 100,000 copies. The first paperback edition was published by New American Library in July 1959, with an initial run of 150,000.[39] A 35th-anniversary edition was published by E. P. Dutton in 1992, with an introduction by Rand's heir, Leonard Peikoff.[40] The novel has been translated into more than 30 languages.[a]

Title and chapters
Painting of Atlas holding a sphere
The title refers to the mythological Atlas.
The working title of the novel was The Strike, but Rand thought this title would reveal the mystery element of the novel prematurely.[42] She was pleased when her husband suggested Atlas Shrugged, previously the title of a single chapter, for the book.[43] The title is a reference to Atlas, a Titan in Greek mythology, who is described in the novel as "the giant who holds the world on his shoulders".[b] The significance of this reference appears in a conversation in which Francisco d'Anconia asks Rearden what advice he would give Atlas if "the greater [the Titan's] effort, the heavier the world bore down on his shoulders". With Rearden unable to answer, d'Anconia gives his own advice: "To shrug".[45]

The novel is divided into three parts consisting of ten chapters each. Each part is named in honor of one of Aristotle's laws of logic: "Non-Contradiction" after the law of noncontradiction; "Either-Or", which is a reference to the law of excluded middle; and "A Is A" in reference to the law of identity.[46] Each chapter also has a title; Atlas Shrugged is the only one of Rand's novels to use chapter titles.[47]

Atlas Shrugged is a 1957 novel by Ayn Rand. It is her longest novel, the fourth and final one published during her lifetime, and the one she considered her magnum opus in the realm of fiction writing.[1] She described the theme of Atlas Shrugged as "the role of man's mind in existence" and it includes elements of science fiction, mystery, and romance. The book explores a number of philosophical themes from which Rand would subsequently develop Objectivism, including reason, property rights, individualism, libertarianism, and capitalism and depicts what Rand saw as the failures of governmental coercion. Of Rand's works of fiction, it contains her most extensive statement of her philosophical system.

The book depicts a dystopian United States in which heavy industry companies suffer under increasingly burdensome laws and regulations. Railroad executive Dagny Taggart and her lover, steel magnate Hank Rearden, struggle against "looters" who want to exploit their productivity. They discover that a mysterious figure called John Galt is persuading other business leaders to abandon their companies and disappear as a strike of productive individuals against the looters. The novel ends with the strikers planning to build a new capitalist society based on Galt's philosophy.

Atlas Shrugged received largely negative reviews, but achieved enduring popularity and ongoing sales in the following decades. The novel has been cited as an influence on a variety of libertarian and conservative thinkers and politicians. After several unsuccessful attempts to adapt the novel for film or television, a film trilogy was released from 2011 to 2014 to negative reviews; two theatrical adaptations have also been staged.

Synopsis
Setting
Atlas Shrugged is set in a dystopian United States at an unspecified time, in which the country has a "national legislature" instead of Congress and a "head of state" instead of a president. The United States appears to be approaching an economic collapse, with widespread shortages, business failures, and decreased productivity. Writer Edward Younkins said, "The story may be simultaneously described as anachronistic and timeless. The pattern of industrial organization appears to be that of the late 1800s—the mood seems to be close to that of the depression-era 1930s. Both the social customs and the level of technology remind one of the 1950s".[2]

Many early 20th-century technologies are available, but later technologies such as jet planes and computers are largely absent.[3] There is very little mention of historical people or events, not even major events such as World War II.[4] Aside from the United States, most countries are referred to as "People's States" that are implied to be either socialist or communist.[2][5]

Plot
See also: List of Atlas Shrugged characters
A diesel-engine train sitting at a station
Rand studied operations of the New York Central Railroad as research for the story.
Dagny Taggart, the operating vice-president of Taggart Transcontinental Railroad, keeps the company going amid a sustained economic depression. As economic conditions worsen and government enforces statist controls on successful businesses, people repeat the cryptic phrase "Who is John Galt?" which means: "Don't ask questions nobody can answer."[6]

Her brother Jim, the railroad's president, seems to make irrational decisions, such as buying from Orren Boyle's unreliable Associated Steel. Dagny is also disappointed to discover that the Argentine billionaire Francisco d'Anconia, her childhood friend and first love, is risking his family's copper company by constructing the San Sebastián copper mines, even though Mexico will probably nationalize them.

Despite the risk, Jim and Boyle invest heavily in a railway for the region while ignoring the Rio Norte Line in Colorado, where entrepreneur Ellis Wyatt has discovered large oil reserves. Mexico nationalizes the mines and railroad line, but the mines are discovered to be worthless. To recoup the railroad's losses, Jim influences the National Alliance of Railroads to prohibit competition in prosperous areas such as Colorado. Wyatt demands that Dagny supply adequate rails to his wells before the ruling takes effect.

In Philadelphia, self-made steel magnate Hank Rearden develops Rearden Metal, an alloy lighter and stronger than conventional steel. Dagny opts to use Rearden Metal in the Rio Norte Line, becoming the first major customer for the product. After Hank refuses to sell the metal to the State Science Institute, a government research foundation run by Dr. Robert Stadler, the Institute publishes a report condemning the metal without identifying problems with it. As a result, many significant organizations boycott the line. Although Stadler agrees with Dagny's complaints about the unscientific tone of the report, he refuses to override it. To protect Taggart Transcontinental from the boycott, Dagny decides to build the Rio Norte Line as an independent company named the John Galt Line.

Hank is unhappy with his manipulative wife Lillian, but feels obliged to stay with her. He is attracted to Dagny, and when he joins her for the inauguration of the John Galt Line, they become lovers. On a vacation, Hank and Dagny discover an abandoned factory with an incomplete but revolutionary motor that runs on atmospheric static electricity. They begin searching for the inventor, and Dagny hires scientist Quentin Daniels to reconstruct the motor; however, a series of economically harmful directives are issued by Wesley Mouch, a former Rearden lobbyist who betrayed Hank in return for a job leading a government agency. Wyatt and other important business leaders quit and disappear, leaving their industries to failure.

Dagny and Hank realize that Francisco is hurting his copper company intentionally, although they do not understand why. When the government imposes a directive that forbids employees from leaving their jobs and nationalizes all patents, Dagny violates the law by resigning in protest. To gain Hank's compliance, the government blackmails him with threats to publicize his affair with Dagny. After a major disaster in one of Taggart Transcontinental's tunnels, Dagny returns to work. On her return, she receives notice that Quentin Daniels is also quitting in protest and she rushes across the country to convince him to stay.

Photo of the town of Ouray
Ouray, Colorado, was the basis for Rand's descriptions of Galt's Gulch.
On her way to Daniels, Dagny meets a hobo with a story that reveals the motor was invented and abandoned by an engineer named John Galt, who is the inspiration for the common saying. When she chases after Daniels in a private plane, she crashes and discovers the secret behind the disappearances of business leaders: Galt is leading a strike of "the men of the mind". She has crashed in their hiding place, an isolated valley known as Galt's Gulch. As she recovers from her injuries, the strikers explain their motives, and she learns that the strikers include Francisco and many prominent people, such as her favorite composer, Richard Halley, and infamous pirate Ragnar Danneskjöld. Dagny falls in love with Galt, who asks her to join the strike.

Reluctant to abandon her railroad, Dagny leaves Galt's Gulch, but finds the government has devolved into dictatorship. Francisco finishes sabotaging his mines and quits. After he helps stop an armed takeover of Hank's steel mill, Francisco convinces Hank to join the strike. Galt follows Dagny to New York, where he hacks into a national radio broadcast to deliver a three-hour speech that explains the novel's theme and Rand's Objectivism.[7]

The authorities capture Galt and unsuccessfully attempt to persuade him to lead the restoration of the country's economy. Jim then decides to torture Galt, but becomes delirious after witnessing how the authorities are too incompetent to even fix the torture device. Dagny rescues Galt, the government collapses, and the novel closes as Galt announces that the strikers can rejoin the world.

History
Context and writing
Photo of Ayn Rand
Ayn Rand in 1943
Rand's stated goal for writing the novel was "to show how desperately the world needs prime movers and how viciously it treats them" and to portray "what happens to the world without them".[8] The core idea for the book came to her during a 1943 telephone conversation with her friend Isabel Paterson, who asserted that Rand owed it to her readers to write fiction about her philosophy. Rand disagreed and replied, "What if I went on strike? What if all the creative minds of the world went on strike? ... That would make a good novel". After the conversation ended, Rand's husband Frank O'Connor, who had overheard, affirmed to Rand, "That would make a good novel."[9] Rand then began Atlas Shrugged to depict the morality of rational self-interest,[10] by exploring the consequences of a strike by intellectuals refusing to supply their inventions, art, business leadership, scientific research, or new ideas to the rest of the world.[11]

Rand began the first draft of the novel on September 2, 1946.[12] She initially thought it would be easy to write and completed quickly, but as she considered the complexity of the philosophical issues she wanted to address, she realized it would take longer.[13] After ending a contract to write screenplays for Hal Wallis and finishing her obligations for the film adaptation of The Fountainhead, Rand worked full-time on the novel that she tentatively titled The Strike. By the summer of 1950, she had written 18 chapters;[14] by September 1951, she had written 21 chapters and was working on the last of the novel's three sections.[15]

As Rand completed new chapters, she read them to a circle of young admirers who had begun gathering at her home to discuss philosophy. This group included Nathaniel Branden, his wife Barbara Branden, Barbara's cousin Leonard Peikoff, and economist Alan Greenspan.[16] Progress on the novel slowed considerably in 1953, when Rand began working on Galt's lengthy radio address. She spent more than two years completing the speech, finishing it on October 13, 1955.[17] The remaining chapters proceeded more quickly, and by November 1956 Rand was ready to submit the almost-completed manuscript to publishers.[18] Atlas Shrugged was Rand's last completed work of fiction. It marked a turning point in her life—the end of her career as a novelist and the beginning of her role as a popular philosopher.[19][20]

Influences
Photo of J. Robert Oppenheimer
Rand used interviews with scientist J. Robert Oppenheimer for the character Robert Stadler.
Rand biographer Anne Heller traces some ideas that would go into Atlas Shrugged back to a never-written novel that Rand outlined when she was a student at Petrograd State University. The futuristic story featured an American heiress luring the most talented men away from a mostly communist Europe. The heiress would have had an assistant called Eddie Willers, the name of Dagny's assistant in Atlas Shrugged.[21]

To depict the industrial setting of Atlas Shrugged, Rand conducted research on the American railroad and steel industries. She toured and inspected a number of industrial facilities, such as the Kaiser Steel plant,[22] visited facilities of the New York Central Railroad,[23][24] and briefly operated a locomotive on the Twentieth Century Limited.[25] Rand also used her previous research for an uncompleted screenplay about the development of the atomic bomb, including her interviews of J. Robert Oppenheimer, which influenced the character Robert Stadler and the novel's depiction of the development of "Project X".[26]

Rand's descriptions of Galt's Gulch were based on the town of Ouray, Colorado, which Rand and her husband visited in 1951 when they were relocating from Los Angeles to New York.[15] Other details of the novel were affected by the experiences and comments of her friends. For example, her portrayal of leftist intellectuals (such as the characters Balph Eubank and Simon Pritchett) was influenced by the college experiences of Nathaniel and Barbara Branden,[27] and Alan Greenspan provided information on the economics of the steel industry.[28]

American libertarian writer Justin Raimondo described similarities between Atlas Shrugged and Garet Garrett's 1922 novel The Driver, which is about an idealized industrialist named Henry Galt, who is a transcontinental railway owner trying to improve the world and fighting against government and socialism.[29] Raimondo believed the earlier novel influenced Rand's writing in ways she failed to acknowledge, although there was no "word-for-word plagiarism" and The Driver was published four years before Rand emigrated to the United States.[30] Journalist Jeff Walker echoed Raimondo's comparisons in his book The Ayn Rand Cult and listed The Driver as one of several unacknowledged precursors to Atlas Shrugged.[31] In contrast, Chris Matthew Sciabarra said he "could not find any evidence to link Rand to Garrett",[32] and considered Raimondo's claims to be "unsupported".[33] Liberty magazine editor R. W. Bradford said Raimondo made an unconvincing comparison based on a coincidence of names and common literary devices.[34]

Publishing history
Photo of Bennet Cerf
Random House CEO Bennett Cerf oversaw the novel's publication in 1957.
Due to the success of Rand's 1943 novel The Fountainhead, she had no trouble attracting a publisher for Atlas Shrugged. This was a contrast to her previous novels, which she had struggled to place. Even before she began writing it, she had been approached by publishers interested in her next novel. However, her contract for The Fountainhead gave the first option to its publisher, Bobbs-Merrill Company. After reviewing a partial manuscript, they asked her to discuss cuts and other changes. She refused, and Bobbs-Merrill rejected the book.[35]

Hiram Hayden, an editor she liked who had left Bobbs-Merrill, asked her to consider his new employer, Random House. In an early discussion about the difficulties of publishing a controversial novel, Random House president Bennett Cerf proposed that Rand should submit the manuscript to multiple publishers simultaneously and ask how they would respond to its ideas, so she could evaluate who might best promote her work. Rand was impressed by the bold suggestion and by her overall conversations with them. After speaking with a few other publishers from about a dozen who were interested, Rand decided multiple submissions were not needed; she offered the manuscript to Random House. Upon reading the portion Rand submitted, Cerf declared it a "great book" and offered Rand a contract. It was the first time Rand had worked with a publisher whose executives seemed enthusiastic about one of her books.[36]

When the completed manuscript exceeded 600,000 words, Cerf asked Rand to make cuts, but backed off when she compared the idea to cutting the Bible.[37] With 1168 pages in the first edition, Atlas Shrugged is Rand's longest published book.[38] Random House published the novel on October 10, 1957. The initial print run was 100,000 copies. The first paperback edition was published by New American Library in July 1959, with an initial run of 150,000.[39] A 35th-anniversary edition was published by E. P. Dutton in 1992, with an introduction by Rand's heir, Leonard Peikoff.[40] The novel has been translated into more than 30 languages.[a]

Title and chapters
Painting of Atlas holding a sphere
The title refers to the mythological Atlas.
The working title of the novel was The Strike, but Rand thought this title would reveal the mystery element of the novel prematurely.[42] She was pleased when her husband suggested Atlas Shrugged, previously the title of a single chapter, for the book.[43] The title is a reference to Atlas, a Titan in Greek mythology, who is described in the novel as "the giant who holds the world on his shoulders".[b] The significance of this reference appears in a conversation in which Francisco d'Anconia asks Rearden what advice he would give Atlas if "the greater [the Titan's] effort, the heavier the world bore down on his shoulders". With Rearden unable to answer, d'Anconia gives his own advice: "To shrug".[45]

The novel is divided into three parts consisting of ten chapters each. Each part is named in honor of one of Aristotle's laws of logic: "Non-Contradiction" after the law of noncontradiction; "Either-Or", which is a reference to the law of excluded middle; and "A Is A" in reference to the law of identity.[46] Each chapter also has a title; Atlas Shrugged is the only one of Rand's novels to use chapter titles.[47]

William Shakespeare[a] (c. 23 April 1564[b] – 23 April 1616)[c] was an English playwright, poet and actor. He is widely regarded as the greatest writer in the English language and the world's pre-eminent dramatist. He is often called England's national poet and the "Bard of Avon" or simply "the Bard". His extant works, including collaborations, consist of some 39 plays, 154 sonnets, three long narrative poems and a few other verses, some of uncertain authorship. His plays have been translated into every major living language and are performed more often than those of any other playwright. Shakespeare remains arguably the most influential writer in the English language, and his works continue to be studied and reinterpreted.

Shakespeare was born and raised in Stratford-upon-Avon, Warwickshire. At the age of 18, he married Anne Hathaway, with whom he had three children: Susanna, and twins Hamnet and Judith. Sometime between 1585 and 1592 he began a successful career in London as an actor, writer, and part-owner ("sharer") of a playing company called the Lord Chamberlain's Men, later known as the King's Men after the ascension of King James VI of Scotland to the English throne. At age 49 (around 1613) he appears to have retired to Stratford, where he died three years later. Few records of Shakespeare's private life survive; this has stimulated considerable speculation about such matters as his physical appearance, his sexuality, his religious beliefs and even certain fringe theories as to whether the works attributed to him were written by others.

Shakespeare produced most of his known works between 1589 and 1613. His early plays were primarily comedies and histories and are regarded as some of the best works produced in these genres. He then wrote mainly tragedies until 1608, among them Hamlet, Othello, King Lear and Macbeth, all considered to be among the finest works in English. In the last phase of his life he wrote tragicomedies (also known as romances) such as The Winter's Tale and The Tempest, and collaborated with other playwrights.

Many of Shakespeare's plays were published in editions of varying quality and accuracy during his lifetime. However, in 1623 John Heminges and Henry Condell, two fellow actors and friends of Shakespeare's, published a more definitive text known as the First Folio, a posthumous collected edition of Shakespeare's dramatic works that includes 36 of his plays. Its preface includes a prescient poem by Ben Jonson, a former rival of Shakespeare, who hailed Shakespeare with the now-famous epithet: "not of an age, but for all time".

Life
Main article: Life of William Shakespeare
Early life

John Shakespeare's house, believed to be Shakespeare's birthplace, in Stratford-upon-Avon
Shakespeare was the son of John Shakespeare, an alderman and a successful glover (glove-maker) originally from Snitterfield in Warwickshire, and Mary Arden, the daughter of an affluent landowning family.[3] He was born in Stratford-upon-Avon, where he was baptised on 26 April 1564. His date of birth is unknown but is traditionally observed on 23 April, Saint George's Day.[1] This date, which can be traced to William Oldys and George Steevens, has proved appealing to biographers because Shakespeare died on the same date in 1616.[4][5] He was the third of eight children, and the eldest surviving son.[6]

Although no attendance records for the period survive, most biographers agree that Shakespeare was probably educated at the King's New School in Stratford,[7][8][9] a free school chartered in 1553,[10] about a quarter-mile (400 m) from his home. Grammar schools varied in quality during the Elizabethan era, but grammar school curricula were largely similar: the basic Latin text was standardised by royal decree,[11][12] and the school would have provided an intensive education in grammar based upon Latin classical authors.[13]

At the age of 18, Shakespeare married 26-year-old Anne Hathaway. The consistory court of the Diocese of Worcester issued a marriage licence on 27 November 1582. The next day, two of Hathaway's neighbours posted bonds guaranteeing that no lawful claims impeded the marriage.[14] The ceremony may have been arranged in some haste; the Worcester chancellor allowed the marriage banns to be read once instead of the usual three times.[15][16] Six months after the marriage, Anne gave birth to a daughter, Susanna, baptised 26 May 1583.[17] Twins, son Hamnet and daughter Judith, followed almost two years later and were baptised 2 February 1585.[18] Hamnet died of unknown causes at the age of 11 and was buried 11 August 1596.[19]


Shakespeare's coat of arms, from the 1602 book The book of coates and creasts. Promptuarium armorum. It features spears as a pun on the family name.[d]
After the birth of the twins, Shakespeare left few historical traces until he is mentioned as part of the London theatre scene in 1592. The exception is the appearance of his name in the "complaints bill" of a law case before the Queen's Bench court at Westminster dated Michaelmas Term 1588 and 9 October 1589.[20] Scholars refer to the years between 1585 and 1592 as Shakespeare's "lost years".[21] Biographers attempting to account for this period have reported many apocryphal stories. Nicholas Rowe, Shakespeare's first biographer, recounted a Stratford legend that Shakespeare fled the town for London to escape prosecution for deer poaching in the estate of local squire Thomas Lucy. Shakespeare is also supposed to have taken his revenge on Lucy by writing a scurrilous ballad about him.[22][23] Another 18th-century story has Shakespeare starting his theatrical career minding the horses of theatre patrons in London.[24] John Aubrey reported that Shakespeare had been a country schoolmaster.[25] Some 20th-century scholars suggested that Shakespeare may have been employed as a schoolmaster by Alexander Hoghton of Lancashire, a Catholic landowner who named a certain "William Shakeshafte" in his will.[26][27] Little evidence substantiates such stories other than hearsay collected after his death, and Shakeshafte was a common name in the Lancashire area.[28][29]

London and theatrical career
It is not known definitively when Shakespeare began writing, but contemporary allusions and records of performances show that several of his plays were on the London stage by 1592.[30] By then, he was sufficiently known in London to be attacked in print by the playwright Robert Greene in his Groats-Worth of Wit from that year:

... there is an upstart Crow, beautified with our feathers, that with his Tiger's heart wrapped in a Player's hide, supposes he is as well able to bombast out a blank verse as the best of you: and being an absolute Johannes factotum, is in his own conceit the only Shake-scene in a country.[31]

Scholars differ on the exact meaning of Greene's words,[31][32] but most agree that Greene was accusing Shakespeare of reaching above his rank in trying to match such university-educated writers as Christopher Marlowe, Thomas Nashe and Greene himself (the so-called "University Wits").[33] The italicised phrase parodying the line "Oh, tiger's heart wrapped in a woman's hide" from Shakespeare's Henry VI, Part 3, along with the pun "Shake-scene", clearly identify Shakespeare as Greene's target. As used here, Johannes Factotum ("Jack of all trades") refers to a second-rate tinkerer with the work of others, rather than the more common "universal genius".[31][34]

Greene's attack is the earliest surviving mention of Shakespeare's work in the theatre. Biographers suggest that his career may have begun any time from the mid-1580s to just before Greene's remarks.[35][36][37] After 1594 Shakespeare's plays were performed at The Theatre, in Shoreditch, only by the Lord Chamberlain's Men, a company owned by a group of players, including Shakespeare, that soon became the leading playing company in London.[38] After the death of Queen Elizabeth in 1603, the company was awarded a royal patent by the new King James I, and changed its name to the King's Men.[39]

All the world's a stage,
and all the men and women merely players:
they have their exits and their entrances;
and one man in his time plays many parts ...

—As You Like It, Act II, Scene 7, 139–142[40]
In 1599 a partnership of members of the company built their own theatre on the south bank of the River Thames, which they named the Globe. In 1608 the partnership also took over the Blackfriars indoor theatre. Extant records of Shakespeare's property purchases and investments indicate that his association with the company made him a wealthy man,[41] and in 1597 he bought the second-largest house in Stratford, New Place, and in 1605 invested in a share of the parish tithes in Stratford.[42]

Some of Shakespeare's plays were published in quarto editions, beginning in 1594, and by 1598 his name had become a selling point and began to appear on the title pages.[43][44][45] Shakespeare continued to act in his own and other plays after his success as a playwright. The 1616 edition of Ben Jonson's Works names him on the cast lists for Every Man in His Humour (1598) and Sejanus His Fall (1603).[46] The absence of his name from the 1605 cast list for Jonson's Volpone is taken by some scholars as a sign that his acting career was nearing its end.[35] The First Folio of 1623, however, lists Shakespeare as one of "the Principal Actors in all these Plays", some of which were first staged after Volpone, although one cannot know for certain which roles he played.[47] In 1610, John Davies of Hereford wrote that "good Will" played "kingly" roles.[48] In 1709 Rowe passed down a tradition that Shakespeare played the ghost of Hamlet's father.[49] Later traditions maintain that he also played Adam in As You Like It, and the Chorus in Henry V,[50][51] though scholars doubt the sources of that information.[52]

Throughout his career, Shakespeare divided his time between London and Stratford. In 1596, the year before he bought New Place as his family home in Stratford, Shakespeare was living in the parish of St Helen's, Bishopsgate, north of the River Thames.[53][54] He moved across the river to Southwark by 1599, the same year his company constructed the Globe Theatre there.[53][55] By 1604 he had moved north of the river again, to an area north of St Paul's Cathedral with many fine houses. There he rented rooms from a French Huguenot named Christopher Mountjoy, a maker of women's wigs and other headgear.[56][57]

Later years and death

Shakespeare's funerary monument in Stratford-upon-Avon
Nicholas Rowe was the first biographer to record the tradition, repeated by Samuel Johnson, that Shakespeare retired to Stratford "some years before his death".[58][59] He was still working as an actor in London in 1608; in an answer to the sharers' petition in 1635, Cuthbert Burbage stated that after purchasing the lease of the Blackfriars Theatre in 1608 from Henry Evans, the King's Men "placed men players" there, "which were Heminges, Condell, Shakespeare, etc.".[60] However, it is perhaps relevant that the bubonic plague raged in London throughout 1609.[61][62] The London public playhouses were repeatedly closed during extended outbreaks of the plague (a total of over 60 months closure between May 1603 and February 1610),[63] which meant there was often no acting work. Retirement from all work was uncommon at that time.[64] Shakespeare continued to visit London during the years 1611–1614.[58] In 1612 he was called as a witness in Bellott v Mountjoy, a court case concerning the marriage settlement of Mountjoy's daughter, Mary.[65][66] In March 1613 he bought a gatehouse in the former Blackfriars priory;[67] and from November 1614 he was in London for several weeks with his son-in-law, John Hall.[68] After 1610 Shakespeare wrote fewer plays, and none are attributed to him after 1613.[69] His last three plays were collaborations, probably with John Fletcher,[70] who succeeded him as the house playwright of the King's Men. He retired in 1613, before the Globe Theatre burned down during the performance of Henry VIII on 29 June.[69]

Shakespeare died on 23 April 1616, at the age of 52.[e] He died within a month of signing his will, a document which he begins by describing himself as being in "perfect health". No extant contemporary source explains how or why he died. Half a century later, John Ward, the vicar of Stratford, wrote in his notebook: "Shakespeare, Drayton, and Ben Jonson had a merry meeting and, it seems, drank too hard, for Shakespeare died of a fever there contracted",[72][73] not an impossible scenario since Shakespeare knew Jonson and Michael Drayton. Of the tributes from fellow authors, one refers to his relatively sudden death: "We wondered, Shakespeare, that thou went'st so soon / From the world's stage to the grave's tiring room."[74][f]


Holy Trinity Church, Stratford-upon-Avon, where Shakespeare was baptised and is buried
He was survived by his wife and two daughters. Susanna had married a physician, John Hall, in 1607,[75] and Judith had married Thomas Quiney, a vintner, two months before Shakespeare's death.[76] Shakespeare signed his last will and testament on 25 March 1616; the following day, Thomas Quiney, his new son-in-law, was found guilty of fathering an illegitimate son by Margaret Wheeler, both of whom had died during childbirth. Thomas was ordered by the church court to do public penance, which would have caused much shame and embarrassment for the Shakespeare family.[76]

Shakespeare bequeathed the bulk of his large estate to his elder daughter Susanna[77] under stipulations that she pass it down intact to "the first son of her body".[78] The Quineys had three children, all of whom died without marrying.[79][80] The Halls had one child, Elizabeth, who married twice but died without children in 1670, ending Shakespeare's direct line.[81][82] Shakespeare's will scarcely mentions his wife, Anne, who was probably entitled to one-third of his estate automatically.[g] He did make a point, however, of leaving her "my second best bed", a bequest that has led to much speculation.[84][85][86] Some scholars see the bequest as an insult to Anne, whereas others believe that the second-best bed would have been the matrimonial bed and therefore rich in significance.[87]


Shakespeare's grave, next to those of Anne Shakespeare, his wife, and Thomas Nash, the husband of his granddaughter
Shakespeare was buried in the chancel of the Holy Trinity Church two days after his death.[88][89] The epitaph carved into the stone slab covering his grave includes a curse against moving his bones, which was carefully avoided during restoration of the church in 2008:[90]

Good frend for Iesvs sake forbeare,
To digg the dvst encloased heare.
Bleste be yͤ man yͭ spares thes stones,
And cvrst be he yͭ moves my bones.[91][h]

Translation:
Good friend, for Jesus' sake forbear,
To dig the dust enclosed here.
Blessed be the man that spares these stones,
And cursed be he that moves my bones.

Some time before 1623 a funerary monument was erected in his memory on the north wall, with a half-effigy of him in the act of writing. Its plaque compares him to Nestor, Socrates, and Virgil.[92] In 1623, in conjunction with the publication of the First Folio, the Droeshout engraving was published.[93] Shakespeare has been commemorated in many statues and memorials around the world, including funeral monuments in Southwark Cathedral and Poets' Corner in Westminster Abbey.[


    A Doll's House (Danish and Bokmål: Et dukkehjem; also translated as A Doll House) is a three-act play written by Norwegian playwright Henrik Ibsen. It premiered at the Royal Danish Theatre in Copenhagen, Denmark, on 21 December 1879, having been published earlier that month.[1] The play is set in a Norwegian town c. 1879.

The play concerns the fate of a married woman, who, at the time in Norway, lacked reasonable opportunities for self-fulfillment in a male-dominated world. Despite the fact that Ibsen denied it was his intent to write a feminist play, it was a great sensation at the time[2] and caused a "storm of outraged controversy" that went beyond the theater to the world of newspapers and society.[3]

In 2006, the centennial of Ibsen's death, A Doll's House held the distinction of being the world's most-performed play that year.[4] UNESCO has inscribed Ibsen's autographed manuscripts of A Doll's House on the Memory of the World Register in 2001, in recognition of their historical value.[5]

The title of the play is most commonly translated as A Doll's House, though some scholars use A Doll House. John Simon says that A Doll's House is "the British term for what [Americans] call a 'dollhouse'".[6] Egil Törnqvist says of the alternative title: "Rather than being superior to the traditional rendering, it simply sounds more idiomatic to Americans."[7]

List of characters
Reproduction of a sepiatone photograph of Adeleide Johannessen in corset and floor-length dress. She leans forward at the waist, her right arm stretched upward and outward, with a tambourine grasped in her hand. Her tense eyes look to her left. Written on one side of the card are the words Langaard's cigaretter! (which translates from Danish and Swedish as Langaard's cigarettes).
Cigarette card (c. 1880 – c. 1882) depicting Adeleide Johannessen as Nora Helmer.
Nora Helmer
Wife of Torvald; she is a mother of three. She is living out the ideal of the 19th-century wife. She feels stifled in her marriage.
Torvald Helmer
Nora's husband; a recently promoted bank manager. He professes to be enamored of his wife.
Dr. Rank / Peter Rank
A family friend. He is terminally ill. The dialogue implies that his "tuberculosis of the spine" originates from a venereal disease contracted by his father.
(The character's name in Michael Meyer's 1965 translation is Peter Rank.)

Kristine Linde / Christine Linde
Nora's old school friend, widowed, is seeking employment. She was in a relationship with Krogstad prior to the play's setting.
(Some English translations localize the spelling of the name Kristine to Christine.)

Nils Krogstad
An employee at Torvald's bank. He is a single father pushed to desperation. We are led to believe that he is a scoundrel, but events reveal him to be a long-lost lover of Kristine.
The Children
Nora and Torvald's three children: Ivar, Bobby, and Emmy.
Anne Marie
Nanny to the Helmer children, and former nanny to their mother Nora. She says that she gave up her own daughter to "strangers" when she became the only mother that Nora knew.[8]
Helene
The Helmers' maid.
The Porter
Delivers a Christmas tree to the Helmer household at the beginning of the play.
Casting in some notable productions
Character	Performer
1975 Broadway revival	1997 Broadway revival	2023 Broadway revival
Nora Helmer	Liv Ullmann	Janet McTeer	Jessica Chastain
Torvald Helmer	Sam Waterston	Owen Teale	Arian Moayed
Dr. Rank	Michael Granger	John Carlisle	Michael Patrick Thornton
Kristine Linde	Barbara Colby	Jan Maxwell	Jesmille Darbouze
Nils Krogstad	Barton Heyman	Peter Gowen	Okieriete Onaodowan
Anne Marie	Helen Stenborg	Robin Howard	
Tasha Lawrence

Synopsis
Act One

Mrs. Linde and Nora converse (from a 2012 production)
At Christmas, Nora Helmer returns from shopping carrying many packages and presents. Her husband, Torvald, is working in his study. He playfully rebukes Nora for spending so much money on Christmas gifts, calling her his "little squirrel". He teases her about how, the previous year, she had spent weeks making gifts and ornaments by hand because money was scarce. This year, Torvald is due a promotion at the bank where he works, so Nora feels that they can let themselves go a little. The maid announces two visitors: Mrs. Kristine Linde, an old friend of Nora's, who has come seeking employment; and Dr. Rank, a close friend of the family, who is let into the study. Kristine explains that she has fallen into poverty following her husband's death. Nora is empathetic, recalling her own past troubles: Torvald became sick, and they had to travel to Italy for a rest cure. Nora promises to talk to Torvald about finding her a job. Kristine gently tells Nora that she is like a child. Nora is offended, so she tells her that she got money from "some admirer" so they could travel to Italy to improve Torvald's health. She told Torvald that her father gave her the money, but, in fact, she illegally borrowed it without his knowledge (women were forbidden from conducting financial activities such as signing checks without a man's endorsement). Since then, she has been secretly working and saving up to pay off the loan.

Krogstad, a low level employee at Torvald's bank, arrives and goes into the study. Nora is clearly uneasy when she sees him. Dr. Rank leaves the study and mentions that he feels wretched, though like everyone he wants to go on living. In contrast to his physical illness, he says that the man in the study, Krogstad, is "morally diseased".

After the meeting with Krogstad, Torvald comes out of the study. Nora asks him if he can give Kristine a position at the bank and Torvald is very positive, saying that this is a fortunate moment, as a position has just become available. Torvald, Kristine, and Dr. Rank leave the house, leaving Nora alone. The nanny returns with the children, and Nora plays with them for a while until Krogstad creeps through the ajar door into the living room and surprises her. Krogstad tells Nora that Torvald intends to fire him from the bank and asks her to intercede with Torvald to allow him to keep his job. She refuses, and Krogstad blackmails her about the loan she took out for the trip to Italy; he knows that she obtained this loan by forging her father's signature after his death. Krogstad leaves, and, when Torvald returns, Nora tries to convince him not to fire Krogstad. Torvald refuses to hear her pleas, explaining that Krogstad is a liar and a hypocrite and that, years before, he had committed a crime: he forged other people's signatures. Torvald feels physically ill in the presence of a man "poisoning his own children with lies and dissimulation".

Act Two
Kristine arrives to help Nora repair a dress for a costume function that she and Torvald plan to attend the next day. Torvald returns from the bank, and Nora pleads with him to reinstate Krogstad, claiming she is worried Krogstad will publish libelous articles about Torvald and ruin his career. Torvald dismisses her fears and explains that, although Krogstad is a good worker and seems to have turned his life around, he must be fired because he is too familiar around Torvald in front of other bank personnel. Torvald then retires to his study to work.

Dr. Rank then arrives. Nora asks him for a favor, but Rank responds by revealing that he has entered the terminal stage of his disease and that he has always been secretly in love with her. Nora tries to deny the first revelation and make light of it but is more disturbed by his declaration of love. She then clumsily attempts to tell him that she is not in love with him but loves him dearly as a friend.

Having been fired by Torvald, Krogstad arrives at the house. Nora convinces Dr. Rank to go into Torvald's study so he will not see Krogstad. When Krogstad confronts Nora, he declares that he no longer cares about the remaining balance of Nora's loan but that he will instead preserve the associated bond to blackmail Torvald into not only keeping him employed but also promoting him. Nora explains that she has done her best to persuade her husband, but he refuses to change his mind. Krogstad informs Nora that he has written a letter detailing her crime (forging her father's signature of surety on the bond) and put it in Torvald's mailbox, which is locked.

Nora tells Kristine of her difficult situation, gives her Krogstad's card with his address, and asks her to try to convince him to relent.

Torvald enters and tries to retrieve his mail, but Nora distracts him by begging him to help her with the dance she has been rehearsing for the costume party, feigning anxiety about performing. She dances so badly and acts so childishly that Torvald agrees to spend the whole evening coaching her. When the others go to dinner, Nora stays behind for a few minutes and contemplates killing herself.

Act Three

Torvald addresses Nora (from a 2012 production)
Kristine tells Krogstad that she only married her husband because she had no other means to support her sick mother and young siblings and that she has returned to offer him her love again. She believes that he would not have stooped to unethical behavior if he had not been devastated by her abandonment and in dire financial straits. Krogstad changes his mind and offers to take back his letter to Torvald. Kristine, however, decides that Torvald should know the truth for the sake of his and Nora's marriage.

After Torvald literally drags Nora home from the party, Rank follows them. They chat for a while, with Dr. Rank conveying obliquely to Nora that this is a final goodbye, as he has determined that his death is near. Dr. Rank leaves, and Torvald retrieves his letters. As he reads them, Nora prepares to run away for good, but Torvald confronts her with Krogstad's letter. Enraged, he declares that she is now completely in Krogstad's power; she must yield to Krogstad's demands and keep quiet about the whole affair. He berates Nora, calling her a dishonest and immoral woman and telling her that she is unfit to raise their children. He says that from now on their marriage will be only a matter of appearances.

A maid enters, delivering a letter from Krogstad to Nora, which Torvald demands to read himself. Torvald then exults that he is saved, as Krogstad has returned the incriminating bond, which Torvald immediately burns along with Krogstad's letters. He takes back his harsh words to his wife and tells her that he forgives her. Nora realizes that her husband is not the strong and gallant man she thought he was and that he truly loves himself more than he does Nora.

Torvald explains that, when a man has forgiven his wife, it makes him love her all the more since it reminds him that she is totally dependent on him, like a child. He preserves his peace of mind by thinking of the incident as a mere mistake that she made owing to her foolishness, one of her most endearing feminine traits.

Torvald, this is a settling of accounts. // In all these eight years [...], we have never exchanged a word on any serious subject.

Nora, in Ibsen's A Doll's House (1879)
Nora tells Torvald that she is leaving him and, in a confrontational scene, expresses her sense of betrayal and disillusionment. She says he has never loved her and they have become strangers to each other. She feels betrayed by his response to the scandal involving Krogstad, and she says she must get away to understand herself. She says that she has been treated like a doll to play with for her whole life, first by her father and then by him. Torvald insists that she fulfill her duty as a wife and mother, but Nora says that she has duties to herself that are just as important and that she cannot be a good mother or wife without learning to be more than a plaything. She reveals that she had expected that he would want to sacrifice his reputation for hers and that she had planned to kill herself to prevent him from doing so. She now realizes that Torvald is not at all the kind of person she had believed him to be and that their marriage has been based on mutual fantasies and misunderstandings.

Nora leaves her keys and wedding ring; Torvald breaks down and begins to cry, baffled by what has happened. After Nora leaves the room, Torvald, for one second, still has a sense of hope and exclaims to himself "The most wonderful thing of all—?", just before the door downstairs is heard closing.

Alternative ending
Ibsen's German agent felt that the original ending would not play well in German theaters. In addition, copyright laws of the time would not preserve Ibsen's original work. Therefore, for it to be considered acceptable, and prevent the translator from altering his work, Ibsen was forced to write an alternative ending for the German premiere. In this ending, Nora is led to her children after having argued with Torvald. Seeing them, she collapses, and as the curtain is brought down, it is implied that she stays. Ibsen later called the ending a disgrace to the original play and referred to it as a "barbaric outrage".[9] Virtually all productions today use the original ending, as do nearly all the film versions of the play.

Composition and publication
Real-life inspiration
A Doll's House was based on the life of Laura Kieler (maiden name Laura Smith Petersen), a good friend of Ibsen. Much that happened between Nora and Torvald happened to Laura and her husband, Victor. Similar to the events in the play, Laura signed an illegal loan to save her husband's life—in this case, to find a cure for his tuberculosis.[10] She wrote to Ibsen, asking for his recommendation of her work to his publisher, thinking that the sales of her book would repay her debt. At his refusal, she forged a check for the money. At this point, she was found out. In real life, when Victor discovered Laura's secret loan, he divorced her and had her committed to an asylum. Two years later, she returned to her husband and children at his urging, and she went on to become a well-known Danish author, living to the age of 83.

Ibsen wrote A Doll's House when Laura Kieler had been committed to the asylum. The fate of this friend of the family shook him deeply, perhaps also because Laura had asked him to intervene at a crucial point in the scandal, which he did not feel able or willing to do. Instead, he turned this life situation into an aesthetically shaped, successful drama. In the play, Nora leaves Torvald with head held high, though facing an uncertain future given the limitations single women faced in the society of the time.

Kieler eventually rebounded from the shame of the scandal and had her own successful writing career while remaining discontented with sole recognition as "Ibsen's Nora" years afterward.[11][12]

Composition
Ibsen started thinking about the play around May 1878, although he did not begin its first draft until a year later, having reflected on the themes and characters in the intervening period (he visualized its protagonist, Nora, for instance, as having approached him one day wearing "a blue woolen dress").[13] He outlined his conception of the play as a "modern tragedy" in a note written in Rome on 19 October 1878.[14] "A woman cannot be herself in modern society", he argues, since it is "an exclusively male society, with laws made by men and with prosecutors and judges who assess feminine conduct from a masculine standpoint!"[15]

Publication
Ibsen sent a fair copy of the completed play to his publisher on 15 September 1879.[16] It was first published in Copenhagen on 4 December 1879, in an edition of 8,000 copies that sold out within a month; a second edition of 3,000 copies followed on 4 January 1880, and a third edition of 2,500 was issued on 8 March.[17]



Siddhartha: An Indian novel (German: Siddhartha: Eine Indische Dichtung; German: [ziˈdaʁta] ⓘ) is a 1922 novel by Hermann Hesse that deals with the spiritual journey of self-discovery of a man named Siddhartha during the time of the Gautama Buddha. The book, Hesse's ninth novel, was written in German, in a simple, lyrical style. It was published in the United States in 1951 by New Directions Publishing and became influential during the 1960s. Hesse dedicated the first part of it to the French writer Romain Rolland and the second part to Wilhelm Gundert, his cousin.

The word Siddhartha is made up of two words in the Sanskrit language: siddha (achieved) + artha (what was searched for), which together means "he who has found meaning (of existence)" or "he who has attained his goals".[1] In fact, the Buddha's own name, before his renunciation, was Siddhartha Gautama, prince of Kapilavastu. In this book, the Buddha is referred to as "Gotama".

Plot
The story takes place in ancient India, where Siddhartha, the handsome son of a Brahmin, decides to leave his home in the hope of gaining spiritual illumination by becoming an ascetic Samana. Joined by his best friend Govinda, Siddhartha fasts, becomes homeless, renounces all personal possessions, and intensely meditates. Eventually the pair seek out and personally speak with the enlightened Gautama, but although Govinda hastily joins the Buddha's order, Siddhartha does not. For him, the Buddhist philosophy, though supremely wise, must be individually realized independently of instruction by a teacher. He thus resolves to carry on his quest alone.

Siddhartha crosses a river and the ferryman, whom Siddhartha is unable to pay, predicts that Siddhartha will return later to compensate him in some way. Venturing onward toward city life, Siddhartha encounters the courtesan Kamala, the most beautiful woman he has seen. She notes Siddhartha's handsome appearance and fast wit, but warns him that he must become wealthy to win her affections so that she may teach him the art of love. Although Siddhartha despised materialistic pursuits as a Samana, he agrees now to Kamala's suggestion. She directs him to the employ of Kamaswami, a local businessman, and insists that he have Kamaswami treat him as an equal rather than an underling. Siddhartha easily succeeds, providing a voice of patience and tranquility against Kamaswami's fits of passion - a skill which Siddhartha learned from his days as an ascetic. Thus Siddhartha becomes a rich man and Kamala's lover, but in his middle years he realises that he had fallen into vicious cycle of gambling and indulgence that lacks spiritual fulfilment. Leaving the bustle of the city, Siddhartha returns to the river, disillusioned and contemplating suicide. Falling into a meditative sleep, he is saved only by an internal experience of the holy word, Om.

The next morning, Siddhartha briefly reconnects with Govinda, who is passing through the area as a wandering Buddhist monk. Siddhartha decides to live the rest of his life in the presence of the spiritually inspirational river, companioning Vasudeva, the elderly ferryman, with whom he begins a humbler way of life. Although Vasudeva is a simple man, he has spiritual insight and relates that the river has many voices and significant messages to divulge to any who might listen.

Some years later, Kamala, now a Buddhist convert, is traveling to see the Buddha on his deathbed, accompanied by her reluctant young son, when she is bitten by a venomous snake near the river bank. Siddhartha recognizes her and she informs him that the boy is his own son. After Kamala's death, Siddhartha attempts to console and raise the furiously resistant boy, until one day the child flees altogether. Although Siddhartha is desperate to follow the runaway, Vasudeva urges him to let the boy find his own path, just as Siddhartha did himself in his youth. Listening to the river with Vasudeva, Siddhartha realizes that time is an illusion and that all of his feelings and experiences, even those of suffering, are part of a great and ultimately jubilant fellowship of all things connected in the cyclical unity of nature. After Siddhartha's moment of illumination, Vasudeva claims that his work is done and he must depart into the woods, leaving Siddhartha peacefully fulfilled and alone once more.

Toward the end of his life, Govinda hears about an enlightened ferryman and travels to Siddhartha, not initially recognizing him as his old childhood companion. Govinda asks the now-elderly Siddhartha to relate his wisdom and Siddhartha replies that for every true statement there is an opposite one that is also true; that language and the confines of time lead people to adhere to one fixed belief that does not account for the fullness of the truth. Because nature works in a self-sustaining cycle, every entity carries in it the potential for its opposite and so the world must always be considered complete. Siddhartha simply urges people to identify and love the world in its completeness. He then requests the puzzled Govinda to kiss his forehead; when he does so, Govinda experiences the same visions of timelessness that Siddhartha himself saw with Vasudeva by the river. Govinda then bows to his wise and radiantly smiling friend.

Major themes
In Hesse's novel, experience, the totality of conscious events of a human life, is shown as the best way to approach understanding of reality and attain enlightenment⁠—⁠Hesse's crafting of Siddhartha's journey shows that understanding is attained not through intellectual methods, nor through immersing oneself in the carnal pleasures of the world and the accompanying pain of samsara; rather, it is the completeness of these experiences that allows Siddhartha to attain understanding.

Thus, individual events are meaningless when considered by themselves—⁠Siddhartha's stay with the Samanas and his immersion in the worlds of love and business do not ipso facto lead to nirvana, yet they cannot be considered distractions, for every action and event gives Siddhartha experience, which in turn leads to understanding.

A major preoccupation of Hesse in writing Siddhartha was to cure his "sickness with life" (Lebenskrankheit) by immersing himself in Indian philosophy such as that expounded in the Upanishads and the Bhagavad Gita.[2] The reason the second half of the book took so long to write was that Hesse "had not experienced that transcendental state of unity to which Siddhartha aspires. In an attempt to do so, Hesse lived as a virtual semi-recluse and became totally immersed in the sacred teachings of both Hindu and Buddhist scriptures. His intention was to attain to that 'completeness' which, in the novel, is the Buddha's badge of distinction."[3] The novel is structured on three of the traditional stages of life for Hindu males (student (brahmacharin), householder (grihastha) and recluse/renunciate (vanaprastha) as well as the Buddha's Four Noble Truths (Part One) and Eightfold Path (Part Two) which form twelve chapters, the number in the novel.[4]

Ralph Freedman mentions how Hesse commented in a letter "[my] Siddhartha does not, in the end, learn true wisdom from any teacher, but from a river that roars in a funny way and from a kindly old fool who always smiles and is secretly a saint."[5] In a lecture about Siddhartha, Hesse claimed "Buddha's way to salvation has often been criticized and doubted, because it is thought to be wholly grounded in cognition. True, but it's not just intellectual cognition, not just learning and knowing, but spiritual experience that can be earned only through strict discipline in a selfless life".[6] Freedman also points out how Siddhartha described Hesse's interior dialectic: "All of the contrasting poles of his life were sharply etched: the restless departures and the search for stillness at home; the diversity of experience and the harmony of a unifying spirit; the security of religious dogma and the anxiety of freedom."[7] Eberhard Ostermann has shown how Hesse, while mixing the religious genre of the legend with that of the modern novel, seeks to reconcile with the double-edged effects of modernization such as individualization, pluralism or self-disciplining.[8] The character Siddhartha honors the character Gotama (Gautama Buddha) by not following him in person, but by following Gotama's example.

Cultural reinterpretations
Zachariah, an adaptation loosely based on two Hesse novels including Siddhartha, was released as a musical Western in 1971.[9] In the following year, a film version of the novel was released as Siddhartha, starring Shashi Kapoor and directed by Conrad Rooks.

Musical compositions based on the novel have included Claude Vivier's symphonic poem, Siddhartha (1976),[10][11] and Pete Townshend's song "The Ferryman", written for an amateur dramatisation in June 1976.[12][13] Townshend's wife Rachel Fuller composed a 'literary and musical reinvention' of the novel by 2024, titled "The Seeker", starring him as the ferryman.[14] Earlier, in 2015, Joel Puckett composed a piece for wind ensemble titled "That Secret from the River", based on the Ferryman's words from the book: "Have you also learned that secret from the river; that there is no such thing as time? That the river is everywhere at the same time, at the source and at the mouth, at the waterfall, at the ferry, at the current, in the ocean and in the mountains, everywhere and that the present only exists for it, not the shadow of the past nor the shadow of the future."[15]

There was also an Indian-themed photographic essay by Fred Mayer published in 2011 under the title "Homage to Hermann Hesse and His Siddhartha"[16


Ranchi Municipal Corporation is responsible for the civic administration of the city of Ranchi. It was established in 1979 in erstwhile Bihar and is presently the biggest municipality in Jharkhand. According to Census of India 2011, the area under the municipal body is 175.12 sq km and the population is 1,073,427.[2] There are 55 wards in the city, which are further grouped into zones.[3][4][5] The last election to the municipal corporation took place in 2018, with candidates from Bharatiya Janata Party winning the seats of Mayor and Deputy Mayor.[6]

Municipal history
Ranchi Municipal Corporation was established on 15th September 1979 by merging erstwhile Ranchi Municipality, Doranda Municipality and Ranchi Doranda Joint Water Board. It was the second municipal corporation to be set up in erstwhile Bihar (after Patna Municipal Corporation) and the first in the state of Jharkhand. Ranchi city got its first municipality in 1869, which covered an area of 7.02 sq km.[7] This was extended in 1951 and 1971 to 20.3 sq km and 89.98 sq km respectively. The present Ranchi Municipal Corporation was established in 1979 and the first elections to the corporation were held in 1986.[8] The municipal corporation covers an area of 175.12 sq km and is divided into 55 administrative wards, each represented by an elected councillor.[4]

At the time of the creation of the state of Jharkhand in 2000, the Bihar Municipal Act, 1922 was adapted as the Jharkhand Municipal Act, 2000 to govern all urban local bodies in the new state. Ranchi Municipal Corporation was governed by its own municipal act, the Ranchi Municipal Corporation Act, 2001. With the enactment of the Jharkhand Municipal Act 2011, these two ceased to be in effect and the 2011 became the only governing legislature for all city governments in Jharkhand.[9]

Administrative set up
Municipal commissioner
The commissioner is appointed by the state government. They are either an officer from the Indian Administrative Service or the Jharkhand Administrative Service. According to Section 55 of the Jharkhand Municipal Act, 2011, the appointment can be made on regular basis or contractual basis and no time period is mentioned for the position.[10] The present municipal commissioner at Ranchi Municipal Corporation is Mr. Sandeep Singh, a cadre from the Indian Administrative Services.[11]

Some of the powers and functions of the Commissioner are:[10]

The Standing Committee may delegate some of its power or functions to the commissioner and in turn, the commissioner may delegate their power to any corporation officer or employee (Section 33)
The Commissioner is supposed to nominate an official to be the secretary of the Ward Committee (Section 34) and Conveyor of Zonal Committees (Section 49)
The Commissioner can attend any meeting of the corporation or may authorize any corporation officer to attend (Section 82)
The Commissioner shall prepare a budget estimate every year (Section 108 (1))
The Commissioner has to approve every public advertisement in the city (Section 171)
The Commissioner can serve a notice to an occupier of a land or building for the purpose of the recovery of property tax (Section 185 (1))
Municipal departments
There are 24 administrative departments at Ranchi Municipal Corporation.[12] Some important departments at the Corporation are: Accounts, Birth & Death Registration, Bhoosampada, Engineering, Health, Revenue, Town Planning, and Water supply.

Key administrative posts
Apart from the municipal commissioner, the Jharkhand Municipal Act, 2011 mentions the following key administrative posts to be created at the municipal corporation:[10]

Chief Finance Officer / Chief Accounts Officer
Municipal Internal Auditor
Chief Municipal Engineer
Chief Town Planner and Municipal Architect
Chief Municipal Health Officer
Chief Environmental Engineer (for solid waste management)
Chief Information and Technology Officer
Municipal Law Officer
Municipal Secretary
Additional and Deputy/Assistant Municipal Commissioners
Elected Representatives
Mayor and Deputy Mayor
Under section 26 of the Jharkhand Municipal Act, 2011, the positions of mayor and deputy mayor are elected directly i.e. the position holders are voted in by the people themselves.[10] The last municipal election was in 2018 with a voter turnout of 49.3%, up from 38% in the election in 2013.[6][13] Dr. Asha Lakra and Sanjeev Vijayawargia – both from the Bharatiya Janta Party – won the mayoral and deputy mayoral seats.[14] The 2018 municipal election was the first time the mayoral and deputy mayoral candidates in Jharkhand could use political party symbols, while the candidates for councilor seats continued to use symbols allotted to them by the Jharkhand State Election Commission.[14][15] The next municipal election will take place in 2023.[8]

Councillors
Each ward of the corporation is represented by an elected councillor whose term is coterminous with that of the corporation i.e. of five years.[10] For the purpose of election, the councillors can not use party symbols and are instead provided with election symbols by the Jharkhand State Election Commission.[15] According to the Jharkhand Municipal Act, 2011, around 50% of the seats are to be reserved for the following categories:

Scheduled Castes,
Scheduled Tribes,
Backward Classes,
Women
The seats reserved for persons belonging to the Scheduled Castes and Scheduled Tribes categories will be proportionate to their population share in the municipality. The remaining number of seats within the 50% stipulated reserved seats will be reserved for candidates from Other Backward Classes. Further, up to 50% of the reserved seats and up to50% of the unreserved seats will be reserved for women. The seats so reserved will be allotted on a rotational basis i.e. the councillor seat of any particular ward need not be reserved for the same category or be reserved at all in subsequent elections.[10] Presently, there are 55 councillors in Ranchi representing 55 wards.[3][4]

Committees
Jharkhand Municipal Act, 2011 mandates the establishment of standing committees at every municipality.[10] The mayor is the presiding officer and the term of the members is of five years. Ranchi Municipal Corporation has set up a Standing Committee.[16] The Standing Committee is composed of ten councillors, the mayor, the deputy mayor, the municipal commissioner and RMC officials.[17] The Act also mandates the setting up of Zonal Committees, each representing groups of wards in the city. These too have been formed in Ranchi, with each Zonal Committee having 5–6 wards under it.[5] The function of the Zonal committees is to look after the issues felt at each zone, report them to the corporation, and prepare solutions for the same. The Jharkhand Municipal Act, 2011 mandates the establishment of ward committees for each ward in the municipality.[10] Though ward committees have been formed in Ranchi, they are not active.[18]

Wards
Zone[19]	Ward Number	Ward Name	Areas Covered	Assembly Constituency	Councillor[20]	Political Group
West	1				Smt Asha Lakra	
West	2				Shri Sanjeev Vijaywargiya	
West	3				Shri Nakul Tirkey	
West	4				Shri Gundra Oraon	
West	5				Smt. Basanti Lakra	
West	6				Smt. Husna Aara	
West	7				Smt. Gayatri Devi	
West	8				Smt. Monika Khalkho	
West	9				Smt. Sujata Kachhap	
West	10				Smt. Vina Agrawal	
West	11				Smt. Preety Ranjan	
West	12				Shri Arjun Kumar Yadav	
North	13				Smt. Ranju Singh	
North	14				Shri Kulbhushan Dungdung	
North	15				Smt. Punam Devi	
North	16				Shri Dinesh Ram	
North	17				Smt. Jermin Kujur	
North	18				Smt. Nazima Raja	
North	19				Smt. Shabana Khan	
North	20				Smt. Asha Devi Gupta	
North	21				Smt. Roshni Khalkho	
North	22				Shri Sunil Kumar Yadav	
North	23				Md Ehtesham	
North	24				Smt. Naziya Aslam	
North	25				Smt. Sajda Khatoon	
North	26				Smt. Vijay Laxmi Soni	
South	27				Shri Arjun Ram	
South	28				Shri Arun Kumar Jha	
South	29				Shri OM Prakash	
South	30				Smt. Rashmi Choudhary	
South	31				Smt. Soni Perween	
South	32				Smt. Reema Devi	
South	33				Shri Ashok Yadav	
South	34				Smt. Sunita Devi	
South	35				Smt. Pushpa Toppo	
South	36				Shri Vinod Kumar Singh	
South	37				Shri Jhari Linda	
South	38				Shri Deepak Kumar Lohra	
South	39				Shri Ved Prakash Singh	
East	40				Smt. Suchita Rani Ray	
East	41				Smt. Urmila Yadav	
East	42				Shri Krishna Mahto	
East	43				Smt. Shashi Singh	
East	44				Shri Firoz Alam	
East	45				Shri Nasim Gaddi	
East	46				Smt. Reeta Munda	
East	47				Smt. Kavita Sanga	
East	48				Ms. Marget Minz	
East	49				Smt. Jamila Khatoon	
East	50				Smt. Pushpa Tirkey	
East	51				Smt. Savita Linda	
East	52				Shri Niranjan Kumar	
East	53				Smt. Savita Kujur	
Municipal budget
The Municipal budget is prepared by the Accounts department of Ranchi Municipal Corporation, with inputs from the different administrative departments and elected representatives. The budget is first approved by the Standing committee, after which it has to be approved by the general elected body of the corporation.[17] After getting the approval from the corporation, it is forwarded to the state government for the final approval.[21] The funds are generated by the corporation itself, as well as grants from the state and central governments. The budget for the year 2020–21 is ₹2,276 crores, which is around ₹100 crores less than the 2019–20 municipal budget.[17] The reason for this has been the financial difficulties that the corporation has been facing. Expenditures for the corporation include purchasing water tanks, maintaining night shelters, construction of infrastructure such as roads and footbridges, vending zone, improving facilities at government schools.[17][21] Other expenses include establishment expenditures, programmatic expenditures, procuring fogging machines, setting up ward level offices, beautification of the city, and developing software for the corporation.[22]

Public utilities provision
Ranchi Municipal Corporation provides some important services in the city. The Water Supply department issues new water connections, undertakes boring and installation of hand pumps, and collects fines for illegal connections and boring.[23] The municipal body is responsible for the Solid Waste Management of the city, including collection and disposal of the waste.[24] It has also constructed public toilets in Ranchi and has outsourced the maintenance to private entities.[25] The corporation is responsible for the installation and maintenance of street lights in the city and for painting safety marking on the roads.[26][27] The municipal corporation is also responsible for constructing roads, and is planning to develop sky escalators to ease traffic.[28][29] It maintains municipal parks and regulates parking fees in the city.[30][31][32] The Ranchi Municipal Corporation plans on reopening and operating the only electric crematorium in the city.[33]

Revenue sources
The following are the Income sources for the Corporation from the Central and State Government.[34][35][36]

Revenue from taxes
Following is the Tax related revenue for the corporation.

Property tax.
Profession tax.
Entertainment tax.
Grants from Central and State Government like Goods and Services Tax.
Advertisement tax.
Revenue from non-tax sources
Following is the Non Tax related revenue for the corporation.

Water usage charges.
Fees from Documentation services.
Rent received from municipal property.
Funds from municipal bonds.
Waste User Charges
Processing fees from Building Plan Approvals
Municipal Licences Fees
Auctions of Parking Lots
Rent from Municipal Shops
E-Governance initiatives
Promoting e-governance in the municipality is one of the 'general functions' of Ranchi Municipal Corporation.[37] The municipal corporation is implementing e-governance facilities under the National e-Governance Plan, Jawaharlal Nehru National Urban Renewal Mission and as per the needs of the organisation. The IT department of RMC is headed by Mr.Rajesh Kumar, Department Head of IT.[38] During his tenure RMC has achieved some milestones in e services which includes Computerization of Birth & Death Registration System, Building Plan Approval Management System using Auto-DCR, Online payment of Property Tax, Online Assessment of Properties, Single Window for Municipal License and Water Connection, Master Plan Management System, HRIS for RMC Staffs, e-Rickshaw Management System, Online Registration of Lodge, Hostel & Banquet Halls, Marriage Registration & official web portal of RMC i.e www.ranchimunicipal.com. Social Media of RMC is being handled by the IT Section of RMC.


JUIDCO stands for the Jharkhand Urban Infrastructure Development Company Limited. It is a public sector undertaking under the Government of Jharkhand, serving as the nodal agency for planning, implementing, and managing urban infrastructure projects across the state. 
Key Information
Headquarters: JUIDCO Building, Line Tank Road, Kutchery Chowk, Ranchi, Jharkhand, India.
Formation: Established on November 19, 2013, under the administrative control of the Urban Development & Housing Department, Government of Jharkhand.
Website: The official website is juidco.jharkhand.gov.in. 
Functions and Objectives
JUIDCO aims to improve the quality of life in urban areas of Jharkhand through the development and modernization of urban infrastructure. Its functions include infrastructure development like roads, water supply, and sewerage, creating public amenities such as parks and green spaces, facilitating housing projects for economically weaker sections, providing technical assistance for urban planning, and implementing e-governance in Urban Local Bodies. More details, including tenders and job opportunities, are available on the official JUIDCO website. 


Mars is the fourth planet from the Sun. It is also known as the "Red Planet", for its orange-red appearance.[22][23] Mars is a desert-like rocky planet with a tenuous atmosphere that is primarily carbon dioxide (CO2). At the average surface level the atmospheric pressure is a few thousandths of Earth's, atmospheric temperature ranges from −153 to 20 °C (−243 to 68 °F),[24] and cosmic radiation is high. Mars retains some water, in the ground as well as thinly in the atmosphere, forming cirrus clouds, fog, frost, larger polar regions of permafrost and ice caps (with seasonal CO2 snow), but no bodies of liquid surface water. Its surface gravity is roughly a third of Earth's or double that of the Moon. Its diameter, 6,779 km (4,212 mi), is about half the Earth's, or twice the Moon's, and its surface area is the size of all the dry land of Earth.

Fine dust is prevalent across the surface and the atmosphere, being picked up and spread at the low Martian gravity even by the weak wind of the tenuous atmosphere. The terrain of Mars roughly follows a north-south divide, the Martian dichotomy, with the northern hemisphere mainly consisting of relatively flat, low lying plains, and the southern hemisphere of cratered highlands. Geologically, the planet is fairly active with marsquakes trembling underneath the ground, but also hosts many enormous volcanoes that are extinct (the tallest is Olympus Mons, 21.9 km or 13.6 mi tall), as well as one of the largest canyons in the Solar System (Valles Marineris, 4,000 km or 2,500 mi long). Mars has two natural satellites that are small and irregular in shape: Phobos and Deimos. With a significant axial tilt of 25 degrees, Mars experiences seasons, like Earth (which has an axial tilt of 23.5 degrees). A Martian solar year is equal to 1.88 Earth years (687 Earth days), a Martian solar day (sol) is equal to 24.6 hours.

Mars formed along with the other planets approximately 4.5 billion years ago. During the martian Noachian period (4.5 to 3.5 billion years ago), its surface was marked by meteor impacts, valley formation, erosion, the possible presence of water oceans and the loss of its magnetosphere. The Hesperian period (beginning 3.5 billion years ago and ending 3.3–2.9 billion years ago) was dominated by widespread volcanic activity and flooding that carved immense outflow channels. The Amazonian period, which continues to the present, is the currently dominating and remaining influence on geological processes. Because of Mars's geological history, the possibility of past or present life on Mars remains an area of active scientific investigation, with some possible traces needing further examination.

Being visible with the naked eye in Earth's sky as a red wandering star, Mars has been observed throughout history, acquiring diverse associations in different cultures. In 1963 the first flight to Mars took place with Mars 1, but communication was lost en route. The first successful flyby exploration of Mars was conducted in 1965 with Mariner 4. In 1971 Mariner 9 entered orbit around Mars, being the first spacecraft to orbit any body other than the Moon, Sun or Earth; following in the same year were the first uncontrolled impact (Mars 2) and first successful landing (Mars 3) on Mars. Probes have been active on Mars continuously since 1997. At times, more than ten probes have simultaneously operated in orbit or on the surface, more than at any other planet beyond Earth. Mars is an often proposed target for future crewed exploration missions, though no such mission is currently planned.


